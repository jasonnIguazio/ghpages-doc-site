[{"content":" Overview The platform includes a pre-deployed version of the open-source Grafana analytics and monitoring platform. For detailed information about working with Grafana, see the Grafana documentation. Grafana allows the creation of custom web dashboards that visualize real-time data queries. This tutorial outlines the steps for creating your own Grafana dashboard for querying data in the your cluster's data containers by using the custom iguazio Grafana data source, or by using a Prometheus data source for running Prometheus queries on platform TSDB tables. You can repeat this procedure to create as many dashboards as you wish.\nPrerequisites   To use Grafana to visualize data in the platform, you first need to create a new Grafana service from the dashboard Services page. For general information about creating new services, see creating a new service. Note  In cloud platform environments, Grafana is currently available as a shared single-instance tenant-wide service.\n  To add the iguazio data source to your Grafana service, you must set the service's Platform data-access user parameter to a platform user whose credentials will be used for accessing the data in the platform.\n      To use a Prometheus data source, you first need to create a TSDB table and a Prometheus service that's configured to query this table.\nWhen you create a new Grafana service, it automatically includes data sources for all existing platform Prometheus services that are accessible to the running user of the Grafana service. However, to add a Grafana data source for a new Prometheus service that's create after the creation of the Grafana service, you need to do one of the following:\n  Restart the Grafana service. This will update the service's data sources to include all relevant existing Prometheus services. However, note that any Grafana UI changes that were previously made for this service might be lost.   Manually add the data source for the Prometheus service:\n Open the dashboard Services page and copy the HTTPS URL of the target Prometheus service (the service-name link target in the Name column). Select the service-name link of your Grafana service to open the Grafana UI. From the Grafana side navigation menu, select the configuration gear-wheel icon () to display the Configuration | Data Source tab. Select Add Data Source, and then select Prometheus from the data-sources list. Optionally edit the default data-source name in the Name field (for example, to use the name of the selected Prometheus service). In the URL field, paste the Prometheus service URL that you copied in the first step. Optionally set additional configuration parameters, and then select Save \u0026amp; Test.      To successfully submit queries and visualize data with Grafana, you need to have data in you cluster's data containers that matches your queries. The instructions in this tutorial demonstrate how to create a dashboard that queries the following tables; to query other tables, you'll need to adjust your dashboard configuration, and especially the query parameters, to match the path and attributes of an existing table in your cluster.\n  A bank_nosql NoSQL table in the \u0026quot;bigdata\u0026quot; container. You can create a compatible table by following the instructions in the Converting a CSV File to a NoSQL Table tutorial.   A sample-tsdb TSDB table in the \u0026quot;bigdata\u0026quot; container. You can create such a table by using the TSDB CLI (tsdbctl). For example, you can run the following command from a web-based shell or a Jupyter terminal to create the table used in the tutorial examples:\ntsdbctl create -c bigdata -t sample-tsdb -r 1/s After creating the TSDB table, ingest some sample metrics into the table so that you have data to visualize with Grafana. You can do this by running the V3IO TSDB CLI. For detailed TSDB CLI usage instructions, see The TSDB CLI.\n    Using the iguazio Data Source   Select your Grafana service from the platform dashboard's Services page, and log into the service with your platform credentials.\n  From the side navigation menu, select the plus-sign icon () to display the Create menu, and select Dashboard from the menu.      In the new-panel Add tab, select the desired visualization type. In this step, select Table. (A graph example is provided in Step 11.)      Select Panel Title, and select Edit from the menu.      In the Metrics tab, select the Data Source drop-down arrow, and select iguazio from the list of data sources.      In the first metrics-query row (A), select the query-type drop-down arrow and select the query type. You can select between timeseries for querying a time-series database (TSDB), and table for querying a standard NoSQL table. In this step, select table. (A timeseries example is provided in Step 12.)      Select the drop-down arrow for the query-parameters box (next to the query type) and enter your query parameters. The parameters are entered using the syntax \u0026lt;parameter name\u0026gt;=\u0026lt;parameter value\u0026gt;; [\u0026lt;parameter name\u0026gt;=\u0026lt;parameter value\u0026gt;; ...].    NoteTo see the query parameters in the UI, select the query fly-out menu icon () and then select the Toggle Edit Mode option from the menu. To run the defined queries, select the refresh button () from the top menu toolbar.  The iguazio data source supports the following query parameters:\n  backend [Required] — the backend type.\n For table queries, you must set this parameter to \u0026quot;kv\u0026quot;. For timeseries queries, you must set this parameter to \u0026quot;tsdb\u0026quot;.    container [Required] — the name of the data container that contains the queried table. For example, \u0026quot;projects\u0026quot; or \u0026quot;users\u0026quot;.\n  table [Required] — a Unix file-system path to a table within the configured container. For example, \u0026quot;mytable\u0026quot; (for a table in the container's root directory) or \u0026quot;examples/\u0026quot; for a nested table.\n  fields — a comma-separated list of one or more query fields to include in the query.\n For table queries, this parameter should be set to a comma-separated list of item-attribute names — for example, \u0026quot;id,first_name,last_name\u0026quot;. For timeseries queries, this parameter should be set to a comma-separated list of sample metric names — for example, \u0026quot;temperature\u0026quot; or \u0026quot;cpu0,cpu1,cpu2\u0026quot;.  NoteYou must set either the fields or filter parameter; you can also select to set both parameters.\n    filter — a filter expression that restricts the query to specific items.\n For table queries, the filter is applied to the names of item attributes (table columns). For example, filter=ID\u0026gt;10. For timeseries queries, the filter is applied to the names of labels and/or metrics. To reference a metric name in the filter, use the __name__ attribute. For example, \u0026quot;starts(__name__,'metric') AND device==1\u0026quot;.  As indicated, you must set the filter and/or fields parameter.\n  Enter the following query parameters to query selected attributes in the \u0026quot;bank_nosql\u0026quot; table in the root directory of the \u0026quot;bigdata\u0026quot; container, or edit the parameters to match the data in another NoSQL table in your cluster:\nbackend=kv; container=bigdata; table=zeppelin_getting_started_example/bank_nosql; fields=age,job,marital,education,balance,housing,loan,contact,day,month,duration; You can also add additional queries, if you wish.\n    Select the General tab and edit the panel title. For example, set the title to \u0026quot;Bank\u0026quot;.      You can optionally make additional configurations for your dashboard panel from the Options, Column Styles, and Time range tabs.   From the top-menu toolbar, select the save icon (). Enter a new name for your dashboard, optionally change the dashboard folder, and select Save. For example, set the name to \u0026quot;Iguazio Dashboard\u0026quot; and save the dashboard in the default \u0026quot;General\u0026quot; folder.    TipInstead of selecting the save icon from the toolbar, you can select the settings icon () to display the Settings page. In addition to setting the dashboard name and folder, this page lets you make other configurations; for example, you can define variables to use in your query parameters.  You now have a \u0026quot;Bank\u0026quot; dashboard panel with a table that provides real-time visualization of the data in the \u0026quot;bank_nosql\u0026quot; table in the \u0026quot;bigdata\u0026quot; container.    In the following steps, you'll add a TSDB graph to your dashboard.\n  From the top-menu toolbar, select the add-panel icon () to display the new-panel Add tab that you saw in Step 3. This time, select the Graph visualization type.      Select the Panel Title | Edit menu option (as you did in Step 4). In the Metrics tab, select iguazio as the data source (as you did in Step 5), but this time select the timeseries query type (unlike the table selection in Step 6).      Select the drop-down arrow for the query-parameters box (next to the query type) and enter your query parameters using the syntax \u0026lt;parameter name\u0026gt;=\u0026lt;parameter value\u0026gt;; [\u0026lt;parameter name\u0026gt;=\u0026lt;parameter value\u0026gt;; ...] — similar to what you did in Step 7, except that you need to enter time-series query parameters for a TSDB table; see the supported query parameters in Step 7.    NoteAs explained for the table query, you can toggle the query edit mode from the query menu () to see the parameters in the UI, and use the refresh button () in the top menu toolbar to run the query.  Enter the following query parameters to query \u0026quot;metric0\u0026quot; samples in the \u0026quot;sample-tsdb\u0026quot; table in the root directory of the \u0026quot;bigdata\u0026quot; container, or edit the parameters to match the data in another TSDB table in your cluster:\nbackend=tsdb; container=bigdata; table=sample-tsdb; fields=metric0; You can also add additional queries, if you wish.\n  Select the General tab and edit the panel title, as you did in Step 8, but this time enter another title — for example, \u0026quot;Sample TSDB\u0026quot;.      You can optionally make additional configurations for your dashboard panel from the Axes, Legend Styles, Display, Alert, and Time range tabs.\n  From the top-menu toolbar, select the save icon () to save your changes.\nYou now have a \u0026quot;Sample TSDB\u0026quot; dashboard panel with a graph that provides real-time visualization of the data in the sample-tsdb TSDB table in the \u0026quot;bigdata\u0026quot; container.      Outcome Go to the Grafana home page and select your \u0026quot;Iguazio Dashboard\u0026quot; dashboard from the list. You'll see your two panels on the dashboard. Note that you can change the time range for the visualization from the top-menu toolbar. You can also customize the dashboard by resizing panels and dragging them to change their arrangement. The following image demonstrates a dashboard that displays the panels that you defined in this tutorial:\n   Using a Prometheus Data Source   Log into your Grafana service and select your Grafana dashboard. To edit an existing dashboard, open the dashboard and then, from the top-menu toolbar, select the add-panel icon () to display the new-panel Add tab. To create a new dashboard, follow Steps 2–4 of the iguazio data-source instructions; select the visualization type that suites your need (such as table or graph).\n  In the Metrics tab, select the Data Source drop-down arrow, and select your desired Prometheus data source.\n  From the Metrics tab, add Prometheus TSDB queries.\n  Select the General tab and edit the panel title, as you did in Step 8, but use another title — for example, the name of your Prometheus data source.\n  From the top-menu toolbar, select the save icon () to save your changes.\n  Outcome Go to the Grafana home page and select your dashboard from the list. You should be able to see your new Prometheus panel on the dashboard.\nSee Also  Logging and Monitoring Services  Grafana   The Monitoring Application Service and Grafana Dashboards TSDB application services Time-Series Databases (TSDB) The TSDB CLI  ","keywords":["grafana,","dashboards,","visualiation,","tsdb,","time","series,","timeseries,","nosql,","nosql","tables,","tables,","table,","key","value,","KV,","kv,","graphs,","Iguazio,","Iguazio","data","source,","data","sources,","graphs"],"path":"https://github.com/jasonnIguazio/services/grafana-dashboards/","title":"Adding Grafana Dashboards"},{"content":" Overview The data containers and their contents are referenced differently depending on the programming interface. You need to know how to set the data paths for each interface, as outlined in this guide:\n RESTful Web and Management API Data Paths Frames API Data Paths Spark API Data Paths Presto Data Paths File-System Data Paths  Predefined Environment Variables The platform's command-line services (Jupyter Notebook and the web shell) predefine the following environment variables for simplifying access to the running-user directory of the predefined \u0026quot;users\u0026quot; container:\n V3IO_USERNAME — set to the username of the running user of the Jupyter Notebook service. V3IO_HOME — set to the running-user directory in the \u0026quot;users\u0026quot; container — users/\u0026lt;running user\u0026gt;. V3IO_HOME_URL — set to the fully qualified v3io path to the running-user directory — v3io://users/\u0026lt;running user\u0026gt;.  RESTful Web and Management API Data Paths To refer to a data container or to a specific directory or file in a container from a RESTful web or cluster-management API request, specify the path as part of the URL in the request header:\n\u0026lt;API-endpoint URL\u0026gt;/\u0026lt;container name\u0026gt;[/\u0026lt;path to file or directory\u0026gt;] For example, the following web-API request URL references the \u0026quot;projects\u0026quot; container:\nhttps://default-tenant.app.mycluster.iguazio.com:8443/projects/ And this is a similar cluster-management API (\u0026quot;management API\u0026quot;) request URL:\nhttps://dashboard.default-tenant.app.mycluster.iguazio.com/projects/ The following web-API request URL references a mytable table directory in an iguazio running-user directory in the \u0026quot;users\u0026quot; container:\nhttps://default-tenant.app.mycluster.iguazio.com:8443/users/iguazio/mytable When using the platform's data-service web APIs, you can optionally set the relative file or directory path within the configured container in the request's JSON body. For example, for a NoSQL Web API request, you can end the URL path in the previous example with the container name (users) and set the TableName data parameter in the request's JSON body to \u0026quot;mydata/mytable\u0026quot;.\nFor full details and examples, see the data-service web-API and management-API reference documentation.\nFrames API Data Paths When using the V3IO Frames (Frames) Python API, you create a client object for a specific data container; the container name is specified in the container parameter of the Client constructor. For example:\nimport v3io_frames as v3f # Create a client object for the \u0026#34;users\u0026#34; container: client = v3f.Client(\u0026#34;framesd:8081\u0026#34;, container=\u0026#34;users\u0026#34;, token=\u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34;) To refer to a specific data collection — such as a NoSQL or TSDB table or a stream — you specify in the relevant Client method parameter the relative data path within the container of the parent client object. In most cases, the data path is set in the table parameter. For example:\n# Read from a \u0026#34;mytable\u0026#34; table in the root directory of the `client` object\u0026#39;s # container: df = client.read(backend=\u0026#34;kv\u0026#34;, table=\u0026#34;mytable\u0026#34;) # Read from a \u0026#34;mytable\u0026#34; table in the running-user directory (`V3IO_USERNAME`) # of the `client` object\u0026#39;s container (typically for the \u0026#34;users\u0026#34; container): tsdb_table = os.path.join(os.getenv(\u0026#34;V3IO_USERNAME\u0026#34;), \u0026#34;mytable\u0026#34;) df = client.read(backend=\u0026#34;tsdb\u0026#34;, table=tsdb_table) # Read from a \u0026#34;drivers\u0026#34; stream in a \u0026#34;my_streams\u0026#34; directory in the `client` # object\u0026#39;s container: stream = \u0026#34;/my_streams/drivers\u0026#34; df = client.read(backend=\u0026#34;stream\u0026#34;, table=\u0026#34;/my_streams/drivers\u0026#34;, seek=\u0026#34;earliest\u0026#34;)  For detailed information and examples, see the Frames API reference.\nSpark API Data Paths To refer to data in a data container from Spark API code, such as Spark DataFrames, specify the data path as a fully qualified v3io path of the following format — where \u0026lt;container name\u0026gt; is the name of the parent data container and \u0026lt;data path\u0026gt; is the relative path to the data within the specified container:\nv3io://\u0026lt;container name\u0026gt;/\u0026lt;data path\u0026gt; When using a NoSQL DataFrame, you set the data source to \u0026quot;io.iguaz.v3io.spark.sql.kv\u0026quot;.\nFor example:  val nosql_source = \u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34; // Read from a \u0026#34;mytable\u0026#34; NoSQL table in a \u0026#34;mydata\u0026#34; directory in the \u0026#34;projects\u0026#34; container: var table_path = \u0026#34;v3io://projects/mydata/mytable/\u0026#34; var readDF = spark.read.format(nosql_source).load(table_path) // Read from a \u0026#34;mytable\u0026#34; table in the running-user directory of the \u0026#34;users\u0026#34; container. // The table_path assignments demonstrate alternative methods for setting the same path // for running-user \u0026#34;iguazio\u0026#34; (specified explicitly only in the first example): table_path = \u0026#34;v3io://users/iguazio/mytable\u0026#34; table_path = \u0026#34;v3io://users/\u0026#34; + System.getenv(\u0026#34;V3IO_USERNAME\u0026#34;) + \u0026#34;/mytable\u0026#34; table_path = \u0026#34;v3io://\u0026#34; + System.getenv(\u0026#34;V3IO_HOME\u0026#34;) + \u0026#34;/mytable\u0026#34; table_path = System.getenv(\u0026#34;V3IO_HOME_URL\u0026#34;) + \u0026#34;/mytable\u0026#34; readDF = spark.read.format(nosql_source).load(table_path)  import os nosql_source = \u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34; # Read from a NoSQL table \u0026#34;mytable\u0026#34; in a \u0026#34;mydata\u0026#34; directory in the \u0026#34;projects\u0026#34; container: table_path = \u0026#34;v3io://projects/mydata/mytable/\u0026#34; df = spark.read.format(nosql_source).load(table_path) # Read from a \u0026#34;mytable\u0026#34; table in the running-user directory of the \u0026#34;users\u0026#34; container. # The table_path assignments demonstrate alternative methods for setting the same path # for running-user \u0026#34;iguazio\u0026#34; (specified explicitly only in the first example): table_path = \u0026#34;v3io://users/iguazio/mytable\u0026#34; table_path = \u0026#34;v3io://users/\u0026#34; + os.getenv(\u0026#34;V3IO_USERNAME\u0026#34;) + \u0026#34;/mytable\u0026#34; table_path = \u0026#34;v3io://\u0026#34; + os.getenv(\u0026#34;V3IO_HOME\u0026#34;) + \u0026#34;/mytable\u0026#34; table_path = os.getenv(\u0026#34;V3IO_HOME_URL\u0026#34;) + \u0026#34;/mytable\u0026#34; readDF = spark.read.format(nosql_source).load(table_path)    For detailed information and examples, see the Spark datasets reference — and especially the Data Paths overview and the Table Paths NoSQL DataFrame sections; the Getting Started with Data Ingestion Using Spark tutorial; and the Spark examples in the platform's tutorial Jupyter notebooks (see details in the Ingesting and Preparing Data tutorial).\nPresto Data Paths To refer to a table in a data container from a Presto query, specify the table path using the following format — where \u0026lt;catalog\u0026gt; is the name of the Presto connector catalog (v3io for the Iguazio Presto connector, \u0026lt;container name\u0026gt; is the name of the table's parent data container (the Presto schema), and \u0026lt;table path\u0026gt; is the relative path to the table within the specified container:\n[\u0026lt;catalog\u0026gt;.][\u0026lt;container name\u0026gt;.]\u0026lt;table path\u0026gt; To specify a path to a nested table, use the following syntax:\n[\u0026lt;catalog\u0026gt;.][\u0026lt;container name\u0026gt;.]\u0026#34;/path/to/table\u0026#34; The catalog and container (schema) names are marked as optional ([]) because you can select to configure default values for these parameters when starting the Presto CLI. For example, the presto wrapper that's available in the platform command-line service environments preconfigures v3io as the default catalog.\nFor example, following are Presto CLI queries that reference NoSQL tables in the platform's data containers:\n# Query a \u0026#34;mytable\u0026#34; table in the \u0026#34;projects\u0026#34; container: SELECT * FROM v3io.projects.mytable; # Query a \u0026#34;mytable\u0026#34; table in the \u0026#34;iguazio\u0026#34; running-user directory of the \u0026#34;users\u0026#34; container: SELECT * FROM v3io.users.\u0026#34;/iguazio/mytable\u0026#34;;  Note  When using the presto wrapper instead of the native Presto CLI, you can omit \u0026quot;v3io.\u0026quot; from the path:\nSELECT * FROM projects.mytable; SELECT * FROM users.\u0026#34;/iguazio/mytable\u0026#34;;   You can use a bash table-path variable and the Presto CLI's execute option to replace the hardcoded running-user directory name in the second example (\u0026quot;iguazio\u0026quot;) with the V3IO_USERNAME environment variable:\npresto_table_path=\u0026#34;v3io.users.\\\u0026#34;/$V3IO_USERNAME/mytable\\\u0026#34;\u0026#34; presto --execute \u0026#34;SELECT * FROM $presto_table_path\u0026#34;     Following is an example of an SQL query in a Python Jupyter Notebook, which uses Presto to query a \u0026quot;mytable\u0026quot; table in the running-user directory of the \u0026quot;users\u0026quot; container: presto_table_path = os.path.join(\u0026#39;v3io.users.\u0026#34;/\u0026#39; + os.getenv(\u0026#34;V3IO_USERNAME\u0026#34;) + \u0026#39;/mytable\u0026#34;\u0026#39;) print(\u0026#34;SELECT * FROM \u0026#34; + presto_table_path) %sql SELECT * FROM $presto_table_path For detailed information and examples, see Using Presto, and especially the Table Paths overview and the similar Presto CLI guide that it references.\nFile-System Data Paths  Local file-system data paths Hadoop FS data paths  Local File-System Data Paths To refer to data in the platform from a local file-system command, use the predefined \u0026quot;v3io\u0026quot; data mount:\n/v3io[/\u0026lt;container name\u0026gt;][/\u0026lt;path to file or directory\u0026gt;] To refer to the running-user directory in the \u0026quot;users\u0026quot; container, you can select to use the predefined \u0026quot;User\u0026quot; mount to this directory:\n/User/[\u0026lt;path to file or directory in the users/\u0026lt;username\u0026gt; directory\u0026gt;]  For example:\n# List all data-container directories ls /v3io # List the contents of the \u0026#34;projects\u0026#34; container ls /v3io/projects/ # List the contents of the \u0026#34;mydata\u0026#34; directory in the \u0026#34;projects\u0026#34; container ls -lF /v3io/projects/mydata/ # Copy a myfile.txt file from a \u0026#34;mydata\u0026#34; directory in the \u0026#34;projects\u0026#34; container # to the running-user directory of the \u0026#34;users\u0026#34; container for user \u0026#34;iguazio\u0026#34;. # All of the following syntax variations evaluate to the same copy command: cp /v3io/projects/mydata/myfile.txt /v3io/users/iguazio/ cp /v3io/projects/mydata/myfile.txt /v3io/users/$V3IO_USERNAME cp /v3io/projects/mydata/myfile.txt /v3io/$V3IO_HOME cp /v3io/projects/mydata/myfile.txt /User Hadoop FS File-System Data Paths To refer to a data container or its contents from an Hadoop FS command, specify the data path as a fully qualified v3io path of the following format:\nv3io://\u0026lt;container name\u0026gt;/[\u0026lt;data path\u0026gt;] For example:\n# List the contents of the \u0026#34;projects\u0026#34; container hadoop fs -ls v3io://projects/ # List the contents of the \u0026#34;mydata\u0026#34; directory in the \u0026#34;projects\u0026#34; container hadoop fs -ls -lF v3io://projects/mydata/ # Copy a myfile.txt file from a \u0026#34;mydata\u0026#34; directory in the \u0026#34;projects\u0026#34; container # to the running-user directory of the \u0026#34;users\u0026#34; container for user \u0026#34;iguazio\u0026#34; # All of the following syntax variations evaluate to the same copy command: hadoop fs -cp v3io://projects/mydata/myfile.txt v3io://users/iguazio/ hadoop fs -cp v3io://projects/mydata/myfile.txt v3io://users/$V3IO_USERNAME hadoop fs -cp v3io://projects/mydata/myfile.txt v3io://$V3IO_HOME hadoop fs -cp v3io://projects/mydata/myfile.txt $V3IO_HOME_URL  NoteThe URI generic-syntax specification requires that fully qualified paths contain at least three forward slashes (/). Therefore, to list the contents of a container's root directory you must end the path with a slash, as demonstrated in the examples.  See Also  Data Containers  The Predefined Containers   Data-Layer APIs Overview Data-Layer References Ingesting and Preparing Data  ","keywords":["api","data","paths,","container","data","paths,","data","paths,","v3io","mount,","v3io,","data","apis,","api,","rest","api,","rest,","web","api,","cluster-management","apis,","management","api,","frames,","spark,","presto"],"path":"https://github.com/jasonnIguazio/data-layer/apis/data-paths/","title":"API Data Paths"},{"content":"The platform exposes the following application programming interfaces (APIs).\n  Data science and MLOps APIs — the platform integrates Iguazio's open-source MLRun machine-learning operations (MLOps) orchestration framework and Nuclio serverless-functions framework. For detailed information about these frameworks and their APIs, see Data Science and MLOps.\n  Data-layer APIs — the platform exposes multiple APIs for working with data of all types — NoSQL (\u0026quot;key-value\u0026quot;) databases, time-series databases (TSDBs), streams, and simple objects (files) — including the following APIs. For detailed information, see the data-layer APIs overview and API references.\n The Iguazio V3IO Python SDK for working with NoSQL and stream data The Iguazio V3IO Frames DataFrame API for working with NoSQL and time-series data  Spark SQL and DataFrames and streaming APIs, including the platform's proprietary APIs for working with NoSQL and stream data The platform's simple-object, NoSQL, and streaming RESTful web APIs Local Linux file-system and Hadoop Compatible File System (HCFS) commands for working with files (simple objects)    Cluster-Management APIs (\u0026quot;Management APIs\u0026quot;) [Beta] — the platform exposes REST APIs for performing administrative cluster-management operations. For detailed information, see the cluster-management APIs reference.\n  See Also  Data Science and MLOps Data-Layer APIs Overview Data-Layer References Cluster-Management References API Error Information Introducing the Platform Platform Services  ","keywords":["apis","overview,","overview,","introduction,","apis,","web","apis,","REST,","RESTful,","simple","objects,","S3,","data-service","web","apis,","streaming,","nosql,","key","value,","KV,","scala,","python,","spark,","spark","streaming,","hadoop,","hcfs,","hdfs,","file","system,","FUSE,","management","apis,","dashboard"],"path":"https://github.com/jasonnIguazio/intro/apis-overview/","title":"APIs Overview"},{"content":" Services List To help you locate the services and tools that interest you, following is an alphabetical list with links to relevant documentation:\n Dask Docker Registry Elasticsearch Frames Grafana Hadoop Hive Horovod Jupyter Kubeflow Pipelines Log Forwarder Looker MLRun Monitoring MPI Operator Nuclio NumPy pandas Pipelines Presto Prometheus Pyplot PyTorch QlikView RAPIDS scikit-learn Spark Tableau TensorFlow TSDB CLI Web Shell  See Also  Configuring the DNS Server Working with Services Running Applications over GPUs The Platform Dashboard (UI) Introducing the Platform Logging, Monitoring, and Debugging Data Layer  ","keywords":["application","services,","services,","managed","services,","managed","application","services,","integrated","services,","integrated","application","services,","pre-deployed","services,","preinstalled","services,","ecosystem,","development","ecosystem,","open","source,","kubernetes,","installation,","analytics,","data","analytics,","data","pipelines,","ML","pipelines,","automated","pipelines,","worflow","pipelines,","operational","pipelines,","data-engineering","pipelines,","data","science","pipelines,","serverless,","web","notebook,","visualization,","data","visualization,","dask,","v3io","frames,","frames,","go,","golang,","grafana,","hadoop,","hive,","jupyter,","jupyterlab,","jupyter","notebook,","jupyter","tutorials,","v3io","tutorials,","tutorials,","keras,","kubeflow,","kubeflow","pipelines,","looker,","mlrun,","orc,","pandas,","parquet,","numpy,","parquez,","presto,","presto","cli,","Iguazio","Presto","connector,","v3io","prometheus,","prometheus,","pyplot,","pytortch,","nosql","spark","dataframe,","nuclio,","qlikview,","r","language,","sparkr,","shell,","web","shell,","command","line,","ggplot,","spark,","Iguazio","Spark","connector,","tableau,","tensorflow,","v3io","tsdb,","tsdb,","time","series,","tech","preview"],"path":"https://github.com/jasonnIguazio/services/app-services/overview/","title":"Application-Services Overview"},{"content":"The following binary arithmetic operators are supported in expressions. The operands are numeric.\nAddition Operator (+) OPERAND-A + OPERAND-B The addition operator (+) adds up the values of the two operands, and returns the sum.\nDivision Operator (/) OPERAND-A / OPERAND-B The division operator (/) divides the left operand (OPERAND-A) by the right operand (OPERAND-B), and returns the product.\nMultiplication Operator (*) OPERAND-A * OPERAND-B The multiplication operator (*) multiplies the values of the operands, and returns the result.\nSubtraction Operator (-) OPERAND-A - OPERAND-B The subtraction operator (-) subtracts the right operand (OPERAND-B) from the left operand (OPERAND-A), and returns the difference.\n","keywords":["expression","arithmetic","operators,","arithmetic","operators,","expression","operators,","attributes","arithmetic,","addition","operator,","+","operator,","plus","operator,","subtraction","operator,","-","operator,","minus","operator,","multiplication","operator,","*","operator,","division","operator,","/","operator,","attribute","variables,","attribute","values,","adding","attributes,","subtracting","attributes,","multiplying","attributes,","dividing","attributes"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/arithmetic-operators/","title":"Arithmetic Operators"},{"content":" Note  In the current release, the support for array attributes and the use of array operators and functions in expressions is restricted to the web APIs.\n  See also the array support of the max and min functions.\n    length length(ARRAY) Returns the length of the provided array or array slice (ARRAY) — i.e., the number of elements in the array or slice. The parameter can be the name of an array attribute (ARRAY-ATTRIBUTE), or a slice of an array attribute that indicates the zero-based index range of the array elements to compare (ARRAY-ATTRIBUTE[istart..iend]).\nFor example, for a myArray array attribute with the values 7, 13, 5, and 1, \u0026quot;length(myArray)\u0026quot; returns 4 and \u0026quot;length(myArray[2..3])\u0026quot; returns 2.\nNotelength is supported in update expressions.  init_array init_array(size, type) Creates an array attribute and initializes all array elements to zero. The function receives the following parameters:\n size The size of the array — i.e., the number of elements in the array.\n Type: Integer   Requirement: Required   type The data type of the array elements.\n Type: String   Requirement: Required   Valid Values:\n 'int' — a 64-bit integer 'double' — a 64-bit double      For example, an \u0026quot;arr = init_array(3, 'int')\u0026quot; update expression creates an arr array attribute that represents an integer array with three elements, all initialized to zero. You can also change the values of specific elements in the same expression — for example \u0026quot;arr=init_array(3, 'int'); arr[1]=1; arr[2]=arr[1]+1;\u0026quot; results in a new arr array attribute with three elements whose values are 0 (default), 1, and 2.\nAfter creating an array attribute, you can use the array operator ([]) to reference array elements .\nSee Also  Array-Attribute Expression Variables Array Operator ([ ]) Update Expression Assignment Operator (=)  ","keywords":["array","functions,","init_array","function,","init_array,","array","attributes,","arrays,","attribute","variables,","update","expressions"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/functions/array-functions/","title":"Array Functions"},{"content":"Overview The platform enables you to define array attributes — blob attributes that represent numeric arrays of 64-bit integer or double values. See Array-Attribute Expression Variables. You can use one of the following syntax variations of the array operator ([]) to reference specific elements of an array attribute in expressions. In all variations, an element's index number can also be provided as an expression that evaluates to an integer.\nNoteIn the current release, the support for array attributes and the use of array operators and functions in expressions is restricted to the web APIs.  Single Array Element ATTRIBUTE[i] This syntax references an element at the specified zero-based index (i) of an array attribute (ATTRIBUTE). For example, \u0026quot;km[ctr+1]\u0026quot;, where ctr is a numeric attribute with the value 2, references the fourth element (index 3) of the km array attribute.\nArray Slice ATTRIBUTE[istart..iend]  This syntax references an array slice — a range of elements between the specified zero-based start (istart) and end (iend) indexes of an array attribute (ATTRIBUTE). Note that the range includes the end index. In v3.2.1, the following uses of array slices in expressions are supported:  As a parameter of the max or min function — to retrieve the highest or lowest value from among the elements in the specified array slice. For example, \u0026quot;max(arr[2..10])\u0026quot; returns the highest value from among the values of the arr array attribute's elements at indexes 2 to 10. As an operand of the array operator ([]) in a SET update expression — to assign a slice of an existing array attribute to another array attribute. For example, \u0026quot;SET arr2 = arr1[0..4]\u0026quot; assigns the values of the first five elements of an existing arr1 array attribute to an arr2 attribute.  See Also  Array-Attribute Expression Variables Array Functions Update Expression Assignment Operator (=)  ","keywords":["expression","array","operator,","array","operator,","[]","operator,","[],","expression","operators,","attribute","variables"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/array-operator/","title":"Array Operator ([ ])"},{"content":"OPERAND-A = OPERAND-B The assignment operator (=) is used in SET update expressions to assign the value of the right operand (OPERAND-B) to the left operand (OPERAND-A). The left operand should be either an attribute variable (ATTRIBUTE) or an element of an array attribute variable (ATTRIBUTE[i]).\nNoteIn the current release, the support for array attributes and the use of array operators and functions in expressions is restricted to the web APIs.  See Also  Update Expression Attribute Variables  ","keywords":["expression","assignment","operator,","assignment","operator,","=","operator,","expression","operators,","update","expressions,","SET","expressions,","update","items,","attributes,","attribute","variables,","ATTRIBUTE,","techpreview,","tech","preview,","arrays,","array","attributes"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/assignment-operator/","title":"Assignment Operator (=)"},{"content":"Overview The platform installation requires that you attach data disks (block-storage devices) of a specific type and minimal capacity directly to each of the platform's data-node VMs (\u0026quot;the data nodes\u0026quot;), as outlined in this guide.\nPrerequisites Before you begin, ensure that you have the following:\n Administrative access to a platform PVE cluster with the required networks configuration (see Configuring Virtual Networking (PVE)) and deployed VMs for each of the platform nodes (see Deploying the Platform Nodes (PVE)). At least two 1 TB enterprise-grade SSDs for each of the data nodes. Note that you need a separate set of disks for each data node.  Attaching Data Disks to the Data Nodes To attach local storage devices (data disks) to the platform's data-node VMs, execute the following procedure on each PVE host that has deployed data-node VMs:\n  Establish an SSH connection to the PVE host.\n  List the disks (block devices) that are attached to the PVE host by running the following command:\n~# lsblk -o name,size,vendor,model,serial In the returned output, identify the disks that you want to attach and copy their serial numbers.\nFollowing is an example command output:\nNAME SIZE VENDOR MODEL SERIAL sda 44.7G ATA INTEL_SSDSCKKR048H6 CVLY609000MU048A sdb 953.9G ATA SanDisk_SD8SB8U1T001122 164108420934 ├─sdb1 1007K ├─sdb2 512M └─sdb3 953.4G ├─pve-root 96G ├─pve-swap 8G ├─pve-data_tmeta 8.3G │ └─pve-data-tpool 816.7G │ ├─pve-data 816.7G │ ├─pve-vm--101--disk--0 400G │ └─pve-vm--102--disk--0 400G └─pve-data_tdata 816.7G └─pve-data-tpool 816.7G ├─pve-data 816.7G ├─pve-vm--101--disk--0 400G └─pve-vm--102--disk--0 400G sdc 953.9G ATA SanDisk_SD8SB8U1T001122 163493421984 sdd 953.9G ATA SanDisk_SD8SB8U1T001122 163493420112 In this example, disk \u0026quot;sdb\u0026quot; is the PVE partition disk on which the PVE host is installed (note the pve-\u0026lt;...\u0026gt; partitions). You can select to attach disks \u0026quot;sdc\u0026quot; and/or \u0026quot;sdd\u0026quot; as data disks to either of the host's two data nodes (ID 101 and 102). WarningTake care not to perform any manipulations on data-store devices, such as the PVE partition disk.    Locate the unique data paths for the disks that you want to attach to the data nodes, by running the following command; replace \u0026lt;disk serial number\u0026gt; with the disk serial numbers that you copied in the previous step:\n~# lsblk ls /dev/disk/by-id/* | grep -v part | grep \u0026lt;disk serial number\u0026gt; Copy the data paths for your selected disks.\nThe following example shows the commands and output for attaching disks \u0026quot;sdc\u0026quot; and \u0026quot;sdd\u0026quot; from the example in the previous step; note that the command filters the results by the disks' serial numbers, as identified in the previous step — 163493421984 and 163493420112 (respectively):\nroot@pve-intel1:~# ls /dev/disk/by-id/* | grep -v part | grep 163493421984 /dev/disk/by-id/ata-SanDisk_SD8SB8U1T001122_163493421984 root@pve-intel1:~# ls /dev/disk/by-id/* | grep -v part | grep 163493420112 /dev/disk/by-id/ata-SanDisk_SD8SB8U1T001122_163493420112   Configure the VM options for each data disk that you want to attach, by running the following command; replace the \u0026lt;...\u0026gt; placeholders, as explained later in this step:\ndev=/dev/disk/by-id/\u0026lt;data-disk data path\u0026gt; ; qm set \u0026lt;data-node ID\u0026gt; --scsi\u0026lt;n\u0026gt; ${dev[}[,iothread=1],snapshot=0,backup=0,serial=$(lsblk -nd -o serial ${dev})  \u0026lt;data-disk data path\u0026gt; is the disk's data path, as identified in the previous step. The --scsi\u0026lt;n\u0026gt; option configures the name of the disk's computer system interface (SCSI). Assign a unique name by replacing \u0026lt;n\u0026gt; with a unique numeric SCSI ID. NoteThe VM's boot disk is mapped to interface \u0026quot;scsi1\u0026quot;. Therefore, for the data disks, start with \u0026quot;scsi1\u0026quot;, and then increment the SCSI ID in each command (--scsi1, --scsi2, etc.).   Use the --serial option to pass the disk's serial numbers to the VM. Note that the command extracts the serial number from the provided data path, so you don't need to enter it manually. You can optionally set iothread=1 to creates a separate I/O thread per disk, which improves performance.  The following example maps the two selected data disks from the previous example — \u0026quot;sdc\u0026quot; (data path \u0026quot;ata-SanDisk_SD8SB8U1T001122_163493421984\u0026quot;) and \u0026quot;sdd\u0026quot; (data path \u0026quot;ata-SanDisk_SD8SB8U1T001122_163493420112\u0026quot;) — to interfaces \u0026quot;scsi1\u0026quot; and \u0026quot;scsi2\u0026quot; on the first data node (VM ID 101):\ndev=/dev/disk/by-id/ata-SanDisk_SD8SB8U1T001122_163493421984 ; qm set 101 --scsi1 ${dev},iothread=1,snapshot=0,backup=0,serial=$(lsblk -nd -o serial ${dev}) dev=/dev/disk/by-id/ata-SanDisk_SD8SB8U1T001122_163493420112 ; qm set 101 --scsi2 ${dev},iothread=1,snapshot=0,backup=0,serial=$(lsblk -nd -o serial ${dev})   Verifying the Procedure When you're done, for each data-node VM to which you attached devices, on all relevant PVE hosts, check the devices list under \u0026lt;PVE host\u0026gt; | \u0026lt;data-node VM\u0026gt; | Hardware in the PVE GUI and verify that the data disks that you attached to the respective node VM appear correctly in the list, as demonstrated in the following image:\n   See Also  Proxmox VE installation guide On-Prem Deployment Specifications  ","keywords":["attach","data","disks","to","Proxmox","VE","vm","data","nodes,","attach","data","disks","to","vm","data","nodes,","vm","data-node","disk","attachment,","attach","SSDs","to","vm","data","nodes,","SSD","vm","data-node","attachment,","data","disks,","boot","disks,","data","nodes,","solid","state","drives,","ssd,","raw","device","mapping,","rdm,","scsi,","iothread"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/data-disks-attach/","title":"Attaching Data Disks to the Data-Node VMs (PVE)"},{"content":"Overview The platform installation requires that you attach data disks (block-storage devices) of a specific type and minimal capacity, which are configured with raw device mapping (RDM), directly to each of the platform's data-node VMs (\u0026quot;the data nodes\u0026quot;), as outlined in this guide.\nPrerequisites Before you begin, ensure that you have the following:\n Administrative access to a platform vSphere cluster with the required networks configuration (see Configuring Virtual Networking (vSphere)) and deployed VMs for each of the platform nodes (see Deploying the Platform Nodes (vSphere)). At least two 1 TB enterprise-grade SSDs for each of the data nodes. Note that you need a separate set of disks for each data node.  Attaching Data Disks to the Data Nodes To configure RDM on local storage devices (data disks) and attach the disks to the platform's data-node VMs, execute the following procedure on each ESXi host that has deployed data-node VMs:\n  Establish an SSH connection to the ESXi host.\n  List the disks (block devices) that are attached to the ESXi host by running the following command:\n~# ls /vmfs/devices/disks | grep -v \u0026#34;vml\\|:\u0026#34; In the returned output, identify the data disks that you want to attach and copy their names. The device names are likely to be prefixed with \u0026quot;t10.\u0026quot;, as demonstrated in the following example command output:\nt10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF80350054450RGN__00000001 t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF803500GP450RGN__00000001 t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF8036004R450RGN__00000001 t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF8036004Y450RGN__00000001  WarningTake care not to perform any manipulations on data-store devices.    For each disk that you want to attach to a data-node VM, run the following command to configure the disk as an RDM device and map it to a new RDM-pointer VMDK file in the target data-node VM directory:\nvmkfstools -z /vmfs/devices/disks/\u0026lt;disk name\u0026gt; /vmfs/volumes/\u0026lt;data store name\u0026gt;/\u0026lt;VM directory\u0026gt;/\u0026lt;VM name\u0026gt;.vmdk For example, the following command configures and maps four SSDs to ssd\u0026lt;n\u0026gt;.vmdk files in a data-node VM directory in a \u0026quot;datastore1\u0026quot; data store on the ESXi host:\nvmkfstools -z /vmfs/devices/disks/t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF80350054450RGN__00000001 /vmfs/volumes/datastore1/data-node/ssd1.vmdk vmkfstools -z /vmfs/devices/disks/t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF803500GP450RGN__00000001 /vmfs/volumes/datastore1/data-node/ssd2.vmdk vmkfstools -z /vmfs/devices/disks/t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF8036004R450RGN__00000001 /vmfs/volumes/datastore1/data-node/ssd3.vmdk vmkfstools -z /vmfs/devices/disks/t10.NVMe____INTEL_SSDPE2MX450G7_____________________BTPF8036004Y450RGN__00000001 /vmfs/volumes/datastore1/data-node/ssd4.vmdk   Verify that the configured VMDK files were created in the target data-node VM directory:\nls -la /vmfs/volumes/\u0026lt;data-store name\u0026gt;/\u0026lt;VM directory\u0026gt;/ For example:\nls -la /vmfs/volumes/datastore1/data-node/ Following is an example command output:\ntotal 419431552 drwxr-xr-x 1 root root 77824 Feb 26 08:10 . drwxr-xr-t 1 root root 73728 Feb 26 07:23 .. -rw------- 1 root root 429496729600 Feb 26 07:24 data-node-flat.vmdk -rw------- 1 root root 495 Feb 26 07:23 data-node.vmdk -rw-r--r-- 1 root root 0 Feb 26 07:23 data-node.vmsd -rwxr-xr-x 1 root root 2391 Feb 26 07:23 data-node.vmx -rw------- 1 root root 450098159616 Feb 26 08:10 ssd1-rdmp.vmdk -rw------- 1 root root 494 Feb 26 08:10 ssd1.vmdk -rw------- 1 root root 450098159616 Feb 26 08:10 ssd2-rdmp.vmdk -rw------- 1 root root 494 Feb 26 08:10 ssd2.vmdk -rw------- 1 root root 450098159616 Feb 26 08:10 ssd3-rdmp.vmdk -rw------- 1 root root 494 Feb 26 08:10 ssd3.vmdk -rw------- 1 root root 450098159616 Feb 26 08:10 ssd4-rdmp.vmdk -rw------- 1 root root 494 Feb 26 08:10 ssd4.vmdk   Attach each RDM data disk to the respective data-node VM by using the generated VMDK file:\n Log into the vSphere Web Client. Right-click the target data-node VM and select Edit Settings. Select: Add. On the Device Type page, select the device type Hard Disk, and then select Next. On the Select a Disk page, select Use an existing virtual disk to display the Datastore browser page. Browse to the target data-node VM directory.      Select the relevant VMDK file, and then choose Save on the Edit settings - \u0026lt;data-node VM\u0026gt; page.     Verifying the Procedure When you're done, for each data-node VM to which you attached devices, on all relevant ESXi hosts —\n  Open the VM console from the vSphere Web Client and run the following command to list the examples in the previous steps; you can optionally add options and filters to the command:\nlsblk You can also run the following command to check the disks' SCSI configuration:\nlsscsi   Verify that the data disks that you attached to the data node appear in the output, and save the disks' names. In the following example output, the data-node VM directory has four attached data disks — \u0026quot;sdb\u0026quot;, \u0026quot;sdc\u0026quot;, sdd\u0026quot;, and \u0026quot;sde\u0026quot;:\n     See Also  VMware vSphere installation guide On-Prem Deployment Specifications  ","keywords":["attach","data","disks","to","VMware","vSphere","vm","data","nodes,","attach","data","disks","to","vm","data","nodes,","vm","data-node","disk","attachment,","attach","SSDs","to","vm","data","nodes,","SSD","vm","data-node","attachment,","data","disks,","data","nodes,","solid","state","drives,","ssd,","raw","device","mapping,","rdm"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/data-disks-attach/","title":"Attaching Data Disks to the Data-Node VMs (vSphere)"},{"content":"Description A named JSON object that represents a NoSQL item attribute.\nSyntax \u0026#34;\u0026lt;attribute name\u0026gt;\u0026#34;: \u0026lt;attribute value\u0026gt; Elements  -- \u0026lt;attribute name\u0026gt; The attribute name. See Attribute Names for attribute naming guidelines.\n Type: String   -- \u0026lt;attribute value\u0026gt; An attribute-value object that provides information about the attribute's data type and value.\n Type: Attribute-Value object    Examples Define an attribute named username that signifies a username. The attribute type is a string (\u0026quot;S\u0026quot;), and its data value is \u0026quot;johnd\u0026quot;:\n{\u0026#34;username\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;johnd\u0026#34;}} Define an age attribute named age that signifies a user's age. The attribute type is a number (\u0026quot;N\u0026quot;), and its data value is \u0026quot;34\u0026quot;; note the numeric data value is represented as a string:\n{\u0026#34;age\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;34\u0026#34;}} Define an attribute named is_admin that signifies whether a user has administrator privileges. The attribute type is a Boolean value (\u0026quot;BOOL\u0026quot;), and its data value is false:\n{\u0026#34;is_admin\u0026#34;: {\u0026#34;BOOL\u0026#34;: false}} Define an age attribute named credentials that signifies encoded username and password login credentials. The attribute type is a number (\u0026quot;B\u0026quot;), and its data value is \u0026quot;iguazio:$apr1$YgrCYAYo$6v6iumigwirH4Jsdt4MWr0\u0026quot;; note the blob data value is represented as a string:\n{\u0026#34;credentials\u0026#34;: {\u0026#34;B\u0026#34;: \u0026#34;iguazio:$apr1$YgrCYAYo$6v6iumigwirH4Jsdt4MWr0\u0026#34;}} See Also  Objects Attributes Attribute Value Attribute Data Types Reference  ","keywords":["Attribute,","attribute","object,","attributes,","nosql,","json,","json","objects,","attribute","names,","names,","Attribute","Value,","attribute","values"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/common-objects/attribute/","title":"Attribute"},{"content":"Overview All data objects in the platform have attributes, which provide information (metadata) about the objects. See Objects Attributes. The platform supports the following attribute data types\nAPI-Specific Attribute TypesSome APIs, such as Spark DataFrames and Frames, require support for interface-specific data types and related type conversions to and from platform data types. Information about API-specific attribute data-types support is provided in the relevant API references. See, for example, the Spark DataFrames and Frames data-types references.   Boolean A Boolean value — true or false. Blob A Base64-encoded binary large object (blob). In the web APIs, a blob attribute is represented as a string.\nNoteThe web APIs also support array blob attributes. See Array Attributes.   Number One of the following numeric types:\n  Integer. For example, 0, 16, -5.\n  Floating point — a base-10 (b) 64-bit-encoded floating-point number format. The platform supports a 16-digit precision (p), but numbers with more than 14 significant digits may incur a bias. The exponent (e) values can be between -308 (Emin and +308 (Emax The following examples demonstrate the supported non-bias precision and exponent limits: 12345.678901234, -12345.678901234, 1.2345678901234e-308, 1.2345678901234e+308.\nFor more information about floating points, refer to the IEEE 754: Standard for Binary Floating-Point Arithmetic and the related Wikipedia page.\n  NoteIn some contexts, numeric attribute values are represented as strings, such as in the NoSQL Web API Attribute Value objects. Note that string floating numbers are not identical to true floating-point numbers. For example, \u0026quot;1\u0026quot; and \u0026quot;1.00\u0026quot; are not considered identical. Therefore, consider carefully how you define your floating-point number strings, and avoid comparisons of number strings. Before performing any comparisons or calculations, convert floating-point strings into actual floating-point numbers.   String A Unicode string with UTF-8 binary encoding.\n  Array Attributes The platform's web APIs support array attributes. An array attribute is a special type of blob attribute that the platform identifies as a Base64 encoded array. The current release supports numeric array attributes of type integer and double. In the web APIs, an array attribute, like all blob attributes, is represented as a Base64 encoded string. Array attributes are defined in SET update expressions and can be referenced in update expressions using the array operator ([ ]). For more information, see Array-Attribute Expression Variables.\nSee Also  Objects Attributes NoSQL Web API Attribute and Attribute Value Frames Attribute Data Types Spark DataFrame Data Types Array-Attribute Expression Variables  ","keywords":["attribute","data","types,","reference,","attributes,","data","types,","nosql,","Boolean,","blob,","number,","string,","integer,","floating","point,","IEEE","754,","precision,","16-digit","precision,","nosql","web","api,","Attribute,","Attribute","Value,","utf8,","string","encoding,","encoding,","spark","dataframe","attribute","types,","spark","data","types,","spark,","techpreview,","tech","preview,","arrays,","array","attributes,","blob","attributes,","array","operator,","[]","operator"],"path":"https://github.com/jasonnIguazio/data-layer/reference/data-types/attribute-data-types/","title":"Attribute Data Types Reference"},{"content":"Description A JSON object that represents the value of a NoSQL item attribute:\nSyntax {\u0026#34;\u0026lt;attribute data type\u0026gt;\u0026#34;: \u0026#34;\u0026lt;attribute data value\u0026gt;\u0026#34;} Elements  -- \u0026lt;attribute data type\u0026gt; The name of the Attribute Value object, which signifies the type of the attribute's data value.\n Type: String  The following attribute-value names (attribute data-type strings) are supported. See the Attribute Data Types Reference for documentation of the related attribute data types.\n   \u0026quot;S\u0026quot; — a string attribute.\n   \u0026quot;N\u0026quot; — a number attribute, represented as a string.\n   \u0026quot;BOOL\u0026quot; — a Boolean attribute.\n   \u0026quot;B\u0026quot; — a binary large object (blob) attribute, represented as a Base64-encoded string.\n   -- \u0026lt;attribute data value\u0026gt; The attribute's data value (\u0026quot;attribute value\u0026quot;). This value must match the data type signified by the Attribute Value object name (\u0026lt;attribute data type\u0026gt;).\n Type: String    Examples Define a string attribute value. The name of the attribute-value object is \u0026quot;S\u0026quot;, which signifies that the type of the attribute data is a string, and the value of the object (the attribute's data value) is \u0026quot;johnd\u0026quot;:\n{\u0026#34;S\u0026#34;: \u0026#34;johnd\u0026#34;} Define a numeric attribute value. The name of the attribute-value object is \u0026quot;N\u0026quot;, which signifies that the type of the attribute data is a number, and the value of the object (the attribute's data value) is \u0026quot;34\u0026quot;; note the numeric data value is represented as a string:\n{\u0026#34;N\u0026#34;: \u0026#34;34\u0026#34;} Define a Boolean attribute value. The name of the attribute-value object is \u0026quot;BOOL\u0026quot;, which signifies that the type of the attribute data is Boolean, and the value of the object (the attribute's data value) is false:\n{\u0026#34;BOOL\u0026#34;: false} Define a blob attribute value. The name of the attribute-value object is \u0026quot;B\u0026quot;, which signifies that the type of the attribute data is a blob, and the value of the object (the attribute's data value) is \u0026quot;iguazio:$apr1$YgrCYAYo$6v6iumigwirH4Jsdt4MWr0\u0026quot;; note the blob data value is represented as a string:\n{\u0026#34;B\u0026#34;: \u0026#34;iguazio:$apr1$YgrCYAYo$6v6iumigwirH4Jsdt4MWr0\u0026#34;} See Also  Objects Attributes Attribute Attribute Data Types Reference  ","keywords":["Attribute","Value,","attribute-value","object,","attribute","values,","attributes,","attribute","data","types,","data","types,","nosql,","json,","json","objects"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/common-objects/attribute-value/","title":"Attribute Value"},{"content":" This section contains installation and how-to guides for installing (deploying) and configuring the MLOps Platform (\u0026quot;the platform\u0026quot;) on an Amazon Web Services (AWS) cloud (including AWS Outposts).\n","keywords":["amazon","web","services","cloud","deployment,","aws","cloud","deployment,","amazon","web","services","deployment,","aws","deployment,","amazon","web","services","installation,","aws","installation,","aws","setup,","aws","cloud,","aws","outposts,","amazon","web","services,","aws"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/","title":"Amazon Web Services (AWS) Cloud Deployment"},{"content":" This section contains how-to guides for for deploying the MLOps Platform (\u0026quot;the platform\u0026quot;) on an Amazon Web Services (AWS) cloud (including AWS Outposts.\n","keywords":["aws","cloud","setup","how-to,","aws","setup","how-to,","aws","installation","how-to,","aws","how-to,","aws","outposts","how-to,","aws","setup,","aws","installation,","aws","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/","title":"AWS Cloud Deployment How-Tos"},{"content":" Overview This document lists the hardware specifications for deployment of version 3.2.1 of the MLOps Platform (\u0026quot;the platform\u0026quot;) on the Amazon Web Services (AWS) cloud; for details, refer to the AWS documentation.\nNote All references to AWS cloud apply also to AWS Outposts, except where otherwise specified. AWS platform deployments also require an Elastic IP address. For more information, see the AWS documentation. All capacity calculations in the hardware specifications are performed using the base-10 (decimal) number system. For example, 1 TB = 1,000,000,000,000 bytes.    Warning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Hardware Configurations The platform is available in two configurations, which differ in a variety of aspects, including the performance capacity, footprint, storage size, and scale capabilities:\n Development Kit A single data-node and single application-node cluster implementation. This configuration is designed mainly for evaluation trials and doesn't include high availability (HA) or performance testing. Operational Cluster A scalable cluster implementation that is composed of multiple data and application nodes. This configuration was designed to achieve superior performance that enables real-time execution of analytics, machine-learning (ML), and artificial-intelligence (AI) applications in a production pipeline. The minimum requirement for HA support is three data nodes and three application nodes.  Both configurations also support an additional backup node for backing up the platform instance.\nAWS Data-Node Specifications Data nodes in platform AWS cloud deployments must use one of the following EC2 instance types and fulfill the related specifications; choose the type that best fits your requirements.\n  i3.2xlarge Component  Specification   vCPUs 8  Memory 61 GiB  Data disks (local storage) 1 x 1.9 TB NVMe SSD  OS boot disk (EBS volume) General Purpose SSD (gp2); 400 GB (minimum)  Usable storage capacity 1 node (Development Cluster) — 1 TB; 3 nodes (Operational Cluster) — 2 TB     i3.4xlarge\nComponent  Specification   vCPUs 16  Memory 122 GiB  Data disks (local storage) 2 x 1.9 TB NVMe SSD  OS boot disk (EBS volume) General Purpose SSD (gp2); 400 GB (minimum)  Usable storage capacity Single node (Development Cluster) — 2.5 TB; 3 nodes (Operational Cluster) — 4 TB     i3.8xlarge Component  Specification   vCPUs 32  Memory 244 GiB  Data disks (local storage) 4 x 1.9 TB NVMe SSD  OS boot disk (EBS) General Purpose SSD (gp2); 400 GB (minimum)  Usable storage capacity 3 nodes (Operational Cluster) — 9 TB      AWS Outposts NoteFor deployment on AWS Outposts, currently only the i3en.6xlarge EC2 instance type is supported.  AWS Application-Node Specifications Application nodes in platform AWS cloud deployments must use one of the following EC2 instance types; choose the type that best fits your requirements. For specification details, refer to the AWS documentation. Elastic Kubernetes Service (EKS) is also supported for application nodes, including multiple node groups, using the instance types listed below.\nNoteAll of the supported application-node configurations also require a 250 GB (minimum) General Purpose SSD (gp2) OS boot disk (EBS volume).  CPU-Based Instances  m5.4xlarge (default configuration) m5.8xlarge m5.12xlarge m5.16xlarge m5.24xlarge   GPU-Based Instances  p3.2xlarge (trial only) p3.8xlarge p3.16xlarge g4dn.12xlarge g4dn.16xlarge  AWS Backup-Node Specifications (Optional) If you wish to back up your instance of the platform, you need an additional backup-node EC2 instance of type m5.4xlarge.\nNote It's strongly recommended that you back up your data on a regular basis. The backup node is used only for backups and can be shut down between backups to save costs. The backup node must have at least 2 TB of network-attached storage (NAS) to be used only for backup purposes. The exact amount of required storage depends on the amount of data that's being used in the platform; consult Iguazio's support team.    See Also  AWS Installation Guide High Availability (HA) Software Specifications and Restrictions Support and Certification Matrix  ","keywords":["aws","cloud","deployment","specifications,","aws","deployment","specifications,","amazon","web","services","deployment","specifications,","aws","deployment","specs,","aws","hardware","specifications,","amazon","web","services","hardware","specifications,","aws","hardware","specs,","aws","specifications,","amazon","web","services","specifications,","aws","specs,","deployment","specifications,","deployment","specs,","hardware","specifications,","hardware","specs,","hardware","configuration,","hardware,","specification,","spec"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/aws-hw-spec/","title":"AWS Cloud Deployment Specifications"},{"content":" Overview This guide outlines the required steps for installing (deploying) an instance of the MLOps Platform (\u0026quot;the platform\u0026quot;) to an Amazon Web Services (AWS) cloud (including AWS Outposts). When you complete the procedure, you'll have a platform instance running under your AWS account. The installation is done by using the platform installer — Provazio — with your AWS credentials.\nNote The deployment procedure requires proficiency in Systems Operations on AWS, and is typically completed in 1–2 hours. Do not use the AWS root user for any deployment operations.    Warning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Prerequisites Before you begin, ensure that you have the following:\n A Provazio API key and a Provazio vault URL, received from Iguazio. Administrative access to an AWS account. Confirmation from Iguazio's support team that platform Amazon Machine Images (AMIs) were configured with proper permissions for your AWS account. A machine running Docker. Access to the internet, or a preloaded Provazio Docker image (gcr.io/iguazio/provazio-dashboard:stable), received from Iguazio as an image archive (provazio-latest.tar.gz).  Deployment Steps To deploy an instance of the platform to an AWS cloud, execute the following steps.\nStep 1: Create an IAM user | Step 2: Create an AWS instance profile | Step 3: Configure the installation environment | Step 4: Run the platform installer | Step 5: Access the installer dashboard | Step 6: Choose the AWS scenario | Step 7: Configure general parameters | Step 8: Configure cluster parameters | Step 9: Configure cloud parameters | Step 10: Review the settings | Step 11: Wait for completion\nStep 1: Create an IAM User Follow the Creating an AWS IAM User guide to create a restricted AWS IAM user with the required credentials for performing the installation. Note that the IAM user is required only during the installation, and can be deleted after the installation, as explained in the post-deployment how-to. Step 2: Create an AWS Instance Profile Follow the Creating an AWS IAM Role and Instance Profile guide to create an AWS instance profile with a restricted IAM role that allows the platform's Amazon Elastic Compute Cloud (EC2) instances to call the AWS API.\nStep 3: Configure the Installation Environment Create a /tmp/env.yaml configuration file with the following environment information.\ndashboard: frontend: cloud_provider_regions: aws: - \u0026lt;AWS Region\u0026gt; client: infrastructure: ec2: access_key_id: \u0026lt;Access Key ID\u0026gt; secret_access_key: \u0026lt;Secret Access Key\u0026gt; data_cluster_instance_profile: IguazioDataScienceNode app_cluster_instance_profile: IguazioDataScienceNode vault: api_key: \u0026lt;Provazio API Key\u0026gt; url: \u0026lt;Provazio vault URL\u0026gt; provisioning: whitelisted_services: [\u0026#34;*\u0026#34;]  Replace the \u0026lt;...\u0026gt; placeholders with the information for your environment:\n AWS Region A list of one or more AWS regions that you'd like to choose from (for example, \u0026quot;us-east-2\u0026quot;).  Access Key ID The AWS Access Key ID for the IAM user created in Step 1.  Secret Access Key The AWS Secret Access Key for the IAM user created in Step 1.  Provazio API Key A Provazio API key, received from Iguazio (see the installation prerequisites).  Provazio Vault URL A Provazio vault URL, received from Iguazio (see the installation prerequisites).   Step 4: Run the Platform Installer Run the platform installer, Provazio, by running the following command from a command-line shell:\ndocker pull gcr.io/iguazio/provazio-dashboard:stable \u0026amp;\u0026amp; docker run --rm --name provazio-dashboard \\  -v /tmp/env.yaml:/tmp/env.yaml \\  -e PROVAZIO_ENV_SPEC_PATH=/tmp/env.yaml \\  -p 8060:8060 \\  gcr.io/iguazio/provazio-dashboard:stable Step 5: Access the Installer Dashboard In a web browser, browse to localhost:8060 to view the Provazio dashboard.    Select the plus-sign icon (+) to create a new system. Step 6: Choose the AWS Scenario On the Installation Scenario page, check AWS, and then select Next.    Step 7: Configure General Parameters On the General page, fill in the configuration parameters, and then select Next.     System Name A platform name (ID) of your choice (for example, \u0026quot;my-platform-0\u0026quot;). The installer prepends this value to the value of System Domain parameter to create the full platform domain.\n Valid Values: A string of 1–12 characters; can contain lowercase letters (a–z) and hyphens (-); must begin with a lowercase letter   Default Value: A randomly generated lowercase string   Description A free-text string that describes the platform instance.  System Version The platform version. This is auto-populated based on the AMIs that you have access to in the region, so make sure to set the Region parameter.\n Owner Full Name An owner-name string, containing the full name of the platform owner, for bookkeeping.  Owner Email An owner-email string, containing the email address of the platform owner, for bookkeeping.  Username The username of a platform user to be created by the installation. This username will be used together with the configured password to log into platform dashboard. You can add additional users after the platform is provisioned.\n User Password A platform password for the user generated by the installation — to be used with the configured username to log into platform dashboard; see the password restrictions. You can change this password after the platform is provisioned.\n Region The region in which to install the platform.  System Domain A custom platform domain (for example, \u0026quot;customer.com\u0026quot;). The installer prepends the value of the System Name parameter to this value to create the full platform domain.\n Allocate Public IP Addresses Check this option to allocate public IP addresses to all of the platform nodes (EC2 instances).  Termination Protection The protection level for terminating the platform installation from the installer dashboard.   Step 8: Configure Cluster Parameters On the Clusters page, fill in the configuration parameters, and then select Next. For additional information and guidelines, see the AWS resource-calculation guide guide.    Common Parameters (Data and Application Clusters) The following parameters are set for both the data and application clusters. Node references in the parameter descriptions apply to the platform's data nodes for the data cluster and application nodes for the application cluster.\n # of Nodes The number of nodes (EC2 instances) to allocate for the cluster.  Node Size The EC2 instance type, which determines the size of the clusters' nodes.  Root Block Device Type The Amazon Elastic Block Store (EBS) type for the control plane. \n Default Value: EBS General Purpose SSD (gp2), which provides a good balance between performance and cost. Note that the data plane uses high-speed NVMe storage.   Root Block Device Size The size of the EBS for the control plane.  Storage Encryption Kind [Tech Preview] The type of encryption to be applied.   Application-Cluster Parameters The following parameters are applicable only to the platform's application cluster.\nEKS Application-Cluster NoteThe following instructions are specific to deployment of a managed vanilla application cluster. To deploy an Amazon Elastic Kubernetes Service (Amazon EKS) application cluster, follow the Deploying an Amazon EKS Application Cluster guide instead.   Kubernetes Kind Leave this set to New Vanilla Cluster (Iguazio Managed).   Step 9: Configure Cloud Parameters On the Cloud page, fill in the configuration parameters, and then select Next.\n VPC mode The cloud configuration configures the platform's virtual private cloud (VPC) networking. You can select between two alternative VPC modes:\n New — Create a new VPC and install the platform in this VPC. Existing — Install the platform in an existing VPC.    The following optional parameters are applicable to both VPC modes; (see the example UI screen shots for the different VPC-mode configurations later in this step):\n Region Name Overrides the value of the Region general-configuration parameter.\n Access Key ID Overrides the value of the Access Key ID environment-configuration parameter.\nNoteThis parameter should typically not be set.   Secret Access Key Overrides the value of the Secret Access Key environment-configuration parameter.\nNoteThis parameter should typically not be set. If you find the need to set it, consult Iguazio personnel first.   Verbose Provisioning Configures very verbose logs.\nNoteLeave this parameter unchecked unless instructed otherwise by Iguazio personnel.   Placement Kind An AWS Placement Group.\nNoteDon't change the default value of this parameter unless instructed otherwise by Iguazio personnel.    Security-Group Parameters The following parameters are used for configuring network security groups. For more information, see the AWS network security-groups configuration guide.\n Whitelisted CIDRs A list of classless inter-domain routing (CIDR) addresses to be granted access to the platform's service port (for example, \u0026quot;200.40.0.1/32\u0026quot;). This parameter is typically relevant when the platform has public IP addresses. For a platform without public IP addresses, you can leave this parameter empty, assuming you have access to the VPC from your network.  Installer CIDR The CIDR of the machine on which you're running the platform installer (for example, \u0026quot;10.0.0.1/32\u0026quot;).  Allow Access from Iguazio Support Check this option to allow Iguazio's support team to access the platform nodes from the Iguazio network This parameter is applicable only when the platform has public IP addresses (see the Allocate Public IP Addresses general-configuration parameter).\n  In addition to the common parameters, there are parameters that are specific to the selected VPC mode:\n New-VPC configuration Existing-VPC configuration  New-VPC Configuration The following parameters are applicable only to the New VPC mode:\n CIDR The CIDR of the VPC.  Subnet CIDRs The CIDRs of the VPC's subnets. The number of CIDRs translates to the number of subnets.\nNoteFor a managed vanilla application cluster, you can currently configure only one subnet, which ensures that all platform nodes use the same availability zone. A deployment with a single availability zone allows minimal latency and doesn't incur data-transfer costs. When deploying an EKS application cluster, you need to configure two subnets (for two availability zones) to fulfill EKS requirements; however, the platform uses only the first configured subnet. To use multiple availability zones (via multiple subnets), contact Iguazio for a quote. Note that while deployment with multiple availability zones offers improved availability when an availability zone is down, it has a performance impact and entails high network-utilization costs, and therefore might not fit your requirements.        Existing-VPC Configuration The following parameters are applicable only to the Existing VPC mode:\n VPC ID The ID of the VPC in which to install the platform.  CIDR The IP address of the CIDR of the chosen VPC (as some VPCs have multiple CIDRs).  Subnet IDs The IDs of the subnets within the VPC or of a subset of these subnets. The installation currently supports two subnets for an EKS application cluster and only a single subnet otherwise. For details, see the note for the Subnet CIDRs new-VPC configuration parameter.\n Security Group Mode Leave this set to New.       Step 10: Review the Settings On the Review page, review and verify your configuration; go back and make edits, as needed; and then select Create to provision a new instance of the platform.\n   Step 11: Wait for Completion Provisioning a new platform instance typically takes around 30–40 minutes, regardless of the cluster sizes. You can download the provisioning logs, at any stage, by selecting Download logs from the instance's action menu.\n   You can also follow the installation progress by tracking the Provazio Docker container logs.\nWhen the installation completes, you should have a running instance of the platform in your cloud. You can use the Provazio dashboard to view the installed nodes (EC2 instances). Then, proceed to the post-deployment steps. Post-Deployment Steps When the deployment completes, follow the post-deployment steps.\nSee Also  AWS Deployment How-Tos Post-Deployment How-Tos AWS Deployment Specifications  ","keywords":["aws","installation,","aws","cloud","installation,","aws","outposts","installation,","aws","deployment,","aws","cloud","deployment,","provazio,","platform","installer,","aws","roles,","iam","roles,","aws","instance","profiles,","aws","network,","aws","vpcs,","vpcs,","aws","ebs,","ebs,","cidrs"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/installationguides/aws-installation-guide/","title":"Installing the Platform on an AWS Cloud"},{"content":"Overview After following the platform's AWS cloud installation guide, you should have a running instance of the platform in your AWS cloud. In some cases, additional post-deployment steps are required before you can properly use the platform, as outlined in this guide.\nRegistering a Custom Platform Domain If you chose to install the platform under a custom domain, you must register a few DNS records. If you need assistance, contact Iguazio's support team. Creating an HTTPS Certificate In some cases, you might need to create an HTTPS certificate for your platform installation. For more information, contact Iguazio's support team. Importing IdP Users and Groups from an Active Directory To import users and groups from an external Microsoft Active Directory, see the platform's IdP documentation. For additional assistance, contact Iguazio's support team. See Also  AWS cloud installation guide  ","keywords":["aws","cloud","post","deployment,","aws","post","deployment,","aws","post","installation,","cudsom","domain","regsitration,","domain","registration,","ip","addresses,","network,","dns,","idp,","microsoft","active","directory,","active","directory,","microsoft","ad,","idp","users,","http","certificates"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/post-install-steps/","title":"Post-Installation Steps (AWS)"},{"content":" This section contains installation and how-to guides for installing (deploying) and configuring the MLOps Platform (\u0026quot;the platform\u0026quot;) on a Microsoft Azure cloud.\n","keywords":["microsoft","azure","cloud","deployment,","azure","cloud","deployment,","azure","deployment,","azure","installation,","azure","setup,","microsoft","azure","cloud,","microsoft","azure,","azure"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/","title":"Microsoft Azure Cloud Deployment"},{"content":" This section contains configuration how-to guides for deploying the MLOps Platform (\u0026quot;the platform\u0026quot;) on a Microsoft Azure cloud.\n","keywords":["azure","cloud","setup","how-to,","azure","setup","how-to,","azure","installation","how-to,","azure","how-to,","azure","setup,","azure","installation,","azure","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/howto/","title":"Azure Cloud Deployment How-Tos"},{"content":" This section contains installation guides for the MLOps Platform (\u0026quot;the platform\u0026quot;) on a Microsoft Azure cloud.\n","keywords":["azure","cloud","setup","how-to,","azure","setup","how-to,","azure","installation","how-to,","azure","how-to,","azure","setup,","azure","installation,","azure","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/installationguides/","title":"Azure Cloud Deployment How-Tos"},{"content":" Overview This document lists the hardware specifications for deployment of version 3.2.1 of the MLOps Platform (\u0026quot;the platform\u0026quot;) on the Microsoft Azure cloud; for details, refer to the Azure documentation.\nNoteAll capacity calculations in the hardware specifications are performed using the base-10 (decimal) number system. For example, 1 TB = 1,000,000,000,000 bytes.  Warning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Hardware Configurations The platform is available in two configurations, which differ in a variety of aspects, including the performance capacity, footprint, storage size, and scale capabilities:\n Development Kit A single data-node and single application-node cluster implementation. This configuration is designed mainly for evaluation trials and doesn't include high availability (HA) or performance testing. Operational Cluster A scalable cluster implementation that is composed of multiple data and application nodes. This configuration was designed to achieve superior performance that enables real-time execution of analytics, machine-learning (ML), and artificial-intelligence (AI) applications in a production pipeline. The minimum requirement for HA support is three data nodes and three application nodes.  Both configurations also support an additional backup node for backing up the platform instance.\nNoteAzure platform deployments also require a static public IP address. For more information, see the Azure documentation.  Azure Data-Node Specifications Data nodes in platform Azure cloud deployments must fulfill the following hardware specification requirements:\nComponent  Specification   Instance type Standard_L16s_v2  vCPUs 16  Memory 128 GB  Data disks (local storage) 2 x 1.9 TB NVMe SSD  OS boot disk (Azure managed disk) Premium SSD; 400 GB (minimum)  Usable storage capacity 1 node (Development Cluster) — 2.5 TB; 3 nodes (Operational Cluster) — 4 TB   Azure Application-Node Specifications Application nodes in platform Azure cloud deployments must use one of the following instance types; choose the type that best fits your requirements. Azure Kubernetes Service (AKS) is also supported for application nodes, using the instance types listed below. For specification details for each type, refer to the Azure documentation.\nNoteAll of the supported application-node configurations also require a 250 GB (minimum) premium-SSD OS boot disk (Azure managed disk).   Standard_D16s_v3 (default configuration) Standard_D32s_v3 Standard_D48s_v3 Standard_D64s_v3 NCv3-series (GPU optimized)   Azure Backup-Node Specifications (Optional) If you wish to back up your instance of the platform, you need an additional backup-node instance of type Standard_D16s_v3.\nNote It's strongly recommended that you back up your data on a regular basis. The backup node is used only for backups and can be shut down between backups to save costs. The backup node must have at least 2 TB of network-attached storage (NAS) to be used only for backup purposes. The exact amount of required storage depends on the amount of data that's being used in the platform; consult Iguazio's support team.    See Also  Azure Installation Guide High Availability (HA) Software Specifications and Restrictions Support and Certification Matrix  ","keywords":["azure","cloud","deployment","specifications,","azure","deployment","specifications,","azure","deployment","specs,","azure","hardware","specifications,","azure","hardware","specs,","azure","specifications,","azure","specs,","deployment","specifications,","deployment","specs,","hardware","specifications,","hardware","specs,","hardware","configuration,","hardware,","specification,","spec"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/azure-hw-spec/","title":"Azure Cloud Deployment Specifications"},{"content":" Overview This guide outlines the required steps for installing (deploying) an instance of the MLOps Platform (\u0026quot;the platform\u0026quot;) to a Microsoft Azure cloud. When you complete the procedure, you'll have a platform instance running under your Azure account.\nTroubleshootingIf you run into issues during the installation, check out the platform's Azure-installation troubleshooting guide. For further assistance, contact Iguazio's support team.  Warning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Prerequisites Before you begin, ensure that you have the following:\n A Provazio API key and a Provazio vault URL, received from Iguazio. An Azure subscription ID. An Azure Resource Manager template file for deploying the platform mainTemplate.json. An Azure location (for example, \u0026quot;eastus2\u0026quot;) that's capable of provisioning the number and overall size of the Azure instance types (VM sizes) that you plan to use from among those supported by the platform—see the Azure resource-calculation guide. The default data-node size is Standard_L16s_v2, and the default application-node size is Standard_D16s_v3. You can configure the application-node size in Step 4 (see the appVmSize parameter). A working Azure CLI.  Preparing to Install Start out by performing the preliminary steps outlined in the Pre-Installation Steps Using the Azure CLI guide.\nDeployment Steps To deploy an instance of the platform in the Azure cloud, execute the following steps from a command-line shell that has the Azure CLI (installed as part of the pre-installation steps).\nStep 1: Accept the platform terms | Step 2: Create an Azure resource group | Step 3 (Optional): Create an Azure service principal | Step 4: Deploy the platform\nStep 1: Accept the Platform Terms Run the following Azure CLI command to accept the platform terms and conditions. Replace \u0026lt;Azure subscription ID\u0026gt; with your Azure subscription ID.\naz vm image terms accept \\  --offer iguazio-data-science-platform-vm \\  --plan iguazio-data-science-platform-vm \\  --publisher iguazio-5069960 \\  --subscription \u0026lt;Azure subscription ID\u0026gt; Step 2: Create an Azure Resource Group Run the following Azure CLI command to create a new Azure resource group. Replace \u0026lt;location\u0026gt; with the name of your Azure location, and \u0026lt;resource-group name\u0026gt; with the name of the resource group that you want to create.\naz group create --location \u0026lt;location\u0026gt; --name \u0026lt;resource-group name\u0026gt; For example, the following command creates a resource group named \u0026quot;my-resource-group-0\u0026quot; for location \u0026quot;eastus2\u0026quot;:\naz group create --location eastus2 --name my-resource-group-0 Step 3 (Optional): Create an Azure Service Principal By default, the installer grants itself Contributor access to the resource group of the VNet in which the platform is provisioned, and you can safely skip this step. However, if you want to install the platform in an existing VNet that resides in a different resource group than that used for the platform deployment (created in Step 2), you must create an Azure service principal; save its tenant ID, subscription ID, client ID, and client secret; and provide this information to the platform installer as part of the deployment (see Step 4).\nNoteThe service principal must have Contributor roles in both the resource group containing the VNet and the resource group in which the platform is provisioned.  Step 4: Deploy the Platform Run the following Azure CLI command to the start deploying a new platform instance.\naz deployment group create \\  --resource-group \u0026lt;Azure Resource Group\u0026gt; \\  --template-file \u0026lt;Resource Template\u0026gt; \\  --name \u0026lt;Deployment Name\u0026gt;\\  --parameters \u0026lt;Deployment Parameters\u0026gt; Replace the \u0026lt;...\u0026gt; placeholders with the information for your environment:\n Azure Resource Group The name of the Azure resource group that you created in Step 2.  Resource Template Path to your mainTemplate.json Azure Resource Manager template file (see the installation prerequisites).  Deployment Name A unique Azure deployment name (for example, \u0026quot;iguazio-deployment-0\u0026quot;), which is required by the Azure CLI. Note that platform identifies deployment instances by their custom platform name (ID) — see the systemId deployment parameter.  Deployment Parameters Additional deployment parameters, as outlined in the following subsection.   Deployment Parameters The --parameters deployment flag allows you to set additional deployment parameters for configuring various installation settings. The parameters are passed as a string containing space-separated \u0026lt;parameter name\u0026gt;=\u0026lt;parameter value\u0026gt; key-value pairs.\n Required Deployment Parameters Optional Deployment Parameters  Required Deployment Parameters The following deployment parameters are required\n apiKey A Provazio API key, received from Iguazio (see the installation prerequisites).  vaultUrl A Provazio vault URL, received from Iguazio (see the installation prerequisites).  adminUsername A username for logging into the platform dashboard. More users can be added later.  adminPassword A user password for logging into platform dashboard; see the password restrictions. This can be changed later.   Optional Deployment Parameters  ownerName A free-text platform-owner string that contains your name or email address, for bookkeeping.  systemId A platform name (ID) of your choice (for example, \u0026quot;my-platform-0\u0026quot;). The installer prepends this value to the value of systemDomain parameter to create the full platform domain.\n Valid Values: A string of 1–12 characters; can contain lowercase letters (a–z) and hyphens (-); must begin with a lowercase letter   Default Value: A randomly generated lowercase string   systemDomain A custom platform domain (for example, \u0026quot;my-domain.com\u0026quot;). The installer prepends the value of the systemId parameter to this value to create the full platform domain. \n Default Value: \u0026quot;iguazio-c0.com\u0026quot;   vnetName The name of an existing VNet in which to provision the platform. \n Default Behavior: If this parameter isn't set, a new VNet named \u0026quot;\u0026lt;system ID\u0026gt;-vnet\u0026quot; is created.   vnetSubnetName The name of the subnet in which to provision the platform. \n Default Behavior: If this parameter isn't set, a new subnet named \u0026quot;\u0026lt;system ID\u0026gt;-subnet\u0026quot; is created.   vnetResourceGroup The resource group of the configured platform VNet (see the vnetName parameter). To set this parameter, you must first create an Azure service principal, as outlined in Step 3.\n Default Value: The resource group that's used for the platform deployment (--resource-group Azure Resource Group)\n   vnetAddressPrefix The CIDR of the newly created VNet; applicable only when the vnetName parameter isn't set (resulting in the creation of a new VNet). \n Default Value: \u0026quot;172.38.0.0/16\u0026quot;   numDataNodes The number of platform data nodes (VMs). \n Valid Values: 1 or 3   numAppNodes The number of platform application nodes (VMs). \n Valid Values: 1–N   appVmSize Application-node size, as an Azure general-purpose VM size. For the supported sizes, see the Azure resource-calculation guide. \n Default Value: Standard_D16s_v3   whitelistedCidrs A list of classless inter-domain routing (CIDR) addresses to be granted access to the platform's service port (for example, \u0026quot;10.0.0.0/16,40.10.10.31/32\u0026quot;). This parameter is typically relevant when the platform has public IP addresses (when allocatePublicIpAddresses is set to true). \n Default Value: An empty list (\u0026quot;\u0026quot;)   allocatePublicIpAddresses Set to true to allocate public IP addresses for all platform nodes (VMs). \n Default Value: false   whitelistIguazioNetwork Set to true to allow Iguazio's support team to access the platform nodes from the Iguazio network. This parameter is applicable only when the platform has public IP addresses (see the allocatePublicIpAddresses parameter).\n Default Value: false   appClusterKubernetesKind This parameter determines the type of Kubernetes cluster. There are two values that can be used:\n vanilla—a standard AKS deployment aks—a versioned deployment of AKS  Note Do not include this parameter if you are using a vanilla installation. If you selected AKS, you must use the appClusterKubernetesVersion parameter.     appClusterKubernetesVersion The version currently supported by AKS.   Security-Principal Parameters If you created a service principal (see Step 3), you must also set the following parameters:\n spTenantId The tenant ID of the service principal.  spSubscriptionId The subscription ID of the service principal (your Azure subscription ID).  spClientId The client iD of the service principal.  spClientSecret The client secret of the service principal.   Example az deployment group create \\  --resource-group my-resource-group-0 \\  --template-file /home/installer/mainTemplate.json \\  --name iguazio-deployment-0 \\  --parameters apiKey=\u0026#39;myapikey\u0026#39; vaultUrl=\u0026#39;https://vault.trial.provazio.iguazio.com\u0026#39; adminUsername=admin adminPassword=mypassword ownerName=\u0026#39;John Doe\u0026#39; systemId=my-platform-0 systemDomain=my-domain.com allocatePublicIpAddresses=true whitelistIguazioNetwork=true Deployment Note The deployment requires the command-line shell to remain open only until a \u0026quot;Running\u0026quot; message is displayed (typically, approximately 10 minutes after running the deployment command). The deployment takes approximately two hours. The Azure CLI has a fixed timeout period of 1.5 hours, so the command line shows a timeout indication during the deployment process, even though the deployment is still running. This is the expected behavior and no action is needed on your part. After Iguazio's support engineers confirm that the deployment completed successfully, they will guide you on how to log into the platform, and Iguazio's customer-success team will initiate a getting-started session to help you with your first steps.\nPost-Deployment Steps When the deployment completes, follow the post-deployment steps.\nSee Also  Azure Deployment How-Tos Post-Deployment How-Tos Azure Deployment Specifications  ","keywords":["azure","installation,","azure","cloud","installation,","azure","deployment,","azure","cloud","deployment,","provazio,","platform","installer"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/installationguides/azure-installation-guide/","title":"Installing the Platform on a Microsoft Azure Cloud"},{"content":" Overview This guide outlines the required steps for installing (deploying) an instance of the MLOps Platform (\u0026quot;the platform\u0026quot;) to a Microsoft Azure cloud. When you complete the procedure, you will have a platform instance running under your Azure account.\nTroubleshootingIf you run into issues during the installation, see the platform's Azure-installation troubleshooting guide. For further assistance, contact Iguazio's support team.  Warning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Prerequisites Before you begin, ensure that you have the following:\n A Provazio API key and a Provazio vault URL, provided by Iguazio. An Azure subscription ID. An Azure Resource Manager template file for deploying the platform mainTemplate.json. An Azure location (For example, \u0026quot;eastus2\u0026quot;) that's capable of provisioning the number and overall size of the Azure instance types (VM sizes) that you plan to use from among those supported by the platform. For reference, see the Azure resource-calculation guide. The default data-node size is Standard_L16s_v2. Additional VMs are required for the Azure AKS cluster. A working Azure CLI.  Preparing to Install Start out by performing the preliminary steps outlined in the Pre-Installation Steps Using the Azure CLI guide.\nDeployment Steps To deploy an instance of the platform in the Azure cloud, execute the following steps from a command-line shell that has the Azure CLI (installed as part of the pre-installation steps).\nStep 1: Accept the platform terms | Step 2: Create an Azure resource group | Step 3: Create an Azure user assigned managed identity | Step 4 (Optional): Create an Azure service principal | Step 5: Deploy the platform\nStep 1: Accept the Platform Terms Run the following Azure CLI command to accept the platform terms and conditions. Replace \u0026lt;Azure subscription ID\u0026gt; with your Azure subscription ID.\naz vm image terms accept \\  --offer iguazio-data-science-platform-vm \\  --plan iguazio-data-science-platform-vm \\  --publisher iguazio-5069960 \\  --subscription \u0026lt;Azure subscription ID\u0026gt; Step 2: Create an Azure Resource Group Run the following Azure CLI command to create a new Azure resource group. Replace \u0026lt;location\u0026gt; with the name of your Azure location, and \u0026lt;resource-group name\u0026gt; with the name of the resource group that you want to create.\naz group create --location \u0026lt;location\u0026gt; --name \u0026lt;resource-group name\u0026gt; For example, the following command creates a resource group named \u0026quot;my-resource-group-0\u0026quot; for location \u0026quot;eastus2\u0026quot;:\naz group create --location eastus2 --name my-resource-group-0 Step 3 : Create an Azure User Assigned Managed Identity Follow the following guide and create a User Assigned Managed Identity. Create the User Assigned Managed Identity inside the Resource Group created in Step 2. Provide the User Assigned Managed Identity a \u0026quot;Contributor Role\u0026quot; to the Resource Group Step 2. If the vnet used in the deployment is in another Resource Group, Contributor Role should be provided to the vnet's Resource Group.\nStep 4 (Optional): Create an Azure Service Principal By default, the installer grants itself Contributor access to the resource group of the VNet in which the platform is provisioned, and you can safely skip this step. However, if you want to install the platform in an existing VNet that resides in a different resource group than that used for the platform deployment (created in Step 2), you must create an Azure service principal; save its tenant ID, subscription ID, client ID, and client secret; and provide this information to the platform installer as part of the deployment (see Step 4).\nNoteThe service principal must have Contributor roles in both the resource group containing the VNet and the resource group in which the platform is provisioned.  Step 5: Deploy the Platform Run the following Azure CLI command to the start deploying a new platform instance.\naz deployment group create \\  --resource-group \u0026lt;Azure Resource Group\u0026gt; \\  --template-file \u0026lt;Resource Template\u0026gt; \\  --name \u0026lt;Deployment Name\u0026gt; \\  --parameters apiKey=\u0026lt;API Key\u0026gt; adminUsername=\u0026lt;User Name\u0026gt; adminPassword=\u0026lt;Password\u0026gt; vaultUrl=\u0026lt;Vault URL\u0026gt; systemId=\u0026lt;System ID\u0026gt; allocatePublicIpAddresses=\u0026lt;true/fales\u0026gt; whitelistedCidrs=\u0026lt;IP list\u0026gt; numDataNodes=\u0026lt;number of data nodes\u0026gt; systemDomain=\u0026lt;system domain\u0026gt; systemVersion=\u0026lt;Platform Version\u0026gt; appClusterKubernetesKind=aks appClusterKubernetesVersion=1.19.11 userAssignedManagedIdentity=\u0026lt;link to the Assigned Managed Identity\u0026gt; \u0026#39;appClusterKubernetesNodeGroups=\u0026lt;Node Groups Details\u0026gt;\u0026#39; Replace the \u0026lt;...\u0026gt; placeholders with the information for your environment:\n Azure Resource Group The name of the Azure resource group that you created in Step 2.  Resource Template Path to your mainTemplate.json Azure Resource Manager template file (see the installation prerequisites).  Deployment Name A unique Azure deployment name (for example, \u0026quot;iguazio-deployment-0\u0026quot;), which is required by the Azure CLI. Note that platform identifies deployment instances by their custom platform name (ID)—see the systemId deployment parameter.  Deployment Parameters  apiKey A Provazio API key, received from Iguazio (see the installation prerequisites).  vaultUrl A Provazio vault URL, received from Iguazio (see the installation prerequisites).  adminUsername A username for logging into the platform dashboard. More users can be added later.  adminPassword A user password for logging into platform dashboard; see the password restrictions. This can be changed later.    systemId A platform name (ID) of your choice (for example, \u0026quot;my-platform-0\u0026quot;). The installer prepends this value to the value of systemDomain parameter to create the full platform domain.\n Valid Values: A string of 1–12 characters; can contain lowercase letters (a–z) and hyphens (-); must begin with a lowercase letter   Default Value: A randomly generated lowercase string   systemDomain A custom platform domain (for example, \u0026quot;my-domain.com\u0026quot;). The installer prepends the value of the systemId parameter to this value to create the full platform domain. \n Default Value: \u0026quot;iguazio-c0.com\u0026quot;   systemVersion Platform Version (to be recieved from Iguazio's support team).  vnetName The name of an existing VNet in which to provision the platform. \n Default Behavior: If this parameter isn't set, a new VNet named \u0026quot;\u0026lt;system ID\u0026gt;-vnet\u0026quot; is created.   vnetSubnetName The name of the subnet in which to provision the platform. \n Default Behavior: If this parameter isn't set, a new subnet named \u0026quot;\u0026lt;system ID\u0026gt;-subnet\u0026quot; is created.   vnetResourceGroup The resource group of the configured platform VNet (see the vnetName parameter). To set this parameter, you must first create an Azure service principal, as outlined in Step 4.\n Default Value: The resource group that's used for the platform deployment (--resource-group Azure Resource Group)\n   vnetAddressPrefix The CIDR of the newly created VNet; applicable only when the vnetName parameter isn't set (resulting in the creation of a new VNet). \n Default Value: \u0026quot;172.38.0.0/16\u0026quot;   numDataNodes The number of platform data nodes (VMs). \n Valid Values: 1 or 3   whitelistedCidrs A list of classless inter-domain routing (CIDR) addresses to be granted access to the platform's service port (for example, \u0026quot;10.0.0.0/16,40.10.10.31/32\u0026quot;). This parameter is typically relevant when the platform has public IP addresses (when allocatePublicIpAddresses is set to true). \n Default Value: An empty list (\u0026quot;\u0026quot;)   allocatePublicIpAddresses Set to true to allocate public IP addresses for all platform nodes (VMs). \n Default Value: false   whitelistIguazioNetwork Set to true to allow Iguazio's support team to access the platform nodes from the Iguazio network. This parameter is applicable only when the platform has public IP addresses (see the allocatePublicIpAddresses parameter).\n Default Value: false   appClusterKubernetesKind This parameter determines the type of Kubernetes cluster. Set to AKS.\nappClusterKubernetesVersion The Kubernetes version received from Iguazio.  appClusterKubernetesNodeGroups The node pool/s details to be created. An initial node pool must be created with a minimum of 1 Application Node that will not be scaled down. Additional node pools can be created with minimum of 0 nodes.\n Value Example: 'appClusterKubernetesNodeGroups=initial:2,Standard_D16s_v3;added0:0,0,2,Standard_NC6s_v3' will create an initial node pool of 2 Applicaiton nodes and a Node Pool of a minimum of 0 nodes and a maximum of 2 nodes from the Azure NC-series VMs.   userAssignedManagedIdentity In Azure Console go to \u0026quot;Managed Identities\u0026quot; -\u0026gt; Identity created in Step 3 -\u0026gt; Overview -\u0026gt; JSON view. Copy the \u0026quot;id\u0026quot;: URL and use it with this parameter.\n Value Example: userAssignedManagedIdentity=/subscriptions/8d81bc0b-6abd-4395-be83-000251d9fdbe/resourcegroups/example/providers/Microsoft.ManagedIdentity/userAssignedIdentities/example-mgmt-id    Security-Principal Parameters If you created a service principal (see Step 4), you must also set the following parameters:\n spTenantId The tenant ID of the service principal.  spSubscriptionId The subscription ID of the service principal (your Azure subscription ID).  spClientId The client iD of the service principal.  spClientSecret The client secret of the service principal.   Example az deployment group create \\  --resource-group rg-example \\  --template-file mainTemplate.json \\  --name example1 \\  --parameters apiKey=xKsaG34ED8pa9rSUSexaVzkaQxj2T6g42P9UZTwy5FQ9Gmc adminUsername=admin adminPassword=TempPass123! vaultUrl=https://vault.trial.provazio.iguazio.com systemId=example allocatePublicIpAddresses=true whitelistedCidrs=0.0.0.0/0 whitelistIguazioNetwork=true numDataNodes=1 systemDomain=iguazio-c0.com systemVersion=3.0_b177_20210806003728 appClusterKubernetesKind=aks appClusterKubernetesVersion=1.19.11 userAssignedManagedIdentity=/subscriptions/8d81bc0b-6abd-4395-be83-000251d9fdbe/resourcegroups/example/providers/Microsoft.ManagedIdentity/userAssignedIdentities/example-mgmt-id \u0026#39;appClusterKubernetesNodeGroups=initial:2,Standard_D16s_v3;added0:0,0,2,Standard_NC6s_v3\u0026#39; Deployment Note The deployment requires the command-line shell to remain open only until a \u0026quot;Running\u0026quot; message is displayed (typically, approximately 10 minutes after running the deployment command). The deployment takes approximately two hours. The Azure CLI has a fixed timeout period of 1.5 hours, so the command line shows a timeout indication during the deployment process, even though the deployment is still running. This is the expected behavior and no action is needed on your part. After Iguazio's support engineers confirm that the deployment completed successfully, they will guide you on how to log into the platform, and Iguazio's customer-success team will initiate a getting-started session to help you with your first steps.\nPost-Deployment Steps When the deployment completes, follow the post-deployment steps.\nSee Also  Azure Deployment How-Tos Post-Deployment How-Tos Azure Deployment Specifications   ","keywords":["azure","installation,","azure","cloud","installation,","azure","deployment,","azure","cloud","deployment,","provazio,","platform","installer"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/installationguides/azure-installation-guide-aks/","title":"Installing the Platform on a Microsoft Azure Cloud With AKS"},{"content":"Overview After following the platform's Azure cloud installation guide, you should have a running instance of the platform in your Azure cloud. In some cases, additional post-deployment steps are required before you can properly use the platform, as outlined in this guide.\nRegistering a Custom Platform Domain If you chose to install the platform under a custom domain, you must register a few DNS records. If you need assistance, contact Iguazio's support team. Creating an HTTPS Certificate In some cases, you might need to create an HTTPS certificate for your platform installation. For more information, contact Iguazio's support team. Importing IdP Users and Groups from an Active Directory To import users and groups from an external Microsoft Active Directory, see the platform's IdP documentation. For additional assistance, contact Iguazio's support team. See Also  Azure cloud installation guide  ","keywords":["azure","cloud","post","deployment,","azure","post","deployment,","azure","post","installation,","cudsom","domain","regsitration,","domain","registration,","ip","addresses,","network,","dns,","idp,","microsoft","active","directory,","active","directory,","microsoft","ad,","idp","users,","http","certificates"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/howto/post-install-steps/","title":"Post-Installation Steps (Azure)"},{"content":" Overview When creating a new collection — namely, a NoSQL table or a directory in the file system — you should select a primary key for uniquely identifying the objects in the collection; (for streaming, this is handled implicitly by the platform). An object's primary key serves also as the object's name (i.e., the name of the object file) and is stored by the platform in the __name system attribute. The primary-key value of a data object (such as a table item or a file) is composed of a sharding key — which determines the data shard (slice) on which the object is physically stored — and optionally also a sorting key — which determines how objects with the same sharding key are sorted on the assigned slice. For more information about object keys and names, see Object Names and Primary Keys.\nYour aim should be to select a primary key that results in an even distribution of the data and the data-access workload across the available data slices. This should be done by taking into account the collection's expected data set and ingestion and consumption patterns. Following are best-practice guidelines to help you in this mission.\nUsing a Compound Primary Key for Faster NoSQL Table Queries A major motivation for using a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key for a NoSQL-table collection is to optimize query performance. A compound key enables the platform to support optimized range-scan and item-specific queries that include sharding- and sorting-key filters — see NoSQL read optimization. Such queries can be significantly faster than the standard full-table queries, because the platform searches the names of the clustered and sorted object files on the relevant data slice instead of scanning all table items. Range scans are most beneficial if the majority of the table queries are expected to include a common filter criteria that consists of the same one or two attributes. Therefore, when selecting your keys, consider which queries are likely to be issued most often. For example, if you have a connected-cars application and most of your queries will include car-ID and trip-ID filter criteria — such as \u0026quot;car_id = \u0026lt;value\u0026gt;\u0026quot;, \u0026quot;car_id = \u0026lt;value\u0026gt; AND trip_id = \u0026lt;value\u0026gt;\u0026quot;, or \u0026quot;car_id = \u0026lt;value\u0026gt; AND trip_id \u0026gt; \u0026lt;value\u0026gt; AND trip_id \u0026lt;= \u0026lt;value\u0026gt;\u0026quot; — it would make sense to use the car-ID attribute as your table's sharding key and the trip-ID attribute as the sorting key, resulting in a \u0026lt;car_id\u0026gt;.\u0026lt;trip_id\u0026gt; primary key. Using a String Sorting Key for Faster Range-Scan Queries To support Presto and Spark DataFrame range-scan queries, the table must have sharding- and sorting-key user attributes. For faster range scans, use sorting-key attributes of type string: for a string sorting key, after identifying the relevant data slice by using the query's sharding-key value, the platform scans only the items within the query's sorting-keys range; but for a non-string sorting-key, the platform ignores the sorting key and scans all items on the data slice. The reason for ignoring non-string sorting keys is that the lexicographic sorting-key sort order that's used for storing the items on the data slice might not match the logical sort order for a non-string sorting key.\nSelecting a Sharding Key for Even Workload Distribution As explained, a collection's sharding key affects the distribution of the data among the available storage slices, as all objects with the same sharding-key value are assigned to the same slice. To balance the system's workload and optimize performance, try to select a sharding key for which there are enough distinct values in your data set to distribute evenly across the available slices (1024 slices per container). In addition, consider the expected data-access patterns. Note that even if your data set has many potential distinct sharding-key values, if most of the ingested objects or most of the table queries are for the same few sharding-key values, the performance is likely to suffer. As a general rule, increasing the ratio of accessed sharding-key values to the total number of distinct sharding-key values in the collection improves the efficiency of the data processing.\nFor example, a username for a data set with many users is likely to be a good sharding key, unless most of the information applies only to a few users. But an area ZIP code for a data set with only a few ZIP codes relative to the number of objects would not make for a good sharding key.\nUsing a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key can also help improve your collection's data distribution. For example, you can use a date sharding key and a device-ID sorting key (with values such as 20180523.1273) to store all data generated on the same day on the same data slice, sorted by the ID of the device that generated the data.\nIf you can't identify a sufficiently good sharding key for your collection, consider recalculating the sharding-key values as part of the objects' ingestion — see Recalculating Sharding-Key Values for Even Workload Distribution.\nRecalculating Sharding-Key Values for Even Workload Distribution To better balance the system's workload and improve performance when working with a non-uniform data set, consider recalculating the sharding-key values during the objects' ingestion so as to split a single object sharding-key value into multiple values. The objective is to increase the number of distinct sharding-key values in the collection and thus distribute the data objects more evenly across the available data slices.\nNote This method relevant only for collections with a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key, which allows for multiple objects in the collection to have the same sharding-key value. This method should typically be used with \u0026quot;flat\u0026quot; tables or with tables that have only one or two partitions. Data in multiple-partition tables should already be evenly distributed across the available data slices. For more information on partitioning of NoSQL tables, see Partitioned Tables.    One method for splitting a sharding-key value is to add a random number to the end of the original sharding-key value (separated from the original value by a predefined character, such as an underscore), thus randomizing the data ingestion across multiple slices instead of a single slice. For example, objects with the username sharding-value johnd, such as johnd.20180602 and johnd.20181015, might be ingested as johnd_1.20181002 and johnd_2.20181015.\nHowever, using a random sharding-key suffix makes it difficult to retrieve a specific object by its original primary-key value, because you don't know which random suffix was used when ingesting the specific object. This can be solved by calculating the suffix that is added to the original sharding-key value based on the value of an attribute that you might use for retrieving (reading) objects from the collection. For example, you can apply a hash to the value of the sorting-key attribute, as done by the even-distribution write option of the platform's NoSQL Spark DataFrame (see details in the following subsection). The new object primary-key values look similar to the values for the randomized-suffix method — for example, johnd_1.20180602 and johnd_2.20181015. But because the numeric suffixes in this case were created by using a known formula that is based on an object attribute, you can always apply the same calculation to retrieve a specific object by its original sharding- and sorting-key values.\nNote that regardless of the sharding-key recalculation method, retrieving all objects for a given sharding-key value from the original data set requires submitting multiple read requests (queries) — one for each of the new primary-key object values. However, the platform's NoSQL Spark DataFrame and Presto interfaces handle this implicitly, as outlined in the following subsection.\nUsing a NoSQL Spark DataFrame for Even Workload Distribution The platform's NoSQL Spark DataFrame has a custom range-scan-even-distribution write option to simplify even distribution of ingested items by allowing the platform to recalculate the sharding- and primary-key values of the ingested items based on the value of the items' sorting-key.\nIn addition, the NoSQL Spark DataFrame and the Iguazio Presto connector allow you to query such tables by using the original sharding-key value (which remains stored in the item's sharding-key attribute), instead of submitting separate queries for each of the new sharding-key values that are used in the item's primary-key values.\nFor more information, see the NoSQL DataFrame Even Workload Distribution reference and the following behind-the-scenes implementation details.\nBehind the Scenes The platform calculates the new item sharding-key value for the NoSQL Spark DataFrame even-distribution write option in the following manner:\n Apply the xxHash hash function to the item's sorting-key value. Perform a modulo operation on the sorting-key hash result, using the value that is configured in the platform's v3io.kv.range-scan.hashing-bucket-num configuration property (default = 64) as the modulus. Append to the original sharding-key value an underscore ('_') followed by the result of the modulo operation on the sorting-key hash.  The result is a new primary-key value of the format \u0026lt;original sharding-key value\u0026gt;_{xxHash(\u0026lt;sorting-key value\u0026gt;) % \u0026lt;preconfigured modulus\u0026gt;}.\u0026lt;sorting-key value\u0026gt;. For example, johnd_1.20180602 for an original johnd sharding-key value. With this design, items with the same original sharing-key value are stored on different data slices, but all items with the same sorting-key value (which remains unchanged) are stored on the same slice.\nWhen you query a table with a NoSQL Spark DataFrame or with the Presto CLI, you use the original sharding-key value. Behind the scenes, the platform can identify whether the original sharding-key value was recalculated using the method of the Spark DataFrame even-distribution option (provided the table has a schema that was inferred with a NoSQL DataFrame or the Iguazio Presto connector. In such cases, the platform searches all relevant slices to locate and return information for all items with the same original sharding-key value.\nUsing the NoSQL Web API for Even Workload Distribution The NoSQL Web API doesn't have specific support for even workload distribution, but you can select to implement this manually: when ingesting items with the PutItem or UpdateItem operation, recalculate the sharding-key value (using the provided guidelines) and set the item's primary-key value to \u0026lt;recalculated sharing-key value\u0026gt;.\u0026lt;sorting-key value\u0026gt;.\nNote that when submitting a NoSQL Web API GetItem request, you need to provide the full item primary-key value (with the recalculated sharding-key value), and when submitting a GetItems range-scan request, you need to provide the full recalculated sharding-key value. As explained in this guide, to retrieve all items with the same original sharding-key value, you'll need to repeat the requests for each of the newly calculated sharding-key values.\nNoSQL Spark DataFrame and Presto Notes    When ingesting an item with the web API, if you wish to also support table reads (queries) using Spark DataFrames or Presto —\n You must define user attributes for the original sharding-key and sorting-key values, regardless of whether you recalculate the sharding-key value. Take care not to modify the values of such attributes after the ingestion.  If you select to do recalculate the sharding-key value, use the method that is used by the NoSQL Spark DataFrame's even-distribution option (as outlined in the previous subsection) — i.e., use primary-key values of the format \u0026lt;original sharding-key value\u0026gt;_{xxHash(\u0026lt;sorting-key value\u0026gt;) % \u0026lt;modulus set in the v3io.kv.range-scan.hashing-bucket-num property\u0026gt;}.\u0026lt;sorting-key value\u0026gt; — to allow simplified Spark and Presto queries that use the original sharding-key value.    To use the web API to retrieve items that were ingested by using the NoSQL Spark DataFrame's even-distribution write option (or an equivalent manual web-API ingestion), you need to repeat the get-item(s) request with \u0026lt;original sharding key\u0026gt;_1 to \u0026lt;original sharding key\u0026gt;_\u0026lt;n\u0026gt; sharding-key values, where \u0026lt;n\u0026gt; is the value of the v3io.kv.range-scan.hashing-bucket-num configuration property.\nFor example, for an original sharding-key value of johnd, a sorting-key value of 20180602, and the default configuration-property value of 64 — call GetItem with primary-key values from johnd_1.20180602 to johnd_64.20180602, and call GetItems with sharding-key values from johnd_1 to johnd_64.   Distributing the Data Ingestion Efficiently It's recommended that you don't write (ingest) multiple objects with the same sharding-key value at once, as this might create an ingestion backlog on the data slice to which the data objects are assigned while other slices remain idle.\nAn alternative ingestion flow, which better balances the ingestion load among the available data slices, is to order the ingestion based on the values of the objects' sorting key. Objects with the same sorting-key value don't have the same sharding-key value and therefore won't be assigned to the same slice. For example, if you have a collection with a device-ID sharding key and a date sorting key, you can ingest all objects for a given date (sorting-key value) and then proceed to the next date in the data set. See Also  Object Names and Primary Keys Objects Attributes Range Scans NoSQL Spark DataFrame Reference The Presto CLI NoSQL Web API Reference  ","keywords":["best","practices,","primary","key,","defining","primary","keys,","keys,","sharding","key,","sorting","key,","collection","keys,","collections,","table","keys,","object","names,","item","names,","names,","__name,","naming","objects,","workload","distribution,","data","distribution,","even","distribution,","even","workload","distribution,","nosql,","range","scan,","scan","optimization,","table","partitioning,","nosql","spark","dataframe,","nosql","web","api,","range-scan-even-distribution,","PutItem,","UpdateItem,","GetItem,","GetItems"],"path":"https://github.com/jasonnIguazio/data-layer/objects/object-names-and-keys/best-practices-primary-keys-n-workload-distribution/","title":"Best Practices for Defining Primary Keys and Distributing Data Workloads"},{"content":"The following bitwise operators are supported in expressions. The operands are numeric, and the result is a binary number.\nBinary Bitwise Operators Bitwise AND Operator (\u0026amp;) OPERAND-A \u0026amp; OPERAND-B The bitwise AND operator (\u0026amp;) sets a result bit to 1 if the corresponding bit in both operands is 1; otherwise, sets the result bit to 0.\nBitwise OR Operator (|) The bitwise OR operator (|) sets a result bit to 1 if the corresponding bit in either of the operands is 1; otherwise, sets the result bit to 0.\nBitwise XOR Operator (^) OPERAND-A ^ OPERAND-B The bitwise XOR operator (^) sets a result bit to 1 if the corresponding bit in either, but not both, of the operands is 1; otherwise, sets the result bit to 0.\nBitwise Left-Shift Operator (\u0026lt;\u0026lt;) OPERAND-A \u0026lt;\u0026lt; OPERAND-B The bitwise left-shift operator (\u0026lt;\u0026lt;) shifts the bits of the left operand (OPERAND-A) to the left, shifting in zeros from the right. The right operand (OPERAND-B) determines the number of shifted bits.\nBitwise Right-Shift Operator (\u0026gt;\u0026gt;) OPERAND-A \u0026gt;\u0026gt; OPERAND-B The bitwise right-shift operator (\u0026gt;\u0026gt;) shifts the bits of the left operand (OPERAND-A) to the right, discarding shifted-off bits. The right operand (OPERAND-B) determines the number of shifted bits.\nUnary Bitwise Operators Bitwise NOT / Ones-Complement Operator (~) ~OPERAND The bitwise NOT (ones-complement) operator (~) inverts the bits of the operand in the result: use 1 instead of 0, and 0 instead of 1.\n","keywords":["expression","bitwise","operators,","bitwise","operators,","expression","operators,","attribute","variables,","binary","bitwise","operators,","binary","operators,","bitwise","AND","operator,","\u0026","operator,","bitwise","OR","operator,","|","operator,","bitwise","XOR","operator,","^","operator,","bitwise","left-shift","operator,","\u003c\u003c","operator,","bitwise","righ-shift","operator,","\u003e\u003e","operator,","unary","bitwise","operators,","unary","operators,","bitwise","NOT","operator,","one-complement","operator,","~","operator"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/bitwise-operators/","title":"Bitwise Operators"},{"content":"Allocated Infrastructure Resources When the platform is installed, it creates the following resources. Take this into account when selecting your installation configuration. Note that you also need to consider the service quotas (a.k.a. limits) for your AWS account. Type Amount When Notes  Data node 1 or 3 Always EC2 instance type i3.2xlarge, i3.4xlarge or i3.8xlarge. High-availability (HA) requires three nodes.   Data auto-scaling group 1 Always   Application node 1 or more Always EC2 CPU instance type m5.4xlarge or larger / GPU instance type p3.8xlarge or larger, or g4dn.12xlarge larger.  High-availability (HA) requires at least three nodes.    Application auto-scaling group 1 Always   OS boot disk (EBS volume) 1 per node Always EBS volume type General Purpose SSD (gp2). At least 400 GB for each data node and 250 GB for each application node.   Elastic IP address 1 per node The platform has public IP addresses.    VPC 1 The platform is deployed to a new VPC.    Subnet 1 or more The platform is deployed to a new VPC.    Route table 1 The platform is deployed to a new VPC.    Internet gateway 1 The platform is deployed to a new VPC.    Security group 2 The platform is deployed to a new VPC. See Network Security Groups Configuration (AWS).   EKS Cluster 1 EKS is used as the Application Cluster    See Also  AWS cloud installation guide AWS cloud deployment specifications  ","keywords":["calaculating","aws","infrastructure","reesources,","aws","infrastructure","resources","calculation,","subnet"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/resources-calculate/","title":"Calculating Required Infrastructure Resources (AWS)"},{"content":"Allocated Infrastructure Resources When the platform is installed, it creates the following resources. Take this into account when selecting your installation configuration. Type Amount When Notes  Data node 1 or 3 Always Azure instance type (VM size) Standard_L16s_v2.  High-availability (HA) requires three nodes.   Data-cluster application security group 1 per data cluster Always   Application node 1 or more Always Azure instance type (VM size) Standard_D16s_v3 or larger or NCv3-series (GPU optimized).  High-availability (HA) requires at least three nodes.   Application-cluster application security group 1 per application cluster Always    Network interface 1 per node Always   OS boot disk (Azure managed disk) 1 per node Always Premium SSD. At least 400 GB for each data node and 250 GB for each application node.   Public IP address 1 per node The platform has public IP addresses.    VNet 1 The platform is deployed to a new VNet.    Network security group 1 Always See Network Security Groups Configuration (Azure).   Installer node with a disk, NIC, and application security group 1 Always A Standard_D8s_v3 Azure VM installer node with additional resources that are used only for the platform installation and can be deleted after the installation completes.   AKS Cluster 1 When AKS is used as the application cluster    See Also  Azure cloud installation guide Azure Deployment Specifications  ","keywords":["calaculating","azure","infrastructure","reesources,","azure","infrastructure","resources","calculation,","subnet"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/howto/resources-calculate/","title":"Calculating Required Infrastructure Resources (Azure)"},{"content":"Allocated Infrastructure Resources When the platform is installed, it creates the following resources. Take this into account when selecting your installation configuration.\nYou also need to consider the GCP quotas (known as limits) for your GCP account.\nType Amount When Notes  Data node 1 or 3 Always Instance type n2-highmem-16. High-availability (HA) requires three nodes.   Application node 1 or more Always Instance type c2-standard-16 or larger.  High-availability (HA) requires at least three nodes.    OS boot disk 1 per node Always At least 400 GB for each data node and 250 GB for each application node.   External IP address 1 per node The platform has external IP addresses.    VPC Network 1 The platform is deployed to a new VPC network.    Subnet 1 The platform is deployed to a new VPC network.    FW rules 4 The platform is deployed to a new VPC network.   GKE Cluster 1 Always    See Also  GCP deployment specifications  ","keywords":["calaculating","gcp","infrastructure","resources,","gcp","infrastructure","resources","calculation,","subnet"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/howto/resources-calculate/","title":"Calculating Required Infrastructure Resources (GCP)"},{"content":"","keywords":null,"path":"https://github.com/jasonnIguazio/categories/","title":"Categories"},{"content":" This section contains specifications, guides, and how-to tutorials for installing (deploying) and configuring the MLOps Platform on supported enterprise cloud services.\n","keywords":["cloud","installation","and","setup,","cloud","installation,","cloud","setup,","cloud","configuration,","cloud"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/","title":"Cloud Deployment"},{"content":"Learn how to deploy, manage, and monitor MLOps Platform clusters.\nNote For information about managing platform users and securing platform operations, see Platform Users and Security. For security aspects related to cluster management, such as the predefined management policies that determine the users' scope of operation, see Security. For information about managing platform services, see Platform Services.    ","keywords":["cluster","management,","cluster","deployment,","clusters,","data","clusters,","application","clusters,","deployment,","administration,","admin"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/","title":"Cluster Management and Deployment"},{"content":"API Endpoint /api/cluster_info\n","keywords":["cluster-information","management","api,","cluster-information","api,","management,","cluster","management,","cluster","information,","/api/cluster_info,","api","endpoints"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/cluster-info-api/","title":"Cluster-Information API"},{"content":" Browse references for custom MLOps Platform (\u0026quot;platform\u0026quot;) cluster-management application programming interfaces (APIs).\nNoteFor an high-level overview of all platform APIs, see the APIs overview.  ","keywords":["clutster-management","references,","management","references,","cluster-management","api","references,","management","api","references,","api","reference,","cluster-management","apis,","management","apis,","cluster","management,","management,","beta"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/","title":"Cluster-Management References"},{"content":" This reference describes common data types that are supported by the platform.\n","keywords":["reference,","data","types,","common","data","types,","attribute","data","types"],"path":"https://github.com/jasonnIguazio/data-layer/reference/data-types/","title":"Common Data-Layer Data-Types Reference"},{"content":"This section describes JSON objects are used in the HTTP body of NoSQL web-API operations.\n","keywords":["nosql","objects,","nosql-web","api","reference,","nosql,","json,","json","objects,","http,","http","requests,","http","body,","attributes,","attribute","values"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/common-objects/","title":"Common Objects"},{"content":"The following binary comparison operators are supported in expressions. The operands can be numeric or strings. Strings are compared using lexicographic string comparison.\nNoteIf the operands cannot be compared (for example, when comparing attributes of different types), the comparison evaluates to false.  Equality Operators Equality Operator (==) OPERAND-A == OPERAND-B The equality operator (==) returns true if the operands are equal; otherwise, returns false.\nInequality Operator (!=) The inequality operator (!=) returns true if the operands are not equal; otherwise, returns false.\nOPERAND-A != OPERAND-B Relational Operators Greater-Than Operator (\u0026gt;) OPERAND-A \u0026gt; OPERAND-B The greater-than operator (\u0026gt;) returns true if the left operand (OPERAND-A) is greater than the right operand (OPERAND-B); otherwise, returns false.\nGreater-Than or Equal-To Operator (\u0026gt;=) OPERAND-A \u0026gt;= OPERAND-B The greater-than or equal-to operator (\u0026gt;=) returns true if the left operand (OPERAND-A) is greater than or equal to the right operand (OPERAND-B); otherwise, returns false.\nLess-Than Operator (\u0026lt;) OPERAND-A \u0026lt; OPERAND-B The less-than operator (\u0026lt;) returns true if the left operand (OPERAND-A) is less than the right operand (OPERAND-B); otherwise, returns false.\nLess-Than or Equal-To Operator (\u0026lt;=) OPERAND-A \u0026lt;= OPERAND-B The less-than or equal-to operator (\u0026lt;=) returns true if the left operand (OPERAND-A) is less than or equal to the right operand (OPERAND-B); otherwise, returns false.\n","keywords":["expression","comparison","operators,","comparison","operators,","expression","operators,","attribute","variables,","expression","equality","operators,","equality","operators,","equality","operator,","==","operator,","inequality","operator,","!=","operator,","expression","relational","operators,","relational","operators,","greater-than","operator,","\u003e","operator,","greater-than","or","equal-to","operator,","\u003e=","operator,","less-than","operator,","\u003c","operator,","less-than","or","equal-to","operator,","\u003c=","operator"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/comparison-operators/","title":"Comparison Operators"},{"content":" Overview A condition expression is an expression that defines a condition for executing an operation, based on the attributes of the items against which the expression is evaluated. For example, the NoSQL Web API UpdateItem operation receives an optional condition-expression request parameter (ConditionExpression) that defines a condition that determines whether to update the item or which update expression to use in the case of an if-then-else update.\nArrays SupportIn the current release, the support for array attributes and the use of array operators and functions in expressions is restricted to the web APIs.  Filter Expression For operations that retrieve existing item attributes, the condition expression acts as a filter expression that restricts the operation to a subset of items to be returned in the response. For example, the NoSQL Web API GetItems operation receives an optional FilterExpression request parameter that defines a condition that determines which items to return.\nSyntax \u0026#34;CONDITION [LOGICAL-OPERATOR CONDITION ...]\u0026#34; A condition expression is a Boolean expression that is composed of one or more Boolean expressions (CONDITION) that are connected using a binary logical operator (LOGICAL-OPERATOR) — AND or OR. The unary logical operator NOT can be used to negate a condition. A condition can optionally be enclosed within parentheses.\nNoteIf a condition within the expression references an attribute that is not set in the evaluated item, the condition fails (evaluates to false). The result of the entire condition expression depends on the expression's overall logic.  Examples   Check for \u0026quot;Car\u0026quot; products with less than 15 known bugs:\n\u0026#34;(bugs \u0026lt; 15) AND starts(Product_Name, \u0026#39;Car\u0026#39;)\u0026#34;   Check for 40-year-old people whose first names are \u0026quot;John\u0026quot; and are not residents of Arizona or Massachusetts:\n\u0026#34;(age == 40) and (firstName == \u0026#39;John\u0026#39;) and NOT(State IN (\u0026#39;AZ\u0026#39;,\u0026#39;MA\u0026#39;))\u0026#34;    Check whether the value of the third element in an \u0026quot;intArray\u0026quot; array attribute equals the sum of the first two elements in the array:\n\u0026#34;intArray[2] == intArray[0] + intArray[1]\u0026#34;   Check whether the value of the first or second elements of an \u0026quot;arr\u0026quot; array attribute is 7:\n\u0026#34;arr[0]==7 OR arr[1]==7\u0026#34;   Check for released adult prisoners who haven't reported to their parole officer:\n\u0026#34;(age \u0026gt; 18) AND isReleased==true AND reported==false\u0026#34;    Check whether the first or last five elements of a \u0026quot;ctrs\u0026quot; array contain the highest value:\n\u0026#34;max(ctrs[0..4]) \u0026gt; max(ctrs[5..9])\u0026#34;   See Also  Update Expression Attribute Variables  ","keywords":["condition","expressions,","filter","expressions,","conditional","update,","updat","items,","UpdateItem,","GetItems,","FilterExpression,","nosql,","nosql","web","api,","nosql,","table","items,","attributes,","boolean","expressions,","logical","operators,","binary","logical","operators,","CONDITION,","AND","operator,","OR","operator,","NOT","operator,","if-then,","if-then-else,","tech","preview,","arrays,","array","attributes,","array","operator"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/condition-expression/","title":"Condition Expression"},{"content":" Overview You can configure environmental variables, as key-value pairs, for Jupyter and Web Shell services. The values are applied to every instance of these services. You can define any variable that is supported by these services.\nAfter you configure global environmental variables, you can override the global value for an individual Jupyter service in the Services page. The environmental variables for other services cannot be overridden.\nConfiguring environmental variables in the Dashboard Follow these steps to configure environmental variables for your cluster:\n  Open the platform dashboard and select the settings gear-wheel icon () from the top-right toolbar of any page to open the Settings dialog.\n  Click Environment variables, then click Create a new environment variable and fill in one or more key-value pairs.\n  Click Apply. A loading spinner displays until the values are propagated to the services.\n  To delete a key-value pair, click the Delete icon next to the row of the key-value pair.\n  See Also  Platform Users Security  ","keywords":["environmental","variables"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/post-deployment-howtos/env_vars/","title":"Configuring Environmental Variables"},{"content":"Overview To allow the platform installer (Provazio) to connect to the platform's data and application nodes, you need to configure IP addresses for the management network (\u0026quot;management IP addresses\u0026quot;) on the platform's node VMs, as outlined in this guides.\nPrerequisites Before you begin, ensure that you have the following:\n Administrative access to a platform PVE cluster with the required networks configuration (see Configuring Virtual Networking (PVE)) and deployed VMs for each of the platform nodes (see Deploying the Platform Nodes (PVE)). User credentials for configuring management IP addresses, received from Iguazio. The management network's subnet can accommodate allocation of different IP addresses for each of the platform's node VMs.  Configure the Management IP Addresses To configure the management IP addresses on the platform's node VMs, execute the following procedure from the PVE GUI for each node VM.\nNoteDo not disable IPv6 on the Data and App node VMs.\nDon't configure IP addresses for the platform's data-path (client) and interfaces networks (interfaces \u0026quot;eth1\u0026quot; and \u0026quot;eth2\u0026quot;), as this is handled implicitly by the platform installer (Provazio).    Select to access the VMs' console.\n  Log into the VM using the user credentials for configuring management IP addresses, received from Iguazio (see the prerequisites).\n  Run the following command from the console command line to launch the NetworkManager text user interface (nmtui) CLI tool:\nsudo nmtui   Configure the IP address, subnet, and gateway address on the management-network interface — \u0026quot;eth0\u0026quot;. (Don't configure interfaces \u0026quot;eth1\u0026quot; and \u0026quot;eth2\u0026quot;; this is handled by the platform installer.)\n  Ensure that the Automatically connect option is selected.\n  When you're done with the configuration, exit nmtui and run the following command to restart the network and apply your changes.\nsudo systemctl   Verifying the Configuration After configuring the management IP addresses for all of the platform's node VMs, ensure that the configured addresses are defined and reachable.\n   See Also  PVE Installation Guide On-prem VM hardware specifications  ","keywords":["networking,","vm-networking,","configure","ips,","ip","addresses"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/ips-cfg/","title":"Configuring IP Addresses (PVE)"},{"content":"Overview To allow the platform installer (Provazio) to connect to the platform's data and application nodes, you need to configure IP addresses for the management network (\u0026quot;management IP addresses\u0026quot;) on the platform's node VMs, as outlined in this guides.\nPrerequisites Before you begin, ensure that you have the following:\n Administrative access to a platform vSphere cluster with the required networks configuration (see Configuring Virtual Networking (vSphere)) and deployed VMs for each of the platform nodes (see Deploying the Platform Nodes (vSphere)). User credentials for configuring management IP addresses, received from Iguazio. The management network's subnet can accommodate allocation of different IP addresses for each of the platform's node VMs.  Configure the Management IP Addresses To configure the management IP addresses on the platform's node VMs, execute the following procedure from the vSphere Web Client for each node VM.\nNoteDo not disable IPv6 on the Data and App node VMs.\nDon't configure IP addresses for the platform's data-path (client) and interfaces networks (interfaces \u0026quot;eth1\u0026quot; and \u0026quot;eth2\u0026quot;), as this is handled implicitly by the platform installer (Provazio).    Select to access the VMs' console.\n  Log into the VM using the user credentials for configuring management IP addresses, received from Iguazio (see the prerequisites).\n  Run the following command from the console command line to launch the NetworkManager text user interface (nmtui) CLI tool:\nsudo nmtui   Configure the IP address, subnet, and gateway address on the management-network interface — \u0026quot;eth0\u0026quot;. (Don't configure interfaces \u0026quot;eth1\u0026quot; and \u0026quot;eth2\u0026quot;; this is handled by the platform installer.)\n  Ensure that the Automatically connect option is selected.\n  When you're done with the configuration, exit nmtui and run the following command to restart the network and apply your changes.\nsudo systemctl   Verifying the Configuration After configuring the management IP addresses for all of the platform's node VMs, ensure that the configured addresses are defined and reachable.\n   See Also  vSphere Installation Guide On-prem VM hardware specifications  ","keywords":["vm","network","configuration,","vm","networking,","vm","ip","addresses,","subnets,","ip","addresses,","network"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/ips-cfg/","title":"Configuring IP Addresses (vSphere)"},{"content":" Overview Application services in the MLOps Platform (\u0026quot;the platform\u0026quot;) clusters run on top of Kubernetes (see The Platform's Application Services). The services are accessed via Kubernetes ingresses, which act as gateways that allow access to cluster applications and internal services through service URLs. The platform uses the CoreDNS DNS server to resolve cluster service URLs and map them to internal service IP addresses. The cluster's DNS server should be configured to use conditional forwarding, so that DNS queries that contain the domain name of the cluster, and only such queries, are forwarded to the platform for resolution. This document provides step-by-step instructions for configuring conditional DNS forwarding on Linux or Windows.\nTerminology  DNS Domain Name System — an internet service that translates domain names into IP addresses. DNS forwarding DNS forwarding is the process by which particular sets of DNS queries are forwarded to a designated server for resolution according to the DNS domain name in the query rather than being handled by the initial server that was contacted by the client. This process improves the network's performance and resilience. It provides a way to resolve name queries both inside and outside of the network by passing on namespaces or resource records that aren't contained in the zone of a local DNS server to a remote DNS server for resolution. When a DNS server is configured to use a forwarder, if it can't resolve a name query by using its local primary zone, secondary zone, or cache, it forwards the request to the designated forwarder instead of attempting to resolve it by using root hints (as done when no forwarder is configured). Conditional forwarders Conditional forwarders are DNS servers that only forward queries for specific domain names. Instead of forwarding all queries it cannot resolve locally to a forwarder, a conditional forwarder is configured to forward name queries to specific forwarders based on the domain name contained in the query. Forwarding according to domain names improves conventional forwarding by adding a name-based condition to the forwarding process. It enables improving name resolution between internal (private) DNS namespaces that aren't part of the DNS namespace of the internet, such as results from a company merger. FQDN Fully qualified domain name  Linux DNS Configuration Perform the following steps to configure conditional DNS forwarding on Linux by using BIND — a popular open-source DNS server from the Internet Systems Consortium (ISC), which is found in most Linux distributions; for more information about BIND, see the Additional Resources section of this document.\nNoteThe following procedure assumes that you have a configured BIND server.    Open the BIND name-server configuration file (named.conf) in a text editor, and add the following lines; replace the \u0026lt;domain\u0026gt; placeholder with FQDN of the platform cluster to which you want to forward queries, and replace the \u0026lt;datanode IP\u0026gt; placeholders with the IP addresses of the cluster's master data nodes:\nzone \u0026#34;\u0026lt;domain name\u0026gt;\u0026#34; { type forward; forward only; forwarders { \u0026lt;datanode IP\u0026gt;; [\u0026lt;datanode IP\u0026gt;; ...] }; };   Check and reload the configuration by running the following commands from a Linux command-line shell:\nnamed-checkconf rndc reload   Windows DNS Configuration Perform the following steps to configure conditional DNS forwarding on Windows.\nNoteThe following instructions are compatible with Windows Server 2012 R2. The specific steps and menu options may differ on other versions of Windows.    Open the Windows Server Manager (for example, by entering ServerManager in the Windows command prompt). In the Server Manager window, select the Tools tab. Then, select DNS from the tools list.      In the DNS Manager window, select your DNS server. Then, select Conditional Forwarders from the server browse tree.\n     Select Action from the top menu toolbar, and then select the New Conditional Forwarder menu option.\n     In the New Conditional Forwarder window —\n  In the DNS Domain field, enter the FQDN of the platform cluster for which you want to forward queries.\n  In the IP addresses of the master servers field, add the IP addresses of your cluster's master data nodes.\n  Check the Store this conditional forwarder in Active Directory ... check box, if applicable.        Additional Resources The information in this document is based, in part, on the following resources:\n DNS Forwarding and Conditional Forwarding (Medium Tech Jobs Academy) BIND (ISC GitLab) Linux BIND DNS - Introduction To The DNS Database (BIND) (Firewall.cx) Build your own DNS server on Linux (opensource.com)  See Also  Platform Services  ","keywords":["dns","server,","dns,","dns","server","configuration,","dns","configuration,","dns","forwarding,","conditional","forwarding,","configurations,","prerequisites,","setup,","installation,","security"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/post-deployment-howtos/dns/","title":"Configuring the DNS Server"},{"content":" Overview You can define pip and Conda key-value pairs as:\n Environmental variables for shell and Jupyter Default build arguments for Nuclio and MLRun  The values are applied to every instance of these services. You can define any variable that is supported by pip or Conda. Changing the Pypi server URL causes a restart of Jupyter, MLRun, Nuclio, and Shell, services.\nConfiguring pip and Conda environmental variables in the Dashboard To configure pip and Conda environmental variables for your cluster:\n  Open the platform dashboard and select the settings gear-wheel icon () from the top-right toolbar of any page to open the Settings dialog.\n  Click Python packages, then under PIP Options, click Create a new option and fill in one or more key-value pairs.\n  Under Conda options, click Create a new option and fill in one or more key-value pairs.\n  Click Apply. A loading spinner displays until the values are propagated to the services.\n  To delete a key-value pair, click the Delete icon next to the row of the key-value pair.\n  See Also  Platform Users Security  ","keywords":["python","repositories"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/post-deployment-howtos/python_vars/","title":"Configuring the Python Repositories"},{"content":" Overview The platform sends email messages for different purposes, such as password generation and call home functionality. To support this, the platform clusters needs to be configured to work with a Simple Mail Transfer Protocol (SMTP) server that will be used for sending the email messages.\nPrerequisites The following prerequisites must be filled to successfully configure an SMTP server for your cluster:\n There must be an SMTP server with connectivity to the cluster. When the cluster is connected to the internet, you can use a public server. Otherwise, use a local SMTP server. The call home functionality requires that the SMTP server be able to send emails to the Iguazio domain. The user who configures the SMTP server must have the IT Admin management policy.  Configuring an SMTP Server from the Dashboard Follow these steps to configure an SMTP server for your cluster:\n  Open the platform dashboard and select the settings gear-wheel icon () from the top-right toolbar of any page to open the Settings window.\n  In the SMTP section, ensure that the Enabled option is turned on and fill in the configuration parameters — a valid SMTP email address; the username and password of an SMTP user to be used for sending the email messages; the host IP address and port of the SMTP server; and optionally a list of users with the IT Admin management policy for receiving cluster-alert email notifications — as demonstrated in the following image:\n     Select Apply to apply your changes.\n  See Also  Platform Users Security  ","keywords":["smtp","server,","smtp,","smtp","server","configuration,","smtp","configuration,","smtp","email,","configurations,","prerequisites,","setup,","installation,","security,","support,","call","home,","user","management,","passwords"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/post-deployment-howtos/smtp/","title":"Configuring the SMTP Server"},{"content":" Overview The platform's data and application node VMs (\"the platform nodes\") are connected through three networks:  Management network \u0026mdash; used for user access to the platform dashboard and for direct access the platform nodes.  Data-path (client) network \u0026mdash; used for internal communication among the data and application nodes.  Interconnect network \u0026mdash; used for internal communication among the data nodes.   To allow proper communication, you need to create and map network bridges for these networks, as outlined in this guide. Repeat the procedure for each Proxmox VE hypervisor host machine (\u0026quot;PVE host\u0026quot;) in your platform's PVE cluster.\nPrerequisites Before you begin, ensure that you have administrative access to the platform's PVE cluster, and that each PVE host in the cluster has the following network interfaces:  A single-port 1 Gb (minimum) NIC for the management network  For hosting data-node VMs \u0026mdash; a dual-port 10 Gb (minimum) NIC for the data-path (client) and interconnect networks  For hosting application-node VMs only \u0026mdash; a single-port 10 Gb (minimum) NIC for the data-path (client) network   Each network must be on a different subnet, and the management network's subnet must be able to accommodate allocation of IP addresses for each of the platform's node VMs. Configuring the Management Network The management network is usually connected to the default existing bridge \u0026quot;vmbr0\u0026quot;, and is mapped to the management port. It's recommended that you add a comment marking this interface as \u0026quot;management\u0026quot;: in the PVE GUI, choose the relevant PVE host, select System | Network, and edit \u0026quot;vmbr0\u0026quot; to add the comment.\nConfiguring the Data-Path (Client) Network To configure the data-path (client) network, in the PVE GUI, choose the relevant PVE host and select System | Network. Create a new Linux bridge named \u0026quot;vmbr1\u0026quot;, add the comment \u0026quot;client\u0026quot;, and map the bridge to the platform's data-path (client) NIC port on the host (see the prerequisites). There's no need to configure any IPs for the bridge.    Configuring the Interconnect Network NoteThe interconnect network is used only for data-nodes communication.  To configure the interconnect network, in the PVE GUI, choose the relevant PVE host and select System | Network. Create a new Linux bridge named \u0026quot;vmbr2\u0026quot;, add the comment \u0026quot;interconnect\u0026quot;, and map the bridge to the platform's interconnect NIC port on the host (see the prerequisites). There's no need to configure any IPs for the bridge. In the case of a single data-node, there's also no need to map the bridge to any external interface.    Restarting the PVE Host When you're done, restart the PVE hypervisor host machine (\u0026quot;the PVE host\u0026quot;) to apply your changes.\nVerifying the Configuration When you're done (after the host has restarted), you can verify your configuration in the PVE GUI, as demonstrated in the following image; (the IP addresses and interface names might be different in your configuration):    See Also  Proxmox VE installation guide On-Prem Deployment Specifications  ","keywords":["proxmox","ve","networking,","pve","networking,","proxmox","networking,","virtual","networking,","virtual","networks,","vm","networking,","proxmox","ve","networks,","pve","networks,","proxmox","networks,","networks,","platform","networks,","management","network,","data-path","network,","client","network,","interconnect","network,","networks,","virtual","network","bridges,","network","briges,","linux","bridges,","virtual","network","switches,","virtual","switches,","vswitchea,","network","switches,","network","interface","cards,","nic,","network","cards,","proxmox","ve","gui,","pve","gui"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/network-cfg/","title":"Configuring Virtual Networking (PVE)"},{"content":" Overview The platform's data and application node VMs (\"the platform nodes\") are connected through three networks:  Management network \u0026mdash; used for user access to the platform dashboard and for direct access the platform nodes.  Data-path (client) network \u0026mdash; used for internal communication among the data and application nodes.  Interconnect network \u0026mdash; used for internal communication among the data nodes.   To allow proper communication, you need to create and map virtual network switches (vSwitches) for these networks, as outlined in this guide. Repeat the procedure for each VMware ESXi hypervisor host machine (\u0026quot;ESXi host\u0026quot;) in your platform's vSphere cluster.\nPrerequisites Before you begin, ensure that you have administrative access to the platform's vSphere cluster, and that each ESXi host in the cluster has the following network interfaces:  A single-port 1 Gb (minimum) NIC for the management network  For hosting data-node VMs \u0026mdash; a dual-port 10 Gb (minimum) NIC for the data-path (client) and interconnect networks  For hosting application-node VMs only \u0026mdash; a single-port 10 Gb (minimum) NIC for the data-path (client) network   Each network must be on a different subnet, and the management network's subnet must be able to accommodate allocation of IP addresses for each of the platform's node VMs. Configuring the Management Network The management-network vSwitch needs to be mapped to ports that are on the management virtual LAN (VLAN). If such a vSwitch is already configured in your environment (typically named \u0026quot;vSwitch0\u0026quot;), you may use it instead of creating an additional switch. To create a management-network vSwitch, follow these steps:\n Create a new vSwitch named \u0026quot;igz_management\u0026quot;, and map it to the appropriate vNIC on the ESXi host. Create a new port group named \u0026quot;igz_management\u0026quot;, and map it to the \u0026quot;igz_management\u0026quot; vSwitch.  Configuring the Data-Path (Client) Network NoteIf all the data and application node VMs reside on the same ESXi host, it may be possible to skip this step. Consult Iguazio's support team.  To configure the data-path (client) network —\n Create a new vSwitch named \u0026quot;igz_client\u0026quot;, and map it to the platform's data-path (client) NIC port on the ESXi host (see the prerequisites). Create a new port group named \u0026quot;igz_client\u0026quot;, and map it to the \u0026quot;igz_client\u0026quot; vSwitch.  Configuring the Interconnect Network Note The interconnect network is used only for data-nodes communication. If all the data and application node VMs reside on the same ESXi host, it may be possible to skip this step. Consult Iguazio's support team.    To configure the interconnect network —\n Create a new vSwitch named \u0026quot;igz_interconnect\u0026quot;, and map it to the platform's interconnect NIC port on the ESXi host (see the prerequisites). Create a new port group named \u0026quot;igz_interconnect\u0026quot;, and map it to the igz_interconnect vSwitch.  Verifying the Configuration When you're done, you can verify your configuration in the vSphere Web Client, as demonstrated in the following image:    See Also  VMware vSphere installation guide On-Prem Deployment Specifications  ","keywords":["vmware","vsphere","netowrking,","vsphere","netowrking,","vmware","netowrking,","virtual","networking,","virtual","networks,","vm","networking,","vsphere","networks,","vmware","networks,","platform","networks,","management","network,","data-path","network,","client","network,","interconnect","network,","networks,","virtual","network","switches,","virtual","switches,","vswitchea,","network","switches,","network","interface","cards,","nic,","network","cards,","vsphere","web","client,","vsphere","ui"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/network-cfg/","title":"Configuring Virtual Networking (vSphere)"},{"content":"Overview When installing an instance of the platform with public IP addresses, you must enable and configure allocation of public IP addresses for one or more subnets of the AWS virtual private cloud (VPC) in which the platform will be installed, as outlined in this guide.\nStep 1: AWS Login Log into your AWS Management Console and select the VPC service.    Step 2: Subnets Selection Under Subnets, select the subnet (or subnets) in which you plan to install the platform. Then, from the Actions menu select Modify auto-assign IP settings.    Step 3: Enable Allocation of Public IP Addresses Finally, check Enable auto-assign public IPv4 address, and select Save.    Any EC2 instance provisioned in this subnet will now be assigned a public IP address.\nAdditional Resources  VPC IP Addressing (AWS documentation)  See Also  AWS cloud installation guide Network Security Groups Configuration (AWS)  ","keywords":["aws","public","ip","address,","aws","network","configuration,","aws","ip","addresses,","subnets,","public","ip","addresses,","public","ips,","ip","addresses,","network,","ec2","instances,","ec2"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/subnet-public-ips-alloc-cfg/","title":"Configuring VPC Subnet Allocation of Public IP Addresses (AWS)"},{"content":" Container Names Every container has a name, which is a user-assigned string that uniquely identifies the container within its tenant and in the dashboard.\nContainer-Name Restrictions Container names are subject to the general file-system naming restrictions and the following additional restrictions:\n  Contain only the following characters:\n Lowercase letters (a–z) and numeric digits (0–9) Hyphens (-) Underscores (_)    Begin and end with a lowercase letter (a–z) or a numeric digit (0–9)\n  Contain at least one lowercase letter (a–z)\n  Not contain multiple successive hyphens (-) or underscores (_)\n  Length of 1–128 characters\n  NoteContainer names cannot contain spaces.  Container IDs The platform assigns every container a unique numeric ID.\n Container-ID DeprecationWhenever possible, identify a container by its name (or \u0026quot;alias\u0026quot;) and not by its ID. For platform APIs that support both identification methods, the container-ID option is deprecated and will eventually be removed.  See Also  Container-name software specifications and restrictions Working with Data Containers  ","keywords":["data-container","names,","container","names,","data-container","ids,","container","ids,","container-name","restrictions,","naming","restrictions"],"path":"https://github.com/jasonnIguazio/data-layer/containers/container-names/","title":"Container Names and IDs"},{"content":"Containers resemble Amazon S3 buckets, but unlike S3 buckets, containers can be used to store any type of data. Containers are created and managed either from the dashboard or by using the RESTful Containers Management API [Beta]. You can also perform some container operations through the Simple-Object Web API:\n  GET Service — lists the containers that are visible to the user who sent the request, according to its tenant.   GET Container — returns the objects that are stored in the data container.   HEAD Container — checks whether a container exists and the user has permission to access it.   NoteYou can find examples of Simple Object Web-API container-operation requests in the Working with Data Containers tutorial.  ","keywords":["container","operations,","containers,","simple","objects,","S3,","REST,","RESTful,","HEAD,","find","container,","container","access","permissions,","GET","Container,","get","objects,","GET","Service,","list","containers,","get","containers,","access","permissions,","security,","authentication,","authorization,","cluster-management","apis,","management","apis,","container","managment","web","api,","management,","dashboard"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/simple-object-web-api/container-operations/","title":"Container Operations"},{"content":"API Endpoint /api/containers\n","keywords":["containers","management","api,","containers","api,","management,","containers","management,","containers,","/api/containers,","api","endpoints"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/containers-api/","title":"Containers API"},{"content":"Description Creates a new container.\nYou must provide a name for the new container (see the name request-data parameter). This name will be used to identify your container in the dashboard.\nRequest Request Header Syntax  POST /api/containers HTTP/1.1 Host: \u0026lt;management-APIs URL\u0026gt; Content-Type: application/json Cookie: session=\u0026lt;cookie\u0026gt;  url = \u0026#34;\u0026lt;management-APIs URL\u0026gt;/api/containers\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;session=\u0026lt;cookie\u0026gt;\u0026#34; }    HTTP Method POST\nURL Resource ParametersNone\nRequest Data Syntax  { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; } }  payload = { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;string\u0026#34; } } \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; }    Parameters  attributes Container attributes.\n Type: A JSON object of container attributes   Requirement: Required  The following container attributes are applicable to this request:\n name A unique name for the new container. See Container Names and IDs, and specifically Cotnainer-Name Restrictions.\n Type: String   Requirement: Required   description A textual description of the container. When provided, this description is displayed in the dashboard together with the container name.\n Type: String   Requirement: Required     type The type of the data object. This value must be set to \u0026quot;container\u0026quot;.\n Type: String   Requirement: Required    Response Response Header Syntax HTTP/1.1 \u0026lt;status code; 200 on success\u0026gt; \u0026lt;reason phrase\u0026gt; Content-Type: application/json ... Response Data Syntax { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;admin_status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;cost\u0026#34;: number, \u0026#34;created_at\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;id\u0026#34;: number, \u0026#34;imports\u0026#34;: [], \u0026#34;mapping\u0026#34;: { \u0026#34;cmds_index\u0026#34;: number, \u0026#34;container_id\u0026#34;: number, \u0026#34;mds_instances\u0026#34;: [ { \u0026#34;service_context\u0026#34;: { \u0026#34;internal_xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;rest_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;shared_memory_objects\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;shutdown\u0026#34;: { \u0026#34;phase\u0026#34;: number, \u0026#34;timeout\u0026#34;: number }, \u0026#34;version_info\u0026#34;: { \u0026#34;external\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;git\u0026#34;: \u0026#34;string \u0026#34;, \u0026#34;offline\u0026#34;: \u0026#34;string \u0026#34; }, \u0026#34;xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ] }, \u0026#34;service_id\u0026#34;: { \u0026#34;node_name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;service_instance\u0026#34;: number, \u0026#34;service_name\u0026#34;: \u0026#34;string\u0026#34; } } ], \u0026#34;num_slices\u0026#34;: 0 }, \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;properties\u0026#34;: [], \u0026#34;updated_at\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;id\u0026#34;: number, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; }, ... } Elements The data object in the HTTP response body contains information about the new container, including its name (name), ID (id), creation time (created_at), relevant addresses, and version information (version_info). Full the full list of returned data elements, see response-data syntax above.\nExamples Create a container named \u0026quot;mycontainer\u0026quot; with the description \u0026quot;My first container\u0026quot;. The ID of the created container in this example (as returned in the response) is 1030:\nRequest  POST /api/containers HTTP/1.1 Host: https://dashboard.default-tenant.app.mycluster.iguazio.com Content-Type: application/json Cookie: session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first container\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; } }  import requests url = \u0026#34;https://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D\u0026#34; } payload = { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;My first container\u0026#34; } } \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;cost\u0026#34;: 0.976715087890625, \u0026#34;created_at\u0026#34;: \u0026#34;2018-01-15T08:17:19.904000+00:00\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;id\u0026#34;: 1030, \u0026#34;imports\u0026#34;: [], \u0026#34;mapping\u0026#34;: { \u0026#34;cmds_index\u0026#34;: 0, \u0026#34;container_id\u0026#34;: 0, \u0026#34;mds_instances\u0026#34;: [ { \u0026#34;service_context\u0026#34;: { \u0026#34;internal_xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp:// 10.0.0.1:5000\u0026#34; } ], \u0026#34;rest_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;10.0.0.1:8154\u0026#34; } ], \u0026#34;shared_memory_objects\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_metadata\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsMetadata\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_values\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsValues\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;place_holder_for_log_shm\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;log\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_names\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsMetricNames\u0026#34; } ], \u0026#34;shutdown\u0026#34;: { \u0026#34;phase\u0026#34;: 0, \u0026#34;timeout\u0026#34;: 0 }, \u0026#34;version_info\u0026#34;: { \u0026#34;external\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;git\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;offline\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp:// 10.0.0.1:5001\u0026#34; } ] }, \u0026#34;service_id\u0026#34;: { \u0026#34;node_name\u0026#34;: \u0026#34;igz0\u0026#34;, \u0026#34;service_instance\u0026#34;: 1, \u0026#34;service_name\u0026#34;: \u0026#34;mds\u0026#34; } } ], \u0026#34;num_slices\u0026#34;: 0 }, \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;properties\u0026#34;: [], \u0026#34;updated_at\u0026#34;: \u0026#34;2018-01-30T14:30:00.367000+00:00\u0026#34; }, \u0026#34;id\u0026#34;: 1030, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; }, ... } ","keywords":["Create","Container,","management,","containers,","http","POST,","POST","method,","POST,","container","attributes"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/containers-api/create-container/","title":"Create Container"},{"content":" Description Creates a new TSDB table.\nSyntax create(backend, table, if_exists=FAIL, **kw) The following syntax statement replaces the kw parameter with the additional keyword arguments that can be passed for the NoSQL backend via this parameter:\ncreate(backend, table, if_exists=FAIL, rate[, aggregates, aggregation_granularity])  NoteThe method has additional parameters that aren't currently supported for the TSDB backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after table.\n  Parameters aggregates (kw argument) | aggregation_granularity (kw argument) | backend | kw | if_exists | rate (kw argument) | table\n backend The backend type — \u0026quot;tsdb\u0026quot; for the TSDB backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a TSDB table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/tsdb/my_metrics\u0026quot;.\n Type: str   Requirement: Required   if_exists Determines whether to raise an error when the specified TSDB table (table) already exists.\n Type: pb.ErrorOptions enumeration. To use the enumeration, import the frames_pb2 module; for example:\nfrom v3io_frames import frames_pb2 as fpb    Requirement: Optional   Valid Values: FAIL to raise an error when the specified table already exists; IGNORE to ignore this   Default Value: FAIL   kw This parameter is used for passing a variable-length list of additional keyword (named) arguments. See the following kw Arguments section for a list of additional arguments that are supported for the TSDB backend via the kw parameter.\n Type: ** — variable-length keyword arguments list   Requirement: Required    kw Arguments The TSDB backend supports the following create arguments via the kw parameter for passing a variable-length list of additional keyword arguments:\n rate The ingestion rate of the TSDB metric samples. It's recommended that you set the rate to the average expected ingestion rate for a unique label set (for example, for a single server in a data center), and that the ingestion rates for a given TSDB table don't vary significantly; when there's a big difference in the ingestion rates (for example, x10), consider using separate TSDB tables.\n Type: str   Requirement: Required   Valid Values: A string of the format \u0026quot;[0-9]+/[smh]\u0026quot; — where 's' = seconds, 'm' = minutes, and 'h' = hours. For example, \u0026quot;1/s\u0026quot; (one sample per minute), \u0026quot;20/m\u0026quot; (20 samples per minute), or \u0026quot;50/h\u0026quot; (50 samples per hour).   aggregates A list of aggregation functions (\u0026quot;aggregators\u0026quot;) for performing over-time aggregation in real time during the metric-samples ingestion (\u0026quot;pre-aggregation\u0026quot;). Over-time aggregation returns an aggregation time series for each unique metric label set. The aggregations results are stored in the TSDB table as array attributes (\u0026quot;pre-aggregates\u0026quot;) and used to handle relevant aggregation queries. Use the aggregation_granularity argument to configure the table's pre-aggregation granularity — the time interval for applying these aggregations.\nNoteYou can also perform aggregation queries for TSDB tables without pre-aggregates, but when configured correctly, pre-aggregation queries are more efficient. To ensure that pre-aggregation is used to process aggregation queries and improve performance —\n When creating the TSDB table, set its aggregation granularity (aggregation_granularity) to an interval that's significantly larger than the table's metric-samples ingestion rate (rate). When querying the table using the read method, set the aggregation window (which is currently the aggregation step — see the step parameter) to a sufficient multiplier of the table's aggregation granularity.  For example, if the table's ingestion rate is 1 sample per second (\u0026quot;1/s\u0026quot;) and you want to use hourly queries (i.e., use a query aggregation window of \u0026quot;1h\u0026quot;), you might set the table's pre-aggregation granularity to 20 minutes (\u0026quot;20m\u0026quot;).\nFor more information about aggregation queries, see the read method.\n   Type: str   Requirement: Optional   Valid Values: A string containing a comma-separated list of supported aggregation functions; for example, \u0026quot;count,avg,min,max\u0026quot;. The following aggregation functions are supported: \navg — the average of the sample values. count — the number of ingested samples. last — the value of the last sample (i.e., the sample with the latest time). max — the maximal sample value. min — the minimal sample value. rate — the change rate of the sample values, which is calculated as \u0026lt;last sample value of the previous interval\u0026gt; - \u0026lt;last sample value of the current interval\u0026gt;) / \u0026lt;aggregation granularity\u0026gt;. stddev — the standard deviance of the sample values. stdvar — the standard variance of the sample values. sum — the sum of the sample values.     aggregation_granularity The TSDB table's pre-aggregation granularity — i.e., the time interval for executing the aggregation functions that are configured in the aggregates argument for real-time aggregation during metric-samples ingestion.\nNoteTo get the expected pre-aggregation performance impact, the size of the aggregation-granularity interval (as set in the aggregation_granularity argument) should be significantly larger than the size of the table's metric-samples ingestion rate (as set in the rate argument). See the aggregates argument notes.\n   Type: str   Requirement: Optional   Valid Values: A string of the format \u0026quot;[0-9]+[mhd]\u0026quot; — where 'm' = minutes, 'h' = hours, and 'd' = days. For example, \u0026quot;30m\u0026quot; (30 minutes), \u0026quot;2h\u0026quot; (2 hours), or \u0026quot;1d\u0026quot; (1 day).   Default Value: \u0026quot;1h\u0026quot; (1 hour)    Errors In case of an error, the method raises a CreateError error.\nExamples Following are some usage examples for the create method of the Frames TSDB backend:\n  Create a TSDB table named \u0026quot;mytsdb\u0026quot; in the root directory of the client's data container (table) with an ingestion rate of 1 sample per second (rate):\ntsdb_table = \u0026#34;mytsdb\u0026#34; client.create(backend=\u0026#34;tsdb\u0026#34;, table=tsdb_table, rate=\u0026#34;1/s\u0026#34;)   Create a TSDB table named \u0026quot;my_metrics\u0026quot; in a tsdb directory in the client's data container (table) with an ingestion rate of 12 samples per hour (rate). The table is created with the count, avg, min, and max aggregation functions (aggregates) and an aggregation granularity of 1 hour (aggregation_granularity):\ntsdb_table = \u0026#34;/tsdb/my_metrics\u0026#34; client.create(\u0026#34;tsdb\u0026#34;, table=tsdb_table, rate=\u0026#34;12/h\u0026#34;, aggregates=\u0026#34;count,avg,min,max\u0026#34;, aggregation_granularity= \u0026#34;1h\u0026#34;)   See Also  Creating a New TSDB (The TSDB CLI) Frames TSDB-Backend Overview Frames Client Constructor  ","keywords":["create","method,","frames","tsdb","create","method,","frames","create,","frames","tsdb","create,","frames","client","create,","frames","client","tsdb","create,","frames","create","reference,","frames","tsdb","create","reference,","tsdb","tables","creation,","tsdb","creation,","aggregation,","TSDB","aggregation,","aggregators,","aggregation","functions,","over-time","aggregation,","pre-aggregation,","pre-aggregates,","aggregation","granularity,","TSDB","ingestion","rate,","aggregates,","aggregation_granularity,","backend,","kw,","rate,","table"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/tsdb/create/","title":"create Method"},{"content":"Description Creates a new management session. The operation returns a session cookie that can be used to authenticate the user when sending other management requests (see the description of the \u0026lt;cookie\u0026gt; parameter in the General Management-API Structure documentation).\nThe cookie is valid for the duration of its time-to-live (TTL) period, which is returned in the operation's response (both in the max-age parameter of the Set-Cookie response header, and in the ttl response-data attribute). When this period elapses, the cookie expires. The cookie's expiration time is returned in the expires_at response-data attribute.\nCreate Session itself doesn't require a session cookie.\nRequest Request Header Syntax  POST /api/sessions HTTP/1.1 Host: \u0026lt;management-APIs URL\u0026gt; Content-Type: application/json  url = \u0026#34;\u0026lt;management-APIs URL\u0026gt;/api/sessions\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }    HTTP Method POST\nURL Resource ParametersNone\nRequest Data Syntax  { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34; } }  payload = { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34; } }    Parameters  attributes User-credentials attributes, as received from your platform's security administrator (see security_admin).\n Type: A JSON object of user-credentials attributes   Requirement: Required  The following user-credentials attributes are applicable to this request:\n username User name.\n Type: String   Requirement: Required   password Password.\n Type: String   Requirement: Required     type The type of the data object. This value must be set to \u0026quot;session\u0026quot;.\n Type: String   Requirement: Required    Response Response Header Syntax HTTP/1.1 \u0026lt;status code; 201 on success\u0026gt; \u0026lt;reason phrase\u0026gt; Content-Type: application/json Set-Cookie: session=\u0026lt;cookie\u0026gt;; max-age=\u0026lt;age\u0026gt;; path=/; ... Set-Cookie Header The Set-Cookie response header contains the new session cookie that you created (\u0026lt;cookie\u0026gt;). Save this cookie and use it to submit requests to other management-API resources (see the Sessions API overview and Create Session description.\nResponse Data Syntax { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;created_at\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;expires_at\u0026#34;: number, \u0026#34;gids\u0026#34;: [ \u0026#34;string\u0026#34; ], \u0026#34;group_ids\u0026#34;: [ \u0026#34;string\u0026#34; ], \u0026#34;plane\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ttl\u0026#34;: number, \u0026#34;uid\u0026#34;: number }, \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;relationships\u0026#34;: { \u0026#34;tenant\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34; } } Elements The data object in the HTTP response body contains information about the new session, such as its creation time (created_at attribute), expiration time (expires_at attribute), and time-to-live period in seconds (ttl). Full the full list of returned data elements, see response-data syntax above.\nExamples Request  POST /api/sessions HTTP/1.1 Host: https://dashboard.default-tenant.app.mycluster.iguazio.com Content-Type: application/json { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;myuser\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;MyPass1298\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34; } }  import requests url = \u0026#34;https://dashboard.default-tenant.app.mycluster.iguazio.com/api/sessions\u0026#34; headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} payload = { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;myuser\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;MyPass1298\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34; } } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 201 Created Content-Type: application/json Date: Mon, 18 Mar 2019 00:48:30 GMT Set-Cookie: session=j%3A%7B%22sid%22%3A%20%22240e02ed-2204-4f30-abce-52ebd8456d94%22%7D; max-age=86400; path=/; Transfer-Encoding: chunked { \u0026#34;data\u0026#34;: { \u0026#34;relationships\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6e040a9a-9403-44bd-8f90-a61e079c6c45\u0026#34; } }, \u0026#34;tenant\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;tenant\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;b7c663b1-a8ee-49a9-ad62-ceae7e751ec8\u0026#34; } } }, \u0026#34;attributes\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;group_ids\u0026#34;: [], \u0026#34;uid\u0026#34;: 0, \u0026#34;gids\u0026#34;: [ 65534 ], \u0026#34;tenant_id\u0026#34;: \u0026#34;b7c663b1-a8ee-49a9-ad62-ceae7e751ec8\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2019-03-18T00:48:30.614000+00:00\u0026#34;, \u0026#34;expires_at\u0026#34;: 1552956510, \u0026#34;plane\u0026#34;: \u0026#34;control\u0026#34;, \u0026#34;ttl\u0026#34;: 86400 }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;290818d2-1ded-4c9c-beeb-0940f8dcf0a5\u0026#34; } } Using Postman To send a Create Session request using Postman, follow these steps:\n  Create a new request and set the request method to POST.\n  In the request URL field, enter the following; replace \u0026lt;management-APIs URL\u0026gt; with the HTTPS URL of the platform dashboard:\n\u0026lt;management-APIs URL\u0026gt;/api/sessions/ For example:\nhttps://dashboard.default-tenant.app.mycluster.iguazio.com/api/sessions/   In the Headers tab, add a Content-Type header (Key) and set its value to application/json.\n  In the Body tab, select the raw format and add the following JSON code; replace the \u0026lt;username\u0026gt; and \u0026lt;password\u0026gt; placeholders with your platform login credentials:\n{ \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\u0026lt;password\u0026gt;\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;session\u0026#34; } }   Select Send to send the request, and then check the response. In the case of a successful request —\n The Headers response tab contains a Set-Cookie header with a session element whose value is the session cookie (session=\u0026lt;cookie\u0026gt;). You can also see the cookie in the Cookies response tab (for example, j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D). Copy and save this cookie. You'll need to pass it as the value of the session parameter of the Cookie header in other management-API requests. The Set-Cookie header also contains a max-age element, which contains the session's time-to-live (TTL) period, in seconds; when this period elapses, the session expires and the cookie is no longer valid. The same value is also returned in the data.attributes.ttl response-body data element, which you can see in the Body tab. In the Body tab, you can see the full JSON response data. Among the returned response-data attributes is a ttl attribute that contains the same session TTL value that's returned in the max-age header parameter, and an expires_at attribute that contains the session's expiration time as a Unix timestamp in seconds. The expiration time can also be seen as a date format in the Expires column of the Cookies response tab.     ","keywords":["Create","Session,","management,","sessions,","session","cookies,","cookies,","Set-Cookie","header,","Set-Cookie,","http","headers,","request","header,","http","POST,","POST","method,","POST,","TTL,","user-credential","attributes,","username,","password"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/sessions-api/create-session/","title":"Create Session"},{"content":"Description Creates and configures a new stream. The configuration includes the stream's shard count and retention period. The new stream is available immediately upon its creation.\nRequest Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: CreateStream \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;CreateStream\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the new stream. You can optionally set the stream name in the request's StreamName JSON parameter instead of in the URL.\nRequest Data Syntax  { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardCount\u0026#34;: number, \u0026#34;RetentionPeriodHours\u0026#34;: number }  payload = { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardCount\u0026#34;: number, \u0026#34;RetentionPeriodHours\u0026#34;: number }    Parameters  StreamName A unique name for the new stream (collection) that will be created.\n Type: String   Requirement: Required if not set in the request URL   ShardCount The steam's shard count, i.e., the number of stream shards to create.\n Type: Number   Requirement: Required   Valid Values: A positive integer (\u0026gt;= 1). For example, 100.   Default Value: 1    RetentionPeriodHours The stream's retention period, in hours. After this period elapses, when new records are added to the stream, the earliest ingested records are deleted.\n Type: Number   Requirement: Required   Valid Values: A positive integer (\u0026gt;= 1). For example, 2 (2 hours).   Default Value: 24 (1 day)    Response Response DataNone\nErrors In the event of an error, the response includes a JSON object with an ErrorCode element that contains a unique numeric error code, and an ErrorMessage element that contains one of the following API error messages: Error Message Description   InvalidArgumentException A provided request parameter is not valid for this request.    Permission denied The sender of the request does not have the required permissions to perform the operation.    ResourceInUseException A collection already exists in the specified stream path.    Examples Create a stream named mycontainer with 1000 shards and a retention period of one hour:\nRequest  POST /mycontainer/MyStream/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: CreateStream X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;ShardCount\u0026#34;: 1000, \u0026#34;RetentionPeriodHours\u0026#34;: 1 }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyStream/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;CreateStream\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = {\u0026#34;ShardCount\u0026#34;: 1000, \u0026#34;RetentionPeriodHours\u0026#34;: 1} response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ","keywords":["CreateStream,","create","stream,","streaming,","stream","names,","names,","StreamName,","stream","shards,","shards,","shard","count,","ShardCount,","stream","retention,","retention","period,","RetentionPeriodHours"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/createstream/","title":"CreateStream"},{"content":"Overview When installing the platform, a service account with Owner permissions needs to be created.\nStep 1: GCP Login Log into your GCP Management Console and select the IAM \u0026amp; Admin section.    Step 2: Create a Service Account In the side navigation menu, select Service Accounts, and then select Create Service Account. Type a name and description, then click Create.    Step 3: Set Owner Permissions In the Grant this service account access to project section, choose Owner.    When you're done, click Done.\nStep 4: Create a JSON Key Go to the Service Account that you just created, click the Keys tab, and select Create new key from the Add Key menu.    Select the Key Type JSON and click CREATE.    The key should be downloaded to your workstation. It will be used in the next step.\n","keywords":["creating","a","gcp","service","account,","service","account,","gcp","service","account"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/howto/service-account-create/","title":"Creating a GCP Service Account"},{"content":"Overview At times, the platform's EC2 instances need to access the AWS API. For example, to achieve high availability, the instances within a cluster share a single secondary IP address, which is allocated to a specific instance at any given moment. Migrating this secondary IP to another instance requires an AWS API call to update the internal AWS network. To perform an AWS API call, the platform must be authenticated using AWS credentials with the necessary permissions for performing this operation. To allow this, the platform installer needs to receive the name of an AWS instance profile that contains an IAM role with the required permissions.\nThis guide walks you through the steps for creating the required IAM role for the platform installation, using the AWS Management Console. When using the console to create a role for Amazon EC2, the console automatically creates an instance profile with the same name as the role. You'll need to provide this name as part of the platform installation, as outlined in the platform's AWS cloud installation guide.\nStep 1: AWS Login Log into your AWS Management Console and select the IAM service.    Step 2: Create a New Role In the side navigation menu, select Access management | Roles, and then select Create role.    Step 3: Select the AWS EC2 Use Case Select the AWS service trusted-entry type and the EC2 use case, and then select Next: Permissions.    Step 4: Create a Policy Select Create policy.    Under the JSON tab, paste the contents of this policy and select Review policy. Give the policy a name (for example, \u0026quot;AssignPrivateIPAddresses\u0026quot; — recommended), optionally add a description, and select Create policy.    Step 5: Create the Role Filter the policies for the name of the policy that you created and select the policy.    Select Next: Tags and optionally assign role tags.\nEnter \u0026quot;IguazioDataScienceNode\u0026quot; as the role name, optionally add a description, and select Create role.\nAdditional Resources  Creating IAM Roles (AWS documentation)  See Also  Creating an AWS IAM User AWS cloud installation guide  ","keywords":["creating","an","aws","iam","role,","creating","an","aws","instance","profile,","iam","roles,","iam","instance","profiles,","aws","roles,","aws","instance","profiles,","roles,","instance","profiles,","ec2","instances,","ec2,","aws","network,","network"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/iam-role-n-instance-profile-create/","title":"Creating an AWS IAM Role and Instance Profile"},{"content":"Overview When installing the platform, the installation needs the credentials of your AWS account to create the required infrastructure. This guide walks you through the steps for creating a restricted AWS IAM user, which has only the minimal set of permissions that are required for the installation. You'll need to provide the credentials of this user as part of the platform installation, as outlined in the platform's AWS cloud installation guide.\nStep 1: AWS Login Log into your AWS Management Console and select the IAM service.    Step 2: Create a New User In the side navigation menu, select Access management | Users, and then select Add user.    Step 3: Set the User's Access Permissions and Name In the Set user details section,\n In the User name field, enter the name of the new user (for example, \u0026quot;Provazio\u0026quot; — recommended). In the Access type field, check the Programmatic access option to allow the user only programmatic access.     When you're done, select Next: Permissions.\nStep 4: Create a Policy Select Attach existing policies directly, and then select Create policy.    Download the platform IAM policy file that matches your selected application-cluster configuration:\n provazio.json for a vanilla cluster. provazio-eks.json for an EKS cluster. If you select to use this policy, edit the file to replace all $AWS_ACCOUNT_ID instances with your AWS Account ID.  Paste the contents of your selected policy file in the JSON tab of the AWS Management Console and select Review policy. Give the policy a name (for example, \u0026quot;ManageIguazioSystems\u0026quot; — recommended), optionally add a description, and select Create policy.    Step 5: Create the User Filter the policies for the name of the policy that you created and select the policy.\nSelect Next: Tags and optionally assign user tags.\nSelect Next: Review and review your role definition. When you're ready, select Create user.\nStep 6: Save the User Credential Download and save the credentials of the new user (Access key iD and Secret access key).    Additional Resources  Creating IAM Users (AWS documentation)  See Also  Creating an AWS IAM Role and Instance Profile Deploying an Amazon EKS Application Cluster AWS cloud installation guide  ","keywords":["creating","an","aws","iam","user,","iam","users,","aws","users,","iam","users,","subnet,","iam,","users,","amzon","eks,","eks,","AWS_ACCOUNT_ID"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/iam-user-create/","title":"Creating an AWS IAM User"},{"content":" Dask is a parallel-computation Python library that features scaled pandas DataFrames and allows running distributed Python code that performs fast Python based data processing.\nYou can easily install Dask on your platform cluster — for example, by using pip or Conda. Dask is pre-deployed in the platform's Jupyter Notebook service. You can find examples of using Dask in the platform's tutorial Jupyter notebooks. See specifically the dask-cluster tutorial.\nFor more information about using Dask in the platform, see Ingesting and Preparing Data. For general information about Dask and how to use it, see the Dask documentation.\nSee Also  Working with Services Running Distributed Python Code with Dask Python Machine-Learning and Scientific-Computation Packages Running Applications over GPUs  ","keywords":["dask,","parallel","computation,","parallel","computing,","parallel","anayltics,","parallelizm,","data","analytics,","anayltics,","scaling,","performance,","python","libraries,","python","packages,","python","apis,","python,","jupyter,","jupyter","notebook,","jupyter","tutorials,","v3io","tutorials,","tutorials,","dask-cluster,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/dask/","title":"Dask"},{"content":" Data is stored as within data containers in the distributed file system (DFS) of the platform's data layer (\u0026quot;data store\u0026quot;). A single container can be used to store different types of data — NoSQL (\u0026quot;key-value\u0026quot;), time-series, stream data, or simple data objects (files). The data is stored as simple objects with attribute metadata, and can be grouped into collections according to the data type. For more information, see Data Objects. As explained in the data-layer overview, you can store and access container data using a variety of APIs, which accommodate multiple data formats, without duplicating the data.\nAll platform instances have some predefined containers, and you can create additional custom containers. The best practice is to have a dedicated container per application. You can also organize data within containers in directories (both in the predefined and custom containers). To learn how to create and delete data containers and container directories, browse their contents, and ingest and consume files, see Working with Data Containers.\nNoteSee the data-elements software specifications and restrictions for container-related restrictions.  ","keywords":["data","containers,","containers,","predefined","containers,","container","directories,","data","directories,","data","layer,","data","store,,","distributed","file","system,","dsf,","file","system,","platform","data,","data","access"],"path":"https://github.com/jasonnIguazio/data-layer/containers/","title":"Data Containers"},{"content":" The MLOps Platform has a built-in multi-model data layer (a.k.a. \u0026quot;data fabric\u0026quot;, \u0026quot;data store\u0026quot;, or \u0026quot;database\u0026quot;) for storing and analyzing various types of data structures — such as NoSQL (\u0026quot;key-value\u0026quot;) tables, time-series databases (TSDB), data streams, binary objects, and files. Data is stored as objects within containers. The data objects can represent any supported data type (such as files, table items, or stream records), and objects can be grouped into type-specific collections (such as stream shards, NoSQL tables, or file-system directories).\nThe platform exposes and supports multiple industry-standard and industry-compatible programming interfaces that allow you to perform high-level data manipulation for the supported data formats. You can optionally switch between different APIs for accessing the same data; for example, you can ingest data through one interface and consume it through another interface, depending on you preferences and needs. You can often also access the same data in different formats. The platform's unique unified data model eliminates the need for multiple data stores, constant synchronization, complex pipelines, and painful extract-transform-load (ETL) processes. For more information, see The Data-Layer APIs.\n","keywords":["data","layer,","data","frabric,","data","store,","multi-model","data","layer,","unified","data","model,","nosql,","key","value,","kv,","streaming,","streams,","tsdb,","time","series,","simple","objects,","data","objects,","objects,","s3,","files:"],"path":"https://github.com/jasonnIguazio/data-layer/","title":"Data Layer"},{"content":" Overview There are various tools that allow you to monitor and query your data and produce graphical interactive representations that make it easy to quickly analyze the data and begin discovering new actionable insights in a matter of seconds, with no programming effort.\nGrafana The Grafana open-source platform for data analytics, monitoring, and visualization is pre-integrated in the platform and available as a user application service. NoteIn cloud platform environments, Grafana is currently available as a shared single-instance tenant-wide service.  In addition, there's a predefined single-instance tenant-wide Grafana service for IT administrators, which has dashboards with monitoring data at the application-cluster level. All Grafana services in the platform have predefined dashboards that leverage the platform's monitoring service to display monitoring data, such as performance statistics, for application services. For more information, see Monitoring Platform Services.\nYou can use the Grafana service to define custom Grafana dashboards for monitoring, visualizing, and understanding data stored in the platform, such as time-series metrics and NoSQL data. This can be done by using the custom iguazio data source, or by using a Prometheus data source for running Prometheus queries on platform TSDB tables. You can also issue data alerts and create, explore, and share dashboards. For more information, see Adding Grafana Dashboards.\nSee also the Grafana restrictions in the Software Specifications and Restrictions documentation.\nRemote Visualization Tools All leading BI data visualization tools can be installed remotely and configured to run on top of the data services of the MLOps Platform over a Java database connectivity (JBDC) connector. The following images display data visualization using the popular Tableau, QlikView, and Looker visualization tools:\n  Tableau   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n  QlikView   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\n  Looker   NoteOther integrated services might also contain data-visualization tools.  See Also  Working with Services Monitoring Platform Services Adding Grafana Dashboards Grafana software specifications and restrictions  ","keywords":["data","visualization,","visualization","services,","visualization","tools,","visualization,","data","analytics,","analytics","services,","analytics","tools,","analytics,","data","monitoring,","monitoring","services,","monitoring","service,","monitoring","tools,","monitoring,","remote","visualization,","grafana,","grafana","dashboards,","application-cluster","monitoring,","app-cluster","monitoring,","application-cluster","dashboards,","application","cluster,","looker,","qlikview,","tableau,","remote","analytics,","remote","monitoring"],"path":"https://github.com/jasonnIguazio/services/app-services/data-monitoring-and-visualization-services/","title":"Data Monitoring and Visualization Services and Tools"},{"content":" Data in the platform is stored as data objects (a.k.a. data items) within data containers in the platform's data layer (\u0026quot;data store\u0026quot;). The platform uses object attributes to store object-metadata information. You can save and access data objects of any type — such as files, binary large objects (blobs), table items, and stream records. You can also group data objects of any type into collections — such as stream shards, NoSQL tables, or file-system directories — and perform high-level type-specific data manipulation.\nNoteSee the data-elements software specifications and restrictions for object-related restrictions.  ","keywords":["data","objects,","objects,","data","items,","items,","simple","objects,","data","files,","files,","data","attributes,","attributes,","objet","metadata,","metadata,","data","collectons,","collections,","nosql","items,","nosq,","key-value,","kv,","data","streams,","streams,","stream","records,","records,","time-series,","tsdb,","directories"],"path":"https://github.com/jasonnIguazio/data-layer/objects/","title":"Data Objects"},{"content":"  ","keywords":["data","science","and","mlops","services,","mlrun,","data","science,","mlops,","machine-learning","opeartions,","machine","learning,","ML"],"path":"https://github.com/jasonnIguazio/ds-and-mlops/","title":"Data Science and Machine-Learning Operations (MLOps)"},{"content":" Overview The platform has pre-deployed services for data science and machine-learning operations (MLOps) automation and tracking:\n MLRun Kubeflow Pipelines  MLRun MLRun is Iguazio's open-source MLOps orchestration framework, which offers an integrative approach to managing machine-learning pipelines from early development through model development to full pipeline deployment in production. MLRun offers a convenient abstraction layer to a wide variety of technology stacks while empowering data engineers and data scientists to define the feature and models. MLRun also integrates seamlessly with other platform services, such as Kubeflow Pipelines, Nuclio, and V3IO Frames.\nThe MLRun server is provided as a default (pre-deployed) shared single-instance tenant-wide platform service (mlrun), including a graphical user interface (\u0026quot;the MLRun dashboard\u0026quot; or \u0026quot;the MLRun UI\u0026quot;), which is integrated as part of the Projects area of the platform dashboard.\nThe MLRun client API is available via the MLRun Python package (mlrun), including a command-line interface (mlrun). You can easily install and update this package from the Jupyter Notebook service by using the /User/align_mlrun.sh script, which is available in your running-user directory after you create a Jupyter Notebook platform service. For more information, see Installing and Updating the MLRun Python Package in the platform introduction.\nThe MLRun library features a generic and simplified mechanism for helping data scientists and developers describe and run scalable ML and other data science tasks in various runtime environments while automatically tracking and recording execution code, metadata, inputs, and outputs. The capability to track and view current and historical ML experiments along with the metadata that is associated with each experiment is critical for comparing different runs, and eventually helps to determine the best model and configuration for production deployment.\nMLRun is runtime and platform independent, providing a flexible and portable development experience. It allows you to develop functions for any data science task from your preferred environment, such as a local IDE or a web notebook; execute and track the execution from the code or using the MLRun CLI; and then integrate your functions into an automated workflow pipeline (such as Kubeflow Pipelines) and execute and track the same code on a larger cluster with scale-out containers or functions.\nFor detailed MLRun information and examples, including an API reference, see the MLRun documentation, which is available also in the Data Science and MLOps section of the platform documentation. See also the MLRun restrictions in the platform's Software Specifications and Restrictions.\nYou can find full MLRun end-to-end use-case demo applications as well as a getting-started and how-to tutorials in the MLRun-demos repository repository. These demos and tutorials are pre-deployed in each user's /User/demos directory for the first Jupyter Notebook service created for the user. You can also find a pre-deployed /User/update-demos.sh script for updating the demo files. For details, see End-to-End Use-Case Applications and How-To Demos in the platform introduction. In addition, check out the MLRun functions marketplace — a centralized location for open-source contributions of function components that are commonly used in machine-learning development.\nKubeflow Pipelines Google Kubeflow Pipelines is an open-source framework for building and deploying portable, scalable ML workflows based on Docker containers. For detailed information, see the Kubeflow Pipelines documentation.\nKubeflow Pipelines is provided as a default (pre-deployed) shared single-instance tenant-wide platform service (pipelines), which can be used to create and run ML pipeline experiments. The pipeline artifacts are stored in a pipelines directory in the \u0026quot;users\u0026quot; data container and pipeline metadata is stored in a mlpipeline directory in the same container. The pipelines dashboard can be accessed by selecting the Pipelines option in the platform dashboard's navigation side menu. (This option is available to users with a Service Admin, Application Admin, or Application Read Only management policy.) See Also  Working with Services Data Science and MLOps Introducing the Platform Nuclio Serverless Functions MLRun software specifications and restrictions  ","keywords":["data","science","automation","services,","data","science","automation,","data","science","services,","data","science,","mlops","automation","services,","mlops","automation,","mlops","services,","mlops,","machine-learning","automation","services,","machine-learning","automation,","machine-learning","services,","machine","learning,","ML","automation","services,","ML","automation,","ML","services,","ML,","mlops","services,","ML","services,","machine","learning,","ML,","tracking","services,","machine-learning","tracking,","ML","tracking,","tracking,","mlrun,","kubeflow","pipelines,","kfp,","kubeflopw,","data","science","pipelines,","pipelines,","data","science","workflows,","workflows,","jupyter,","jupyter","notebook,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/mlops-services/","title":"Data Science Automation (MLOps) Services"},{"content":" As explained in the data-layer overview, the platform exposes multiple proprietary and third-party application programming interfaces (APIs) for working with different types of data, including data ingestion and preparation, and allows you to access the same data from different interfaces.\nThe following table shows the provided programming interfaces for working with different types of data in the platform's data store. For full API references, see the data-layer references.\nData Type  Interfaces    NoSQL (Wide-Column Key/Value) Data The platform's NoSQL data store was built to take advantage of a distributed cluster of physical and virtual machines that use flash memory to deliver in-memory performance while keeping flash economy and density. You can access NoSQL data through these interfaces:\n  NoSQL Web API (Amazon DynamoDB equivalent)\n  NoSQL Spark DataFrame\n  V3IO Frames\n     SQL Data You can work with SQL data in the platform through these interfaces:\n Spark SQL and Datasets — for writing and reading (querying) SQL data Presto — for running SQL queries    Time-Series Data You can create and manage time-series databases in the platform's data store through these interfaces:\n V3IO TSDB CLI — for creating and deleting TSDB tables, and ingesting and consuming (querying) time-series data V3IO Prometheus — for querying TSDBs      Stream You can stream data directly into the platform and consume data from platform streams through the following interfaces:\n  Streaming Web API (Amazon Kinesis equivalent)\n  Apache Spark Streaming API, which is supported via the platform's Spark-Streaming Integration API\n  Apache Kafka distributed streaming platform\n  V3IO Frames\n     File / Simple Data Object You can work with data files and simple data objects — such CSV, Parquet, or Avro files, or binary image or video files — through these interfaces:\n  Local Linux file system\n  Simple-Object Web API (Amazon S3 equivalent)\n  Apache Hadoop Compatible File System (HCFS) (the platform's distributed file system (DFS) is HCFS compliant)\n  V3IO Frames\n  Spark SQL and Datasets — for supported data formats such as CSV and Parquet\n  pandas or Dask\n     Note See Data-Layer APIs Overview for a summary of the platform data-layer APIs; API Data Paths for explanations on how to set the data paths for each API; and Data-Layer References for comprehensive references. See the platform's tutorial Jupyter notebooks for code examples and full use-case applications that demonstrate how to use the different APIs. The platform's web APIs (for working with NoSQL, streaming, and simple-object data) are exposed as an application service. The API endpoint URL of this service is available from the dashboard Services page. For more information about working with the web APIs, see the web-APIs reference, and especially Data-Service Web-API General Structure and Securing Your Web-API Requests. See also The Platform's Application Services for information on related application services — and specifically Spark, Presto, TSDB, pandas, and V3IO Frames.    See Also  API Data Paths Data-Layer References Using Presto Ingesting and Preparing Data Platform Services  ","keywords":["data","layer,","data","fabric,","data","store,","distributed","file","system,","dsf,","platform","data","layer,","platform","data","fabric,","platform","distributed","file","system,","platform","file","system,","file","system,","platform","data,","platform","database,","database,","data","services,","data","apis,","platform","apis,","apis,","nosql,","kv,","nosql","store,","kv","store,","nosql","database,","nosql","tables,","data","streams,","streams,","tsdb,","time-series","data","bases,","time-series,","dask,","frames,","v3io","frames,","kafka,","pandas,","prometheus,","v3io","prometheus,","nuclio,","presto,","spark,","spark","dataframes,","spark","datasets,","spark","nosql","dataframe,","spark","sql,","spark","streaming","api,","v3io","tsdb,","tsdb","cli,","tsdb","nuclio","functions,","web","apis,","nosql","web","api,","simple-object","web","api,","streaming","web","api"],"path":"https://github.com/jasonnIguazio/data-layer/apis/overview/","title":"Overview of the Data-Layer APIs"},{"content":" Browse references for custom MLOps Platform (\u0026quot;platform\u0026quot;) data-layer application programming interfaces (APIs). Your can use these APIs to ingest, prepare, store, access, and analyze large amounts of data in different formats — NoSQL (key-value), time-series (TSDB), streams, and simple objects (files).\nNote For an overview of the available platform APIs for each supported data format, see the data-layer APIs overview. For an high-level overview of all platform APIs, see the APIs overview.    ","keywords":["data-layer","references,","dat-layer","api","references,","data-layer","apis,","reference,","api","reference,","apis,","data","apis,","data-ingetion","apis,","data-preparation","apis,","nosql,","tsdb,","streaming,","streams,","simple","objects,","objects,","files"],"path":"https://github.com/jasonnIguazio/data-layer/reference/","title":"Data-Layer References"},{"content":" To work with data, of any format, as a simple data object, first retrieve the container objects using the GET Container operation, and then use the following S3-like RESTful API object operations:\n  GET Object — retrieves an object from a container.   DELETE Object — deletes an object from a container.    PUT Object — adds a new object to a container, or appends data to an existing object. The option to append data is extension to the S3 PUT Object capabilities — see Appending Data.   POST Object — an alias of the PUT Object operation. NoteUnlike S3, POST Object acts in the same way as PUT Object.    HEAD Object — retrieves the metadata of a data object.\n  NoteYou can find examples of Simple Object Web-API data-object operation requests in the Ingesting and Consuming Files tutorial.  Appending Data To append data to an existing container object, use a PUT Object operation (see PUT Object), and include the following header in the request:\nRange: -1 Examples The following example adds the string \u0026quot;The End\u0026quot; at the end of a \u0026quot;MyObject\u0026quot; object:\n PUT /mycontainer/MyObject HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/octet-stream X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf Range: -1 \u0026#34;The End\u0026#34;  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyObject/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application-octet-stream\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34;, \u0026#34;Range\u0026#34;: \u0026#34;-1\u0026#34; } payload = \u0026#34;The End\u0026#34; response = requests.put(url, data=payload, headers=headers) print(response.text)    ","keywords":["data-object","operations,","simple","objects,","S3,","REST,","RESTful,","objects,","data","objects,","objects,","containers,","DELETE","Object,","GET","Object,","HEAD","Object,","POST","Object,","PUT","Object,","create","object,","add","object,","object","metadata"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/simple-object-web-api/data-object-operations/","title":"Data-Object Operations"},{"content":" The data-service web APIs enable complex manipulation of specific data types. These APIs include the Streaming Web API Reference and the NoSQL Web API Reference.\nRequest Syntax The data-service API operations are HTTP requests that you send to the web-APIs (web-gateway) service of a platform tenant using the PUT or POST HTTP method. The operation to perform is specified within an X-v3io-function HTTP header. Data parameters (where required) are passed within the request's HTTP body, in JSON format. The requests conform to the following general format; (the order of the headers isn't important):\n \u0026lt;method\u0026gt; /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: \u0026lt;operation\u0026gt; \u0026lt;authentication header\u0026gt;: \u0026lt;value\u0026gt; {\u0026lt;data parameters\u0026gt;}  import requests url = \u0026#34;\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;\u0026lt;operation\u0026gt;\u0026#34;, \u0026#34;\u0026lt;authentication header\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; } payload = {\u0026lt;data parameters\u0026gt;} response = requests.\u0026lt;method\u0026gt;(url, json=payload, headers=headers) print(response.text)    Following is an explanation of the \u0026lt;...\u0026gt; placeholders used in the request syntax:\n (request) -- \u0026lt;method\u0026gt; The HTTP method for the request — POST or PUT (post or put in Python). POST and PUT behave the same in the platform.  (request) -- \u0026lt;container\u0026gt; An identifier of the data container of the operation's resource. The container can be identified by its name or by its ID. For example, projects.\nNoteIt is strongly recommended that you identify a container by its name and not by its ID. The option to specify a container ID is deprecated and will eventually be removed. For more information, see Container Names and IDs.   (request) -- \u0026lt;resource\u0026gt; URL resource parameters, signifying the full path to the operation's resource within the container, or the first part of this path. The path can point, for example, to a collection (such as stream or a table), a collection component (such as a shard), or an object in the collection (such as a table item). It is also possible to provide the last part of the resource path via data parameters in the request's HTTP body (in JSON format) — see \u0026lt;data parameters\u0026gt; and Setting the Resource Path.  (request) -- \u0026lt;web-APIs URL\u0026gt; The URL of the web-APIs (web gateway) service of a platform tenant.\nGet this URL by copying the API URL of the web-APIs service (webapi) from the Services dashboard page. You can select between two types of URLs:\n HTTPS Direct (recommended) — a URL of the format https://\u0026lt;tenant IP\u0026gt;:\u0026lt;web-APIs port\u0026gt;; for example, https://default-tenant.app.mycluster.iguazio.com:8443. Requests of this format are assigned to web-APIs servers by the DNS server of the web-APIs service: the DNS cache contains a random list of web-APIs servers, and the server attempts to assign each request to the first server on the list; if the first server becomes unavailable, the request is assigned to the next server in the list, and so forth. This is the recommended method in most cases, as it's typically more efficient; a possible exception is a single web-APIs client. HTTPS — a URL of the format https://webapi.\u0026lt;tenant IP\u0026gt;; for example, https://webapi.default-tenant.app.mycluster.iguazio.com. Requests of this format are redirected to web-APIs servers by the Kubernetes ingress of the web-APIs service, which selects a server per request.   NoteTo run the examples in this reference, you must replace the sample web-APIs URL in the examples with a tenant web-APIs URL for your platform environment.   (request) -- \u0026lt;operation\u0026gt; The requested operation to be performed. For example, GetRecords.  (request) -- \u0026lt;authentication header\u0026gt; You must authenticate the identity of the sender of the request by using either an X-v3io-session-key or Authorization header, as outlined in the Securing Your Web-API Requests documentation.  (request) -- \u0026lt;data parameters\u0026gt; Data parameters for the request, where relevant, in JSON format.\nNoteIf the request URL doesn't include the full path to the target resource (see \u0026lt;resource\u0026gt;), the remaining path (such as a table or stream name, an item's primary key, or a shard ID) must be assigned to relevant JSON parameters in the request's HTTP body.  Note that the values of the request data parameters cannot contain path slashes (/). See Setting the Resource Path.    Setting the Resource Path The resource path for the operation can be provided using any of the following alternative methods. The examples are for a GetItem operation that retrieves an item (resource) from a \u0026quot;MyDirectory/Students\u0026quot; table (collection) in a \u0026quot;mycontainer\u0026quot; container. The item's name and primary-key (\u0026quot;StudentId\u0026quot;) value is \u0026quot;0358123\u0026quot;:\n  The full path is provided in the request URL in the header (see \u0026lt;resource\u0026gt;).\nFor example, the GetItem request URL has the full path to the student item —\nPOST /mycontainer/MyDirectory/Students/0358123 HTTP/1.1   The resource path is split across the request URL in the header and the request body: the start of the path is set in the URL, after the container identifier, and the rest is set in the JSON body.\nNoteWhen using this option, the path in the URL must end in a forward slash (/).  For example, the GetItem request URL sets the path to a \u0026quot;MyDirectory/\u0026quot; directory:\nPOST /mycontainer/MyDirectory/ HTTP/1.1 And the request's JSON body sets the TableName parameter to the name of the \u0026quot;Students\u0026quot; table (located in \u0026quot;MyDirectory/\u0026quot;), and the Key parameter to a StudentID attribute with the primary-key value \u0026quot;0358123\u0026quot; (which is also the item's name — see Item Name and Primary Key):\n{ \u0026#34;TableName\u0026#34;: \u0026#34;Students\u0026#34;, \u0026#34;Key\u0026#34;: {\u0026#34;StudentID\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0358123\u0026#34;}} } Another option is for the request URL to set the full path to the \u0026quot;Students\u0026quot; table —  POST /mycontainer/MyDirectory/Students/ HTTP/1.1  url = \u0026#34;\u0026lt;web-APIs URL\u0026gt;/mycontainer/MyDirectory/Students/\u0026#34;    — and for the request's JSON body to set the Key parameter to the item's primary-key attribute (\u0026quot;StudentID\u0026quot; with the value \u0026quot;0358123\u0026quot;):  {\u0026#34;Key\u0026#34;: {\u0026#34;StudentID\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0358123\u0026#34;}}}  payload = {\u0026#34;Key\u0026#34;: {\u0026#34;StudentID\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;0358124\u0026#34;}}}      Response Syntax The HTTP response to a web-API request includes headers that contain the HTTPS status code of the operation and additional information. Some responses also return data in the response's HTTP body, in JSON format. The responses conform to the following general format; (the order of the headers isn't important):\nHTTP/1.1 \u0026lt;HTTP status code\u0026gt; \u0026lt;HTTP reason phrase\u0026gt; Content-Type: \u0026lt;content type\u0026gt; \u0026lt;last modification-time header\u0026gt; ... {\u0026lt;data elements\u0026gt;}  Following is an explanation of the \u0026lt;...\u0026gt; placeholders used in the response syntax:\n (response) -- \u0026lt;content type\u0026gt; The HTTP media type (content type) of the response data. For example, application/json or application/octet-stream.  (response) -- \u0026lt;last modification-time header\u0026gt; Some web-API responses return one or both of the following headers, which contain the last modification time of the operation object (such as a NoSQL table item, a stream shard, or a binary object) in different formats:\n   X-v3io-transaction-verifier — returns the values of the __mtime_secs and __mtime_nsecs system attributes, which together provide the object's last modification time as a Unix timestamp with nano-seconds resolution:\nX-v3io-transaction-verifier: __mtime_secs==\u0026lt;Unix timesamp in seconds\u0026gt; and __mtime_nsecs==\u0026lt;nanoseconds\u0026gt; For example:\nX-v3io-transaction-verifier: __mtime_secs==1542838673 and __mtime_nsecs==250965583   Last-Modified — returns the object's last modification time as a string of the format ddd, dd MMM yyyy HH:mm:ss K:\nLast-Modified: ddd, dd MMM yyyy HH:mm:ss K For example:\nLast-Modified: Wed, 24 Nov 2018 22:17:53 GMT   (response) -- \u0026lt;data elements\u0026gt; Some successful operations return, within the response HTTP body, a JSON object with additional information relating to the operation that was performed. The structure of this data object is operation-specific.  The response object might also contain additional error information for failed operations, as explained in the Error Information section.   Error Information In the event of an error, in addition to the HTTP status code returned in the response header, the operation's response HTTP body includes an error-information JSON object. This object has an ErrorCode element that contains a unique numeric error code for the failed operation (for example, -1 or -201326592), and an ErrorMessage element that contains an error-message string (for example, \u0026quot;Operation not permitted\u0026quot; or \u0026quot;InvalidArgumentException\u0026quot;):\nHTTP/1.1 \u0026lt;HTTP status code\u0026gt; \u0026lt;HTTP reason phrase\u0026gt; Content-Type: application/json ... { \u0026#34;ErrorCode\u0026#34;: \u0026lt;numeric error code\u0026gt;, \u0026#34;ErrorMessage\u0026#34;: \u0026#34;\u0026lt;error message\u0026gt;\u0026#34; } For multi-object operations, error information might be returned separately for each object that produced an error (see, for example, the PutRecords response).\nExamples The following example is of an HTTP request for a GetRecords operation that retrieves records from shard 199 in a \u0026quot;MyDirectory/MyStream\u0026quot; stream within a \u0026quot;mycontainer\u0026quot; container. The path to the shard resource is set entirely in the request URL, and the request's JSON body sets the values of the Location and Limit parameters.\nRequest  POST /mycontainer/MyDirectory/MyStream/199 HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: GetRecords X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;Location\u0026#34;: \u0026#34;AQAAAAAAAAAAAAAAAAAAAA==\u0026#34;, \u0026#34;Limit\u0026#34;: 2 }  import requests url = \u0026quot;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/MyStream/199\u0026quot; headers = { \u0026quot;Content-Type\u0026quot;: \u0026quot;application/json\u0026quot;, \u0026quot;X-v3io-function\u0026quot;: \u0026quot;GetRecords\u0026quot;, \u0026quot;X-v3io-session-key\u0026quot;: \u0026quot;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026quot; } payload = {\u0026quot;Location\u0026quot;: \u0026quot;AQAAAAAAAAAAAAAAAAAAAA==\u0026quot;, \u0026quot;Limit\u0026quot;: 2}\nresponse = requests.post(url, json=payload, headers=headers) print(response.text)\n\n  Response The following response example is for a successful GetRecords operation that returned one matching record:\nHTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;NextLocation\u0026#34;: \u0026#34;AQAAAAsAAAAFAEC2/+sAAA==\u0026#34;, \u0026#34;MSecBehindLatest\u0026#34;: 0, \u0026#34;RecordsBehindLatest\u0026#34;: 0, \u0026#34;Records\u0026#34;: [ { \u0026#34;ArrivalTimeSec\u0026#34;: 1485685671, \u0026#34;ArrivalTimeNSec\u0026#34;: 160186781, \u0026#34;SequenceNumber\u0026#34;: 15756, \u0026#34;PartitionKey\u0026#34;: \u0026#34;MyKey\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;ZGF0YQ0K\u0026#34; } ] } The following response example is for a failed GetRecords operation that returned HTTP status code 400 (Bad Request) and web-API error code -201326594 and error message ShardIDOutOfRangeException, indicating that the shard ID specified in the request (199) doesn't exist in the specified stream (\u0026quot;MyStream\u0026quot;):\nHTTP/1.1 400 Bad Request Content-Type: application/json ... { \u0026#34;ErrorCode\u0026#34;: -201326594, \u0026#34;ErrorMessage\u0026#34;: \u0026#34;ShardIDOutOfRangeException\u0026#34; } See Also  Securing Your Web-API Requests   ","keywords":["data-service","web-api","structure,","api","structure,","data-service","web","apis,","api","reference,","web","apis,","REST,","RESTful,","http,","http","requests,","http","headers,","request","header,","request","parameters,","http","responses,","http","operations,","web-api","operations,","request","url,","url","resource","parameters,","json,","json","parameters,","json","elements,","python,","web","gateway,","nginx,","host,","ports,","api","endpoints,","X-v3io-function,","security,","authentication,","http","authentication,","errors,","http","status,","debugging"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/data-service-web-api-gen-struct/","title":"Data-Service Web-API General Structure"},{"content":"Fully Qualified Name io.iguaz.v3io.spark.streaming.Decoder\nDescription A trait for defining a decoder class that converts a byte array into the specified type.\nSummary Prototype\ntrait Decoder[T] extends Encoding Methods   fromBytes\ndef fromBytes(bytes: Array[Byte])( implicit encoding: Option[Properties] =\u0026gt; Charset) : T   Prototype trait Decoder[T] extends Encoding Type Parameters  T \u0026mdash;  the type into which to covert (decode) the data.  fromBytes Method Converts a byte array into the specified data type.\nSyntax fromBytes(bytes: Array[Byte])( implicit encoding: Option[Properties] =\u0026gt; Charset) : T Parameters  bytes The byte array to convert (decode).\n Type: Array[Bytes]   Requirement: Required   encoding An implicit encoding function that returns a string character encoding.\n Type: encoding: Option[Properties] =\u0026gt; Charset)   Requirement: Implicit    Return Value Returns the converted (decoded) data in the specified data format (T).\n","keywords":["Decoder,","spark","streaming,","decoding"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/encoding-decoding-types/decoder/","title":"Decoder Trait"},{"content":"Fully Qualified Name io.iguaz.v3io.spark.streaming.Decoder.DefaultDecoder\nDescription A class for defining a Decoder object that returns the input byte array without changes.\nSummary Instance Constructors\nclass DefaultDecoder(config: Option[Properties] = None) extends Decoder[Array[Byte]] Methods   fromBytes\ndef fromBytes(bytes: Array[Byte])( implicit encoding: Option[Properties] =\u0026gt; Charset) : Array[Byte]   Instance Constructors Syntax new DefaultDecoder(config: Option[Properties]) Parameters and Data Members  config Optional properties. The class implementation ignores this parameter.\n Type: Option[Properties]   Requirement: Optional (ignored)    fromBytes Method Returns the input byte array without changes.\nSyntax fromBytes(bytes: Array[Byte])( implicit encoding: Option[Properties] =\u0026gt; Charset) : Array[Byte] Parameters  bytes The byte array to return.\n Type: Array[Bytes]   Requirement: Required   encoding An implicit function that returns a character encoding. The class implementation ignores this parameter.\n Type: encoding: Option[Properties] =\u0026gt; Charset)   Requirement: Implicit (ignored)    Return Value Returns the input byte array without changes.\n","keywords":["DefaultDecoder,","spark","streaming,","decoding"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/encoding-decoding-types/defaultdecoder/","title":"DefaultDecoder Class"},{"content":"Description Deletes a container.\nWarningTake extra care when deleting containers, to avoid data loss or other undesirable consequences. It's recommended that you close all open handles to the container before you delete it. For example, deleting a container without first deleting a Nuclio V3IO volume that references the container might result in consumption of extra Kubernetes resources.  Request Request Header Syntax  DELETE /api/containers/\u0026lt;container\u0026gt; HTTP/1.1 Host: \u0026lt;management-APIs URL\u0026gt; Cookie: session=\u0026lt;cookie\u0026gt;  url = \u0026#34;\u0026lt;management-APIs URL\u0026gt;/api/containers/\u0026lt;container\u0026gt;\u0026#34; headers = {\u0026#34;Cookie\u0026#34;: \u0026#34;session=\u0026lt;cookie\u0026gt;\u0026#34;}    HTTP Method DELETE\nURL Resource Parameters  -- \u0026lt;container\u0026gt; The ID of the container to delete.\n Type: Integer   Requirement: Required    Request DataNone\nResponse Response Header Syntax HTTP/1.1 \u0026lt;status code; 200 on success\u0026gt; \u0026lt;reason phrase\u0026gt; Content-Type: application/json Response Data Syntax { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;container_id\u0026#34;: number, \u0026#34;job_id\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container_deletion\u0026#34;, \u0026#34;id\u0026#34;: 0 } } Elements The container_id attribute of the response data contains the numeric ID of the deleted container.\nExamples Delete a container with ID 1030:\nRequest  DELETE /api/containers/1030 HTTP/1.1 Host: https://dashboard.default-tenant.app.mycluster.iguazio.com Cookie: session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D  import requests url = \u0026#34;https://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers/1030\u0026#34; headers = {\u0026#34;Cookie\u0026#34;: \u0026#34;session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D\u0026#34;} response = requests.delete(url, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;container_id\u0026#34;: 1030, \u0026#34;job_id\u0026#34;: \u0026#34;6e7f9bf5-4a7c-4efb-83b7-31785fa82183\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container_deletion\u0026#34;, \u0026#34;id\u0026#34;: 0 } } ","keywords":["Delete","Container,","management,","containers,","http","DELETE,","DELETE","method,","DELETE,","container","IDs"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/containers-api/delete-container/","title":"Delete Container"},{"content":" Description Deletes a NoSQL table or or specific table items.\nNote  In the current release, to delete a partitioned table you need to delete each table partition separately, by issuing multiple delete calls and setting the table parameter in each call to the path to a different partition directory. After deleting all the partitions, you can call the delete method with the root table path to delete the table's schema file and complete the deletion.\n  When the filter parameter isn't set, the entire table and its schema file (.#schema) are deleted.\n    Syntax delete(backend, table[, filter=\u0026#39;\u0026#39;, if_missing=FAIL]])  NoteThe method has additional parameters that aren't currently supported for the NoSQL backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after table.\n  Parameters backend | filter | if_missing | table\n backend The backend type — \u0026quot;nosql\u0026quot; or \u0026quot;kv\u0026quot; for the NoSQL backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a NoSQL table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/nosql/my_table\u0026quot;. You can also set the path to a table partition; see the partitioned-table deletion note regarding the need to delete the table partitions before attempting to delete the entire table.\n Type: str   Requirement: Required   filter A Boolean filter expression that identifies specific items to delete. See Filter Expression for syntax details and examples.  To reference an item attribute in the target table from within the expression, use the attribute name — \u0026lt;attribute name\u0026gt;. For example, an \u0026quot;is_registered==false OR age\u0026lt;18\u0026quot; filter deletes all table items for which either the value of the is_registered attribute is false or the value of the age attribute is smaller than 18.\n Type: str   Requirement: Optional   Default Value: \u0026quot;\u0026quot; — delete the entire table and its schema file   if_missing Determines whether to raise an error when the specified table (table) doesn't exist.\n Type: pb.ErrorOptions enumeration. To use the enumeration, import the frames_pb2 module; for example:\nfrom v3io_frames import frames_pb2 as fpb    Requirement: Optional   Valid Values: FAIL to raise an error when the specified table doesn't exist; IGNORE to ignore this   Default Value: FAIL    Errors In case of an error, the method raises a DeleteError error.\nExamples Following are some usage examples for the delete method of the Frames NoSQL backend:\n  Delete a mytable table in the client's data container (table). Because the filter parameter isn't set, the entire table is deleted.\ntable = \u0026#34;mytable\u0026#34; client.delete(backend=\u0026#34;nosql\u0026#34;, table=table)   Delete a my_tables/students_11-15 table in the client's data container (table); don't raise an error if the table doesn't exist (if_missing = IGNORE):\nfrom v3io_frames import frames_pb2 as fpb table = \u0026#34;/my_tables/students_11-15\u0026#34; client.delete(\u0026#34;nosql\u0026#34;, table=table, if_missing=fpb.IGNORE)   For a my_tables/students_11-15 table in the client's data container (table), delete all items whose age attribute value is greater than 40 (see filter); don't raise an error if the table doesn't exist (if_missing = IGNORE):\nfrom v3io_frames import frames_pb2 as fpb table = \u0026#34;/my_tables/students_11-15\u0026#34; client.delete(\u0026#34;nosql\u0026#34;, table=table, filter=\u0026#34;age\u0026gt;40\u0026#34;, if_missing=fpb.IGNORE)   See Also  Deleting NoSQL Tables Frames NoSQL-Backend Overview Frames Client Constructor  ","keywords":["delete","method,","frames","nosql","delete","method,","frames","kv","delete","method,","frames","delete,","frames","nosql","delete,","frames","kv","delete,","frames","client","delete,","frames","client","nosql","delete,","frames","client","kv","delete,","frames","delete","reference,","frames","nosql","delete","reference,","frames","kv","delete","reference,","deleting","nosql","tables,","deleting","tables,","delete","by","filter,","backend,","filter,","if_missing,table,","IGNORE,","fpb.IGNORE"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/nosql/delete/","title":"delete Method"},{"content":" Description  Deletes a TSDB table or or specific table items.\n NoteWhen no filter parameter is set (start, end, filter, or metrics), the entire TSDB table and its schema file (.schema) are deleted.\n  Syntax delete(backend, table[, filter=\u0026#39;\u0026#39;, start=\u0026#39;\u0026#39;, end=\u0026#39;\u0026#39;, if_missing=FAIL, metrics=[]]) Parameters backend | end | if_missing | start | table\n backend The backend type — \u0026quot;tsdb\u0026quot; for the TSDB backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a TSDB table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/tsdb/my_metrics\u0026quot;.\n Type: str   Requirement: Required   start Start (minimum) time for the delete operation — i.e., delete only items whose data sample time is at or after (\u0026gt;=) the specified start time.\n Type: str   Requirement: Optional   Valid Values: A string containing an RFC 3339 time, a Unix timestamp in milliseconds, a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where m = minutes, h = hours, and 'd' = days), or 0 for the earliest time. For example: \u0026quot;2016-01-02T15:34:26Z\u0026quot;; \u0026quot;1451748866\u0026quot;; \u0026quot;now-90m\u0026quot;; \u0026quot;0\u0026quot;.   Default Value: \u0026quot;\u0026quot; when neither start nor end are set — to delete the entire TSDB table; one hour earlier than the end time (end - 1h) when end is set   end End (maximum) time for the delete operation — i.e., delete only items whose data sample time is before or at (\u0026lt;=) the specified end time.\n Type: str   Requirement: Optional   Valid Values: A string containing an RFC 3339 time, a Unix timestamp in milliseconds, a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where m = minutes, h = hours, and 'd' = days), or 0 for the earliest time. For example: \u0026quot;2018-09-26T14:10:20Z\u0026quot;; \u0026quot;1537971006000\u0026quot;; \u0026quot;now-3h\u0026quot;; \u0026quot;now-7d\u0026quot;.   Default Value: \u0026quot;\u0026quot; when neither start nor end are set — to delete the entire TSDB table;\u0026quot;now\u0026quot; when start is set   if_missing Determines whether to raise an error when the specified TSDB table (table) doesn't exist.\n Type: pb.ErrorOptions enumeration. To use the enumeration, import the frames_pb2 module; for example:\nfrom v3io_frames import frames_pb2 as fpb    Requirement: Optional   Valid Values: FAIL to raise an error when the specified table doesn't exist; IGNORE to ignore this   Default Value: FAIL    Errors In case of an error, the method raises a DeleteError error.\nExamples Following are some usage examples for the delete method of the Frames TSDB backend.\n  Delete a mytsdb TSDB table in the client's data container (table). Because no filter parameter is set (start, end, filter, or metrics), the entire table is deleted.\ntsdb_table = \u0026#34;mytsdb\u0026#34; client.delete(backend=\u0026#34;tsdb\u0026#34;, table=tsdb_table)   Delete a tsdb/my_metrics TSDB table in the client's data container (table); don't raise an error if the table doesn't exist (if_missing = IGNORE):\nfrom v3io_frames import frames_pb2 as fpb tsdb_table = \u0026#34;/tsdb/my_metrics\u0026#34; client.delete(\u0026#34;tsdb\u0026#34;, table=tsdb_table, if_missing=fpb.IGNORE)   For a mytsdb TSDB table in the client's data container (table), delete all items for data that was sampled during the last 24 hours — between now-1d (start) and now (end):\ntsdb_table = \u0026#34;mytsdb\u0026#34; client.delete(\u0026#34;tsdb\u0026#34;, table=tsdb_table, start=\u0026#34;now-1d\u0026#34;, end=\u0026#34;now\u0026#34;)   See Also  Deleting a TSDB (The TSDB CLI) Frames TSDB-Backend Overview Frames Client Constructor  ","keywords":["delete","method,","frames","tsdb","delete","method,","frames","delete,","frames","tsdb","delete,","frames","client","delete,","frames","client","tsdb","delete,","frames","delete","reference,","frames","tsdb","delete","reference,","deleting","tsdb","tables,","deleting","tsdbs,","delete","by","filter,","backend,","end,","if_missing,","start,","table,","IGNORE,","fpb.IGNORE"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/tsdb/delete/","title":"delete Method"},{"content":" Overview This guide describes how to install (deploy) MLOps Platform application nodes to Amazon Elastic Kubernetes Service (Amazon EKS) in an AWS cloud — i.e., deploy an AWS EKS platform application cluster.\nConfiguring an EKS Application Cluster On the Clusters page of the platform installer (Provazio), fill in the EKS related configuration parameters, and then select Next.\n    Kubernetes Kind Set this to  New EKS Cluster.  Root Block Device Type Leave this set to  General Purpose SSD.  Root Block Device Size The size of the EBS for the control plane.  SSH Key Pair Kind The type of key pairing to use for SSH connections to the EKS application cluster:\n New — Create a new SSH key. Existing — Use an existing SSH key. None — Don't use any SSH key.  Note that when this option is set, you cannot use SSH to connect to the EKS application nodes.   SSH Whitelist CIDRs Leave this empty.  Node Groups You can group application nodes in the EKS application cluster into groups.\nThe installer predefines a default node group named \u0026quot;iniital\u0026quot;. You can select the edit icon for this group to edit its configuration, but you cannot delete this group or change its minimal number of instances (1). The minimum number of instances (Min # of instances) for the default node group is currently not configurable.\n   You can select the plus-sign icon (+) to define one or more additional custom node groups. You must select a lifecycle type for the node group. The default is On Demand.\n   Configure the following parameters for either On Demand or Spot node groups.\n Name The name of the node group.  Lifecycle The EC2 instance lifecycle type. Choose On Demand or Spot\nOn Demand—recommended for applications with workloads that cannot be interrupted\nSpot—cost-effective instances which are flexible about when applications are run and if they can be interrupted\n # of Instances The number of instances (nodes) to deploy for this group.  Min # of Instances The minimum number of nodes in the group.  For the default node group, the value of this parameter is currently not configurable. For additional groups, you can set this parameter to any positive number or to zero. For high availability, it's recommended to have a minimum of two application nodes in each group.  Max # of Instances The maximum number of nodes in the group.    If you selected On Demand as your lifecycle type you will need to configure the size of the node group.  Size The EC2 instance size for the nodes in the group.    If you selected Spot as your lifecycle type, you will need to configure the parameters for each node in the group. Min # of CPUs The minimum number of CPUs in each node.  Max # of CPUs The maximum number of CPUs in each node.  Min # of GPUs The minimum number of GPUs in each node.  Max # of GPUs The maximum number of GPUs in each node.  Min amount of memory (GB) The minimum amount of memory (GB) in each node.  Max amount of memory (GB) The maximum amount of memory (GB) in each node.    When you're done, proceed to Step 9 of the platform's AWS installation guide.\nSee Also  Creating an AWS IAM User AWS Installation Guide   ","keywords":["deploying","an","amazon","eks","application","cluster,","amazon","elastic","kubernetes","service,","amazon","eks,","eks,","aws","eks,","eks","application","cluster,","eks","app","cluster,","application","cluster,","app","cluster,","application","nodes,","app","nodes,","provazio"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/eks-app-cluster-deploy/","title":"Deploying an Amazon EKS Application Cluster"},{"content":" Overview To install (deploy) an instance of the platform on a Proxmox VE (PVE) cluster, you need to deploy virtual machines (VMs) that will serve as the platform's data and application nodes. The nodes can reside on the same PVE hypervisor host machine (\u0026quot;PVE host\u0026quot;) or on different PVE hosts, as long as every node VM meets the required hardware specifications (see the prerequisites in this guide and the On-Prem Deployment Specifications. This guide outlines how to deploy platform VMs from VMA GZ files (images archives).\nPrerequisites Before you begin, ensure that you have the following:\n  A PVE platform virtualization package with VMA GZ files for each of the platform nodes, received from Iguazio.\n  Administrative access to a platform PVE cluster with the required networks configuration (see Configuring Virtual Networking (PVE)).\n  PVE-host data stores with a minimum of 400 GB available storage for each of the platform nodes (to be used for running the nodes' VM boot-disk images).\n  Sufficient dedicated physical resources on the PVE hosts to allow running the platform's node VMs without over-provisioning.\n  The following BIOS settings are configured on the PVE hosts:\n Hyper Threading — disabled Advanced | CPU Configuration | Intel Virtualization Technology — enabled Chipset Configuration | North Bridge | IIO Configuration | Intel VT for Directed I/O (VT-d) — enabled    Renaming the Backup Files (PVE 6.2.1–6.2-9) NoteThis step is required only for PVE versions 6.2-1—6.2-9. If you're using PVE version 6.2-10 or newer, skip to the next step.  PVE version 6.2-1 introduced a strict restriction to the names of VM-backup (virtualization) image files, which requires including the image-creation date in the file name. This requirement was canceled in PVE version 6.2-10. The default names of the PVE virtualization files provided by Iguazio don't include the creation date. Therefore, if you're using PVE version 6.2-1–6.2-9, before deploying the VMs, run the following commands to rename the backup image files to add image-creation dates:\nmv vzdump-qemu-data-node-8.cores-122G.ram-400G.ssd.vma.gz vzdump-qemu-108-2020_05_20-10_00_00.vma.gz mv vzdump-qemu-data-node-16.cores-244G.ram-400G.ssd.vma.gz vzdump-qemu-116-2020_05_20-10_00_00.vma.gz mv vzdump-qemu-app-node-8.cores-61G.ram-400G.ssd.vma.gz vzdump-qemu-208-2020_05_20-10_00_00.vma.gz mv vzdump-qemu-app-node-16.cores-122G.ram-400G.ssd.vma.gz vzdump-qemu-216-2020_05_20-10_00_00.vma.gz Copying the Backup Files to the PVE Hosts Copy the image files from the provided PVE platform virtualization package to the /var/lib/vz/dump/ directory on all PVE hosts in the platform's PVE cluster. You can use the scp command to copy the files.\nDeploying the VMs from the Backup Files To import and deploy the platform's node VMs from the provided VMA GZ image files, execute the following procedure for each of the PVE hosts (hypervisors) in the platform's PVE cluster.\nVM IDsAs part of each VM deployment, you assign a unique cluster-wide numeric ID to the VM. It's recommended that you begin with ID 101 and increment the ID for each deployed platform node VM, regardless of the node type and whether the cluster has a single or multiple PVE hosts. For example, for a single data node and a single application node, assign ID 101 to the data node and ID 102 to the application node.    Open a command-line interface with a connection to the PVE host — either by selecting the PVE host in the PVE GUI and then selecting Shell, or by establishing an SSH connection to the PVE host. The commands in the next steps should be run from this command line.\n  Run the following command:\ncd /var/lib/vz/dump/   Deploy the platform's data and application nodes by repeating the following command for each node; it's recommended that you first deploy all the data-node VMs and then all the application-node VMs:\nqmrestore \u0026lt;data-node VM image file\u0026gt; \u0026lt;node ID\u0026gt; --unique For example, the following command deploys a data-node VM with ID 101:\nqmrestore vzdump-qemu-data-node-8.cores-122G.ram-400G.ssd.vma.gz 101 --unique  Note 101 is the VM's cluster-wide unique numeric ID. As explained, it's recommended to use sequential IDs starting with 101 — see the VM IDs note. The --unique option generates a new unique random MAC addresses for the VM upon deployment (image import). By default, the VM is placed on the default \u0026quot;local-lvm\u0026quot; data store, which is fine in most cases. If you want to place the VM on another data store, use the --storage option to specify the name of the desired data store — --storage \u0026lt;data-store name\u0026gt;.      Renaming the Deployed VMs (Optional) In clusters with more than one node VM of each type (data and application) — as is the case for the platform's Operational Cluster configuration — it's recommended that you also rename each deployed node VM to a unique name. For example, for a cluster with three data nodes and three application nodes, you could name the data nodes \u0026quot;data-node-1\u0026quot;, \u0026quot;data-node-2\u0026quot;, and \u0026quot;data-node-3\u0026quot;, and the application nodes \u0026quot;app-node-1\u0026quot;, \u0026quot;app-node-2\u0026quot; and \u0026quot;app-node-3\u0026quot;. You can rename the VMs from the PVE GUI by editing the VM name under \u0026lt;VM\u0026gt; | Options | Name.\nSee Also  PVE Installation Guide On-Prem Deployment Specifications  ","keywords":["proxmox","ve","vm","deployment,","pve","vm","deployment,","proxmox","vm","deployment,","vm","deployment,","virtualization","package,","virtualization","files,","vm","backups,","vm","images,","vm","templates,","vma","backups,","vam,","gz","files,","gzip"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/vms-deploy/","title":"Deploying the Platform Nodes (PVE)"},{"content":" Overview To install (deploy) an instance of the platform on a VMware vSphere cluster, you need to deploy virtual machines (VMs) that will serve as the platform's data and application nodes. The nodes can reside on the same VMware ESXi hypervisor host machine (\u0026quot;ESXi host\u0026quot;) or on different ESXi hosts, as long as every node VM meets the required hardware specifications (see the prerequisites in this guide and the On-Prem Deployment Specifications. This guide outlines how to deploy platform VMs from open virtualization format (OVF) template files and virtual machine disk (VMDK) files.\nPrerequisites Before you begin, ensure that you have the following:\n A vSphere platform virtualization package with OVF and VMDK files for each of the platform nodes, received from Iguazio. Administrative access to a platform vSphere cluster with the required networks configuration (see Configuring Virtual Networking (vSphere). ESXi-host data stores with a minimum of 400 GB available storage for each of the platform nodes (to be used for running the nodes' VM boot-disk images). Sufficient dedicated physical resources on the ESXi hsots to allow running the platform's node VMs without over-provisioning.  Copying the Backup Files to the ESXi Hosts Copy the image files from the provided vSphere platform virtualization package to a location that's accessible to all of the ESXi hosts and the vSphere Web Client in the platform's vSphere cluster. You can the scp command, for example, to copy the files to the ESXi hosts.\nDeploying the VMs from the Backup Files To deploy the platform's node VMs from the provided OVF and VMDK files, execute the following procedure from the vSphere Web Client for each of the platform's node VMs:\n  Select Virtual Machines | Create / Register VM | Deploy a Virtual Machine from an OVF or OVA.\n  In Select creation type select Deploy a virtual machine from an OVF or OVA file, and select Next.\n  Enter a name for the VM; for example, \u0026quot;iguazio-data-node\u0026quot;.\n  Select Click to select files or drag/drop.\n  Navigate to the directory of the data-node VM; select the relevant OVF SSD file and the VMDK file for the node — for example, data-node-8.cores-122G.ram-400G.ssd and disk-0.vmdk; and select Next.\n  Select a vSphere data store with sufficient space to host the VM boot disk (400 GB minimum), and select Next.\n  Map networks to matching port groups — VM Network for the management network, Client for the data-path (client) network, and Interconnect for the interconnect network (data-nodes only). For more information on the platform networks, see Configuring Virtual Networking (vSphere).\n  Select Thick to configure thick disk provisioning, and select Next.\n  Deselect (uncheck) the Power on automatically option.\n  Select Finish to complete the deployment.\n  See Also  Deploy an OVF or OVA Template (VMware vSphere documentation) VMware vSphere installation guide On-Prem Deployment Specifications  ","keywords":["vmware","vsphere","vm","deployment,","vsphere","vm","deployment,","vmware","vm","deployment,","vm","deployment,","virtualization","package,","virtualization","files,","vm","backups,","vm","images,","vm","templates,","ovf","files,","ovf"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/vms-deploy/","title":"Deploying the Platform Nodes (vSphere)"},{"content":"The MLOps Platform (\u0026quot;the platform\u0026quot;) can be installed (deployed) on bare-metal, virtual-machine (VM), and supported cloud environments. This section includes a software support and certification matrix; software specifications and known restrictions; and installation and setup (deployment) instructions and related infrastructure specifications for the supported deployment environments.\n","keywords":["deployment","and","specifications,","deployment","and","specs,","deployment,","specifications,","specs,","installation,","setup,","configurations,","support","matrix,","software","specifications,","software","specs,","hardware","specifications,","hardware","specs,","hardware","confiugraions,","restrictions,","known","issues,","limitations"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/","title":"Deployment and Specifications"},{"content":"Description Retrieves a stream's configuration, including the shard count and retention period.\nRequest Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: DescribeStream \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;DescribeStream\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the target stream. You can optionally set the stream name in the request's StreamName JSON parameter instead of in the URL.\nRequest Data Syntax  { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, }  payload = {\u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;}    Parameters  StreamName The name of the stream for which to retrieve the description.\n Type: String   Requirement: Required if not set in the request URL    Response Response Data Syntax { \u0026#34;ShardCount\u0026#34;: number, \u0026#34;RetentionPeriodHours\u0026#34;: number } Elements  ShardCount The steam's shard count (the total number of shards in the stream).\n Type: Number   RetentionPeriodHours The stream's retention period, in hours. After this period elapses, when new records are added to the stream, the earliest ingested records are deleted.\n Type: Number    Errors In the event of an error, the response includes a JSON object with an ErrorCode element that contains a unique numeric error code, and an ErrorMessage element that contains one of the following API error messages: Error Message Description   InvalidArgumentException A provided request parameter is not valid for this request.    Permission denied The sender of the request does not have the required permissions to perform the operation.   ResourceNotFoundException The specified resource does not exist.     Examples Retrieve configuration information for a MyStream stream:\nRequest  POST /mycontainer/MyStream/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: DescribeStream X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyStream/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;DescribeStream\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } response = requests.post(url, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;RetentionPeriodHours\u0026#34;: 1, \u0026#34;ShardCount\u0026#34;: 1000 } ","keywords":["DescribeStream,","describe","stream,","streaming,","stream","configuration,","stream","shards,","shards,","shard","count,","ShardCount,","stream","retention,","retention","period,","RetentionPeriodHours"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/describestream/","title":"DescribeStream"},{"content":"Fully Qualified Name io.iguaz.v3io.spark.streaming.Encoding\nDescription A trait for defining a class that returns a string character encoding.\nSummary Prototype\ntrait Encoding Methods   encoding\ndef encoding(config: Option[Properties] = None): Charset   Prototype trait Encoding encoding Method Returns a character encoding.\nSyntax encoding(config: Option[Properties] = None): Charset Parameters  config Optional parameter for setting the returned character encoding. When provided, if the properties include the \u0026quot;v3io.streaming.encoding\u0026quot; property, the method returns a Charset instance that corresponds to the value of this property. The property value can be any StandardCharsets constant (for example, \u0026quot;UTF_8\u0026quot;).\n Type: Option[Properties]   Requirement: Optional   Default Character Encoding: When the config parameter is not provided or does not include the character-encoding configuration property, the method returns a UTF8 character encoding — UTF-8 Charset.\n    Return Value Returns a character encoding (Charset).\n","keywords":["Encoding,","spark","streaming,","encoding"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/encoding-decoding-types/encoding/","title":"Encoding Trait"},{"content":" Description Executes the specified command. This method is used to extend the basic functionality of the other client methods. Currently, the NoSQL backend supports an infer_schema command with an infer alias for inferring the table schema.\nSyntax  execute(backend, table, command=\u0026#34;\u0026#34; NoteThe method has additional parameters that aren't currently supported for the NoSQL backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after table.\n  Parameters and Data Members  backend | command | table\n backend The backend type — \u0026quot;nosql\u0026quot; or \u0026quot;kv\u0026quot; for the NoSQL backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a NoSQL table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/nosql/my_table\u0026quot;.\n Type: str   Requirement: Required   command The command to execute.\n Type: str   Requirement: Required   Valid Values: The NoSQL backend currently supports an \u0026quot;infer_schema\u0026quot; or \u0026quot;infer\u0026quot; command, which infers the schema of a given NoSQL table and creates a schema for the table. The command creates a .#schema file in the table directory. For more information, see Table Schema in the Frames NoSQL backend overview. All references to the infer_schema command in the documentation apply also to the infer alias.    Examples infer_schema Examples Following are examples of using the infer_schema command of the execute method or its infer alias:\n  Infer the schema of a mytable table in the client's data container (table) using the infer command (command):\nclient.execute(backend=\u0026#34;nosql\u0026#34;, table=\u0026#34;mytable\u0026#34;, command=\u0026#34;infer\u0026#34;)   Infer the schema of a my_tables/students table in the client's data container (table) using infer_schema (command):\nclient.execute(\u0026#34;nosql\u0026#34;, table=\u0026#34;/my_tables/students\u0026#34;, command=\u0026#34;infer_schema\u0026#34;)   See Also  NoSQL Table Schema Reference Frames NoSQL-Backend Overview  Table Schema   Frames Client Constructor  ","keywords":["execute","method,","frames","nosql","execute","method,","frames","kv","execute","method,","frames","execute,","frames","nosql","execute,","frames","kv","execute,","frames","client","execute,","frames","client","nosql","execute,","frames","client","kv","execute,","frames","execute","reference,","frames","nosql","execute","reference,","frames","kv","execute","reference,","frames","nosql","infer_schema,","frames","kv","infer_schema,","frames","nosql","infer,","frames","kv","infer,","frames","infer_schema","command,","frames","infer","command,","infer_schema","command,","infer","command,","frames","nosql","infer","schema,","frames","infer","schema,","nosql","infer","schema,","infer","schema,","infer","table","schema,","args,","backend,","command,","table,","key"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/nosql/execute/","title":"execute Method"},{"content":"FUNCTION() FUNCTION represents a function that can be used within an expression. This section documents the supported platform expression functions.\n","keywords":["expression","functions,","expressions,","functions,","FUNCTION"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/functions/","title":"Expression Functions"},{"content":"The platform supports expressions that perform calculations, logical evaluations, and comparisons, by applying operators to operands.\nNote Function operators are described separately. See Expression Functions. For update-expression keyword operators, see Update Expression. The names expression operators are reserved names in the platform. For more information, see Reserved Names.    A binary operator receives two operands:\nOPERAND-A OPERATOR OPERAND-B A unary operator receives a single operand:\nOPERATOR OPERAND There are also special operators such as the array operator ([ ]). An OPERAND can be any expression that evaluates to a value that matches the types supported by the related operator.\nThis section documents the operators can be used in expressions.\n","keywords":["expression","operators,","expressions,","operators,","binary","operators,","unary","operators,","OPERATOR,","OPERAND,","arithmetic","operators,","assignment","operator,","bitwise","operators,","comparison","operators,","logical","operators",",","IN","operator,","techpreview,","tech","preview,","arrays,","array","attributes,","array","operator,","[]","opeartor"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/","title":"Expression Operators"},{"content":"The following Boolean string-comparison functions are supported in expressions.  All functions receive two string parameters. Each parameter can be provided either as a constant literal string (LITERAL), or as an attribute variable (ATTRIBUTE) that represents a string attribute (of type \u0026quot;S\u0026quot;) whose value will be used in the comparison.\nends ends(s1, s2) Checks whether the first string (s1) ends with the second string (s2).\nFor example, ends('4-Jun-2018', year) returns true for a year string attribute whose value is \u0026quot;2018\u0026quot;.\ncontains contains(s1, s2) Checks whether the first string (s1) contains the second string (s2).\nFor example, contains(rainbow, 'yellow') returns true for a rainbow attribute whose value is \u0026quot;red, orange, yellow, green, blue, indigo, violet\u0026quot;.\nlength length(s) Returns the length of the provided string (s) — i.e., the number of characters in the string.  s can also be an expression that evaluates to a string.\nFor example:\n length(\u0026quot;123\u0026quot;) returns 3. length(mystr) for a mystr attribute whose value is \u0026quot;Hello World!\u0026quot; returns 12. length ('abc' + 'efg') returns 6.  Notelength is supported in update expressions.  starts starts(s1, s2) Checks whether the first string (s1) starts with the second string (s2).\nFor example, starts(fullName, 'Jo') returns true for a fullName attribute whose value is \u0026quot;Jon Snow\u0026quot;.\nSee Also  Attribute Variables  ","keywords":["expression","string","functions,","string","functions,","starts","function,","starts,","ends","function,","ends,","contains","function,","contains,","strings,","attribute","variables,","string","attributes,","data","types,","string,","literals,","LITERAL"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/functions/string-functions/","title":"Expression String Functions"},{"content":" The platform APIs support the use of expressions to define execution logic and enhance the support for working with NoSQL items (objects) and their attributes.\nCondition expressions define a logical condition for performing a specific operation.  Update expressions can be used to add and initialize item attributes, update the values of existing attributes or elements in an array attribute, or remove (delete) attributes. This reference outlines the supported expressions and their syntax.\nExpression The EXPRESSION notation represents a basic expression syntax, which can also be included in other expressions. An expression is typically defined as a string that can include a combination of any of the following components (depending on the context).\nNoteThe document uses XXX notations, such as FUNCTION and ATTRIBUTE, as placeholder identifiers for the related components.   Literals (LITERAL) Attribute Variables (ATTRIBUTE) Functions (FUNCTION) Operators and operands (OPERATOR and OPERAND) Other expressions (EXPRESSION).  Note The names of the expression functions and operands are case insensitive. For example, color in (red, blue) and color IN (red, blue) both work (see the in operator). String values within the expression are enclosed within single quotes (' '). Expression keywords, including the names of expression operators, are reserved names in the platform. For more information, see Reserved Names. See the Software Specifications and Restrictions for known expression restrictions.    Literals LITERAL LITERAL represents a constant literal value. For example: 'blue'; 5; true.\nAttribute Variables ATTRIBUTE ATTRIBUTE in expressions represents an attribute variable, which is the name of an item attribute — for example, first-name or color.\nAttribute variables can refer to any supported attribute type, including system attributes such as __mtime_secs or __uid.\nNote\u0026quot;attribute value\u0026quot; in the expressions documentation refers to the attribute data value. In the NoSQL Web API Reference, this refers to the value of an Attribute Value object.  If an attribute name is a reserved word, such as a function name, enclose the word within acute-accent characters (`) — `\u0026lt;reserved word\u0026gt;`. For example, for an attribute named exists, use \u0026quot;`exists` == true\u0026quot;.\nNoteExpressions enable you to use attributes as variables in arithmetic or logical calculations and evaluations. For example, you can define an i attribute to be used as a loop iterator, and increment or decrement its value sequentially.  Array-Attribute Expression Variables The platform enables you to define array attributes as special blob attributes that identify Base64 encoded integer or double arrays. NoteIn the current release, the support for array attributes and the use of array operators and functions in expressions is restricted to the web APIs.  Array attributes are defined in SET update expressions — for example, in the UpdateExpression request parameter of the NoSQL Web API UpdateItem operation. You can either use the init_array expression function to define and initialize a new array attribute (for example, \u0026quot;SET arr=init_array(100,'double')\u0026quot;), or assign an existing array attribute or a slice of such an attribute to a new array attribute (for example, \u0026quot;SET newArr=oldArr\u0026quot; or \u0026quot;SET newArr=oldArr[0..99]\u0026quot;).\nThe elements of an array attribute can be referenced in expressions using the array operator ([ ]). Boolean Conversions In most cases, Boolean values can also be used as numeric operands: the platform implicitly converts true to 1 and false to 0. For example, the following update expression is valid, implicitly converting the results of the embedded Boolean expressions to 1 or 0: res=(1 in (1,2,3,4,5))*10 + (starts('abc','xx'))*100 + (ends('abc','bc'))*1000 + (contains('abxxc','xx'))*100000;. However, this doesn't apply to attribute comparisons because the platform verifies that the compared value matches the current attribute data type. For example, if the value of attribute a is 5 (integer) and the value of attribute b is false (Boolean), the expressions a==true and b==0 will both fail because of a type mismatch. But an expression such as \u0026quot;c = (a==5)*4 + b;\u0026quot; will succeed — implicitly converting the result of the a==5 comparison from true to 1 and the value of b (false) to 0 and setting the value of attribute c to 4 (1*4+0). See Also  Condition Expression and Update Expression. Expression functions and operators Objects Attributes Attribute Data Types Reference System-Attributes Reference The Data-Layer APIs Working with NoSQL Data Expressions software specifications and restrictions  ","keywords":["expressions,","condition","expressions,","filter","expressions,","update","expressions,","update","items,","nosql,","nosql","items,","nosql","tables,","tables,","table","items,","attributes,","attribute","variables,","attributes","arithmetic,","attribute","values,","literals,","expression","functions,","functions,","expression","operators,","operators,","ATTRIBUTE,","EXPRESSION,","LITERAL,","OPERATOR,","OPERAND,","tech","preview,","arrays,","array","attributes,","array","operator,","[]","operator"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/overview/","title":"Expressions Overview"},{"content":" Browse reference documentation for the MLOps Platform's cross-API expression syntax for defining execution logic such as conditional updates and filters, namely for working with NoSQL (key-value) items (objects).\n","keywords":["expressions,","reference,","condition","expressions,","filter","expressions,","update","expressions"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/","title":"Expressions Reference"},{"content":" Browse reference documentation for version 0.8 of the Iguazio V3IO Frames (\u0026quot;Frames\u0026quot;) multi-model open-source Python data-access library for working with NoSQL (key-value) and time-series (TSDB) data in the data store of the MLOps Platform.\n","keywords":["frames","apis,","api","reference,","v3io","frames,","frames,","frames","reference,","dataframes,","nosql,","key-value,","kv,","tsdb,","streaming,","python"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/","title":"V3IO Frames Python API Reference"},{"content":" V3IO Frames currently supports item attributes (DataFrame columns) of the following pandas DataFrame (NumPy) data types. For a description of the DataFrame types, see the pandas and NumPy documentation. For a general reference of the attribute data types that are supported in the platform, see the Attribute Data Types Reference.\nNoteThe platform implicitly converts between pandas DataFrame column data types and platform table-schema attribute data types, and converts integer and short values (int\u0026lt;n\u0026gt;) to long values (int64 / \u0026quot;long\u0026quot;) and floating-point values (float\u0026lt;n\u0026gt;) to double-precision values (float64 / \u0026quot;double\u0026quot;). The \u0026quot;Schema of Data Type\u0026quot; column in the following table indicates the matching platform attribute data type for each pandas data type.  pandas (NumPy) Data Type  Schema Data Type   bool | BooleanDType \u0026quot;boolean\u0026quot;  category \u0026quot;string\u0026quot; float16 | float32 | float64 \u0026quot;double\u0026quot;  int8 | int16 | int32 | int64 \u0026quot;long\u0026quot;  object (string) | StringDtype \u0026quot;string\u0026quot;  datetime64 \u0026quot;timestamp\u0026quot;   See Also  Objects Attributes Attribute Data Types Reference NoSQL Table Schema Reference Frames Overview  ","keywords":["frames","attribute","data","types,","v3io","frames,","frames,","data","types,","attribute","data","types,","pandas","data","types,","pandas","dataframes,","pandas,","nosql,","nosql","table","schema,","table","schema,","schema","types,","boolean,","bool,","category,","double,","float,","float16,","float32,","float64,","float128,","int,","int8,","int16,","int32,","int64,","long,","object,","string"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/attribute-data-types/","title":"Attribute Data Types"},{"content":" Description All Frames operations are executed via an object of the Client class, which is created by using the Client constructor.\nSyntax Client(address=\u0026#34;\u0026#34;, container=\u0026#34;\u0026#34;[, data_url=\u0026#34;\u0026#34;, user=\u0026#34;\u0026#34;, password=\u0026#34;\u0026#34;, token=\u0026#34;\u0026#34;]) Parameters and Data Members address | container | data_url | password | token | user\n address The address of the Frames service.  When running locally on the platform, set this parameter to \u0026quot;framesd:8081\u0026quot; for a gRPC client (recommended) or to \u0026quot;http://framesd:8080\u0026quot; for an HTTP client. (In the platform Jupyter Notebook and web-shell services, the gRPC and HTTP port numbers of the Frames service are stored in predefined FRAMESD_SERVICE_PORT_GRPC and FRAMESD_SERVICE_PORT environment variables, respectively.)  When connecting to the platform remotely, set this parameter to the API address of a Frames platform service in the parent tenant. You can copy this address from the API column of the V3IO Frames service on the Services dashboard page.\n Type: str   Requirement: Required   container The name of the data container that contains the backend data. For example, \u0026quot;projects\u0026quot; or \u0026quot;users\u0026quot;.\n Type: str   Requirement: Required   data_url A web-API base URL for accessing the backend data. By default, the client uses the data URL that's configured for the Frames service, which is typically the HTTPS URL of the web-APIs service of the parent tenant.\n Type: str   Requirement: Optional   token A valid platform access key that allows access to the backend data. See User Authentication.\n Type: str   Requirement: Required when neither the user or password parameters or the authentication environment variables are set.   user The username of a platform user with permissions to access the backend data. See User Authentication.\n Type: str   Requirement: Required when neither the token parameter or the authentication environment variables are set.  When the user parameter is set, the password parameter must also be set to a matching user password.   password A platform password for the user configured in the user parameter. See User Authentication.\n Type: str   Requirement: Required when the user parameter is set.    Return Value Returns a new Frames Client object.\nExamples The following example, for local platform execution creates a Frames client for accessing data in the \u0026quot;users\u0026quot; data container. The identity of the user is authenticated by setting the token parameter of the Client constructor to a platform access key:\nimport v3io_frames as v3f client = v3f.Client(\u0026#34;framesd:8081\u0026#34;, container=\u0026#34;users\u0026#34;, token=\u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34;) The following example is similar to the previous exactly except that the identity of the user is authenticated by setting the user and password parameters of the Client constructor to the name of a platform user and the matching password:\nimport v3io_frames as v3f client = v3f.Client(\u0026#34;framesd:8081\u0026#34;, container=\u0026#34;users, user=\u0026#34;iguazio\u0026#34;, password=\u0026#34;mypass\u0026#34;) See Also  Frames Overview  Initialization User authentication Client Methods    ","keywords":["frames","client","constructor,","frames","client,","frames","client","initialization,","frames","client","init,","user","authentiation,","authentication,","Frames","authentication,","address,","container,","data_url,","password,","token,","user"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/client-constructor/","title":"Client Constructor"},{"content":" This section provides a reference of the V3IO Frames NoSQL backend (\u0026quot;nosql\u0026quot; | \u0026quot;kv\u0026quot;) and the Client methods that it supports.\n","keywords":["frames","nosql","backend,","frames","kv","backend,","frames","nosql","client","methods,","frames","kv","client","methods,","frames","nosql","methods,","frames","kv","methods,","frames","nosql","api,","frames","key-value","api,","frames","kv","api,","frames","nosql,","frames","key-value,","frames","kv,","frames","nosql","reference,","frames","key-value","reference,","frames","kv","reference,","nosql,","key-value,","kv"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/nosql/","title":"NoSQL Backend"},{"content":"Introduction The NoSQL backend of the Frames API supports Client methods and commands for working with NoSQL data in the platform using the NoSQL database service, which enables storing and consuming data in a tabular format. For more information, see Working with NoSQL Data.\n  To add, replace, or update a table item, use the write method.\nTo create a table, just begin adding items. The table will be created automatically with the first item that you add. See also Working with NoSQL Data.\n NoteIn the current release —\n When updating a table item (i.e., when writing an item with the same primary-key attribute value as an existing table item), the existing item is overwritten and replaced with the written item. All items that are written to a given table must conform to the same schema. For more information, see Table Schema in this overview.      To retrieve items or specific item attributes from a table, use the read method.\n  To infer the schema of a NoSQL table and create a schema file, use the infer_schema or infer command of the execute method. Note that that the schema is automatically inferred when using the write method. For more information, see Table Schema in this overview.\n  To delete a table or specific table items, use the delete method. You can also delete a table by using a file-system command. See Working with NoSQL Data.\n  Item Name and Primary Key A table item is a data object, and as such needs to be assigned a unique primary-key value, which serves as the item's name and is stored by the platform in the __name system attribute — see Object Names and Primary Keys. When using the write method to add or update an item in a NoSQL table, you must provide the item's primary-key value (name) by designating the relevant DataFrame column as an index column. For more information see the index-column note in the documentation of the write method.\nTable Schema The NoSQL Frames backend handles structured data. Therefore, the backend needs to be aware of the schema of the data structure. When writing NoSQL data by using the platform's Frames or Spark NoSQL DataFrame APIs, the schema of the data table is automatically identified and saved and then retrieved when reading the data with Frames, a Spark DataFrame, or Presto. However, to use Frames, a Spark DataFrame, or Presto to read NoSQL data that was written to a table in another way (such as using the platform's NoSQL web API), you first need to define the table schema. You can do this by using the infer-schema command of the execute method (infer_schema / infer). For more information about NoSQL table schemas in the platform, see the NoSQL Table Schema Reference.\nNote that all items in the table must conform to the same schema (i.e., have the same attribute names and types). In the current release, the table schema is updated according to the last write operation and you can only read from the table items that match the updated table schema. See Also  Working with NoSQL Data Object Names and Primary Keys NoSQL Table Schema Reference Frames Overview  ","keywords":["frames","nosql","backend","overview,","frames","kv","backend","overview,","frames","nosql","overview,","frames","kv","overview,","frames","nosql","client","methods,","frames","kv","client","methods,","frames","nosql","methods,","frames","kv","methods,","frames","nosql","api,","frames","kv","api,","frames","nosql,","frames","kv,","nosql,","kv"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/nosql/overview/","title":"Overview of the Frames NoSQL Backend"},{"content":" Introduction Iguazio V3IO Frames (\u0026quot;Frames\u0026quot;) is a multi-model open-source data-access library that provides a unified high-performance DataFrame API for working with NoSQL (key-value) and time-series (TSDB) data in the data store of the MLOps Platform (\u0026quot;the platform\u0026quot;). This reference describes the library's DataFrame API for Python 3.7. See also the Frames restrictions in the Software Specifications and Restrictions documentation.\nIn addition to the examples provided in this reference, you can find many examples of using the Frames API in the platform's tutorial Jupyter notebooks. It's recommended that you begin your development with the frames.ipynb notebook, which has getting-started examples and related documentation.\nCreating a Frames Service To use Frames, you first need to create a Frames service:\n  On the Services dashboard page, select New Service from the top action toolbar.\n  In the Basic Settings tab, select V3IO Frames from the Service type drop-down menu. You can optionally edit the service name and description. NoteYou can only create a single shared tenant-wide instance of the Frames service. If the V3IO Frames option is disabled, this means that the parent tenant already has a Frames service (as indicated in the tooltip for this service type). In this case, cancel the service creation, locate the Frames service in the services table, and verify that it's enabled. Otherwise, proceed to the next step.    Proceed to the Common Parameters tab and optionally change the default resource configuration; this isn't advisable for new users.\n  Proceed to the Custom Parameters tab and select Save Service to save your changes.\n  Select Apply Changes from the top action toolbar of the Services page to deploy your changes. When the deployment completes, you should be able to see the Frames service in the services table, as demonstrated in the following image:\n     Initialization To use the Frames API, you need to import the (v3io_frames) Python library. For example:\nimport v3io_frames as v3f Then, you need to create and initialize an instance of the Client class; see Frames Client Constructor. After you have a client object, you can use the Client methods to perform different data operations for the supported backend types.\nBackend Types All Frames client methods receive a backend parameter for setting the Frames backend type. Frames supports the following backend types:\n nosql or kv — a NoSQL backend for working with platform NoSQL (key/value) tables. See Frames NoSQL-Backend API Reference. tsdb — a time-series database (TSDB) backend for working with TSDB tables. See Frames TSDB-Backend API Reference.  csv — a CSV backend for working with comma-separated-value (CSV) files. This backend type is used only for testing purposes.  NoteThe stream backend type isn't supported in the current release.  Client Methods The Client class features the following methods for supporting basic data operations on a data collection, such as a NoSQL or TSDB table:\n create — creates a new collection. NoteThe create method isn't applicable to the NoSQL backend (nosql | kv), because NoSQL tables in the platform don't need to be created prior to ingestion; when ingesting data into a table that doesn't exist, the table is automatically created. See Working with NoSQL Data.   delete — deletes a collection or specific collection items. read — reads data from a collection into pandas DataFrames. write — writes data from pandas DataFrames to a collection. execute — executes a backend-specific command on a collection. Each backend may support multiple commands.  While most methods and commands are supported by all backends, there are differences in the supported parameters and their significance. Therefore, this reference provides separate method documentation for each backend type; see the NoSQL and TDSB backend API references. User Authentication When creating a Frames client, you must provide valid platform credentials for accessing the backend data, which Frames will use to identify the identity of the user. This can be done by using any of the following alternative methods (documented in order of precedence):\n  Provide the authentication credentials in the Client constructor parameters by using either of the following methods:\n  Set the token constructor parameter to a valid platform access key with the required data-access permissions. You can get the access key from the Access Keys window that's available from the dashboard user-profile menu, or by copying the value of the V3IO_ACCESS_KEY environment variable in a web-shell or Jupyter Notebook service.\n  Set the user and password constructor parameters to the username and password of a platform user with the required data-access permissions.\nNoteYou cannot use both methods concurrently: setting both the token and user and password parameters in the same constructor call will produce an error.\n      Set the authentication credentials in environment variables, by using either of the following methods:\n Set the V3IO_ACCESS_KEY environment variable to a valid platform access key with the required data-access permissions. NoteThe platform's Jupyter Notebook service automatically defines the V3IO_ACCESS_KEY environment variable and initializes it to a valid access key for the running user of the service.   Set the V3IO_USERNAME and V3IO_PASSWORD environment variables to the username and password of a platform user with the required data-access permissions. Note When the client constructor is called with authentication parameters (option #1), the authentication-credentials environment variables (if defined) are ignored. When V3IO_ACCESS_KEY is defined, V3IO_USERNAME and V3IO_PASSWORD are ignored.        See Also  Frames Client Constructor Frames NoSQL-Backend API Reference Frames TSDB-Backend API Reference  Frames Attribute Data Types Frames software specifications and restrictions  ","keywords":["frames","apis,","v3io","frames,","frames,","frames","reference,","frames","overview,","frames","backends,","frames","methods,","frames","initialization,","frames","init,","frames","client,","frames","client","backends,","frames","client","methods,","dataframes,","pandas","dataframes,","pandas,","nosql,","key-value,","kv,","tsdb,","streaming,","stream"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/overview/","title":"Overview of the Frames API"},{"content":" This section provides a reference of the V3IO Frames TSDB backend (\u0026quot;tsdb\u0026quot;) and the Client methods that it supports. For general information about working with time-series databases in the platform, see Time-Series Databases (TSDB).\n","keywords":["frames","tsdb","backend,","frames","tsdb","client","methods,","frames","tsdb","methods,","frames","tsdb","api,","frames","tsdb,","frames","tsdb","reference,","tsdb,","time","series"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/tsdb/","title":"Time-Series Database (TSDB) Backend"},{"content":"Introduction The TSDB backend of the Frames API supports Client methods and commands for working with time-series databases (TSDBs). For more information, see also Time-Series Databases (TSDB).\n  To create a new TSDB table, use the create method.\n  To add (ingest) items to a TSDB table, use the write method.\n  To retrieve (consume) items from a TSDB table, use the read method.\n  To delete a TSDB table or specific table items, use the delete method. You can also delete a TSDB table or specific items by using the TSDB CLI or a file-system command.\n  NoteSee also the TSDB software restrictions, in addition to the Frames restrictions.  See Also  Time-Series Databases (TSDB) Frames TSDB-Backend API Reference Frames Overview TSDB sofware specifications and restrictions  ","keywords":["frames","tsdb","backend","overview,","frames","tsdb","overview,","frames","tsdb","client","methods,","frames","tsdb","methods,","frames","tsdb","api,","frames","tsdb,","frames","time-series,","time-series","databsaes,","time-series"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/tsdb/overview/","title":"Overview of the Frames TSDB Backend"},{"content":" This section contains how-to guides for deploying the MLOps Platform (\u0026quot;the platform\u0026quot;) on Google Cloud Platform (GCP).\n","keywords":["gcp","cloud","setup","how-to,","gcp","setup","how-to,","gcp","installation","how-to,","gcp","how-to,","gcp","outposts","how-to,","gcp","setup,","gcp","installation,","gcp","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/howto/","title":"GCP Cloud Deployment How-Tos"},{"content":" Overview This document lists the hardware specifications for deployment of version 3.2.1 of the MLOps Platform (\u0026quot;the platform\u0026quot;) on Google Cloud, also known as Google Cloud Platform (GCP); for details, refer to the Google Cloud Compute Engine documentation.\nNoteAll capacity calculations in the hardware specifications are performed using the base-10 (decimal) number system. For example, 1 TB = 1,000,000,000,000 bytes.  Warning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Hardware Configurations The platform is available in two configurations, which differ in a variety of aspects, including the performance capacity, footprint, storage size, and scale capabilities:\n Development Kit A single data-node and single application-node cluster implementation. This configuration is designed mainly for evaluation trials and doesn't include high availability (HA) or performance testing. Operational Cluster A scalable cluster implementation that is composed of multiple data and application nodes. This configuration was designed to achieve superior performance that enables real-time execution of analytics, machine-learning (ML), and artificial-intelligence (AI) applications in a production pipeline. The minimum requirement for HA support is three data nodes and three application nodes.  Both configurations also support an additional backup node for backing up the platform instance.\nGCP Data-Node Specifications Data nodes in platform GCP deployments must fulfill the following hardware specification requirements:\nComponent  Specification   Instance type n2-highmem-16  vCPUs 16  Memory 128 GB  Data disks (local storage) 8 x 375 GB NVMe SSD  OS boot disk Premium SSD; 400 GB (minimum)  Usable storage capacity 1 node (Development Cluster) — 2 TB; 3 nodes (Operational Cluster) — 3.5 TB   GCP Application-Node Specifications Application nodes in platform GCP deployments are supported only on Google Kubernetes Engine (GKE) and must use one of the following instance types; choose the type that best fits your requirements. For specification details for each type, refer to the Google Cloud Compute Engine documentation.\nNoteAll of the supported application-node configurations also require a 250 GB (minimum) premium-SSD OS boot disk.  CPU-Based Instances  c2-standard-16 (default configuration) c2-standard-30 c2-standard-60  GPU-Based Instances  n1-standard-16 n1-standard-32 n1-standard-64 n1-standard-96  GCP Backup-Node Specifications (Optional) If you wish to back up your instance of the platform, you need an additional backup-node instance of type c2-standard-16.\nNote It's strongly recommended that you back up your data on a regular basis. The backup node is used only for backups and can be shut down between backups to save costs. The backup node must have at least 2 TB of network-attached storage (NAS) to be used only for backup purposes. The exact amount of required storage depends on the amount of data that's being used in the platform; consult Iguazio's support team.    See Also  High Availability (HA) Software Specifications and Restrictions Support and Certification Matrix  ","keywords":["google","cloud","deployment","specifications,","google","cloud","platform","deployment","specifications,","gcp","deployment","specifications,","google","cloud","deployment","specs,","gcp","deployment","specs,","google","cloud","hardware","specifications,","gcp","hardware","specifications,","google","cloud","hardware","specs,","gcp","hardware","specs,","google","cloud","specifications,","google","cloud","platform","specifications,","gcp","specifications,","google","cloud","specs,","gcp","specs,","deployment","specifications,","deployment","specs,","hardware","specifications,","hardware","specs,","hardware","configuration,","hardware,","specification,","spec"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/gcp-hw-spec/","title":"GCP Deployment Specifications"},{"content":"Overview This guide outlines the required steps for installing (deploying) an instance of the MLOps Platform (\u0026quot;the platform\u0026quot;) to Google Cloud Platform (GCP). When you complete the procedure, you'll have a platform instance running under your GCP account. Install the platform with the Provazio installer, with your GCP credentials.\nWarning  Provisioning of the servers is handled automatically by the platform installer (Provazio).  Don't attempt to provision the servers manually prior to the deployment.\n  The data-node instances include Non-Volatile Memory Express (NVMe) SSD-based instance storage, which is optimized for low latency, very high random I/O performance, and high sequential read throughput. The data doesn't persist on the NVMe if the instance is stopped.  Don't attempt to shut down any of the data nodes, as it will erase the data.\n    Prerequisites Before you begin, ensure that you have the following:\n A Provazio API key and a Provazio vault URL, received from Iguazio. Iguazio Platform version provided by Iguazio Support (for example, 3.2.0-b19.20211107120205). Administrative access to a GCP account. A machine running Docker. Access to the internet, or a preloaded Provazio Docker image (gcr.io/iguazio/provazio-dashboard:stable), received from Iguazio as an image archive (provazio-latest.tar.gz). The Kubernetes Engine in your GCP account is enabled.  Deployment Steps To deploy an instance of the platform to an AWS cloud, execute the following steps.\nStep 1: Create a Service Account | Step 2: Configure the installation environment | Step 3: Run the platform installer | Step 4: Access the installer dashboard | Step 5: Choose the AWS scenario | Step 6: Configure general parameters | Step 7: Configure cluster parameters | Step 8: Configure cloud parameters | Step 9: Review the settings | Step 10: Wait for completion\nStep 1: Create a Service Account Follow the Creating a GCP Service Account guide to create a service account with the required credentials for performing the installation.\nStep 2: Configure the Installation Environment Create a /tmp/env.yaml configuration file with the following environment information. Replace the \u0026lt;...\u0026gt; placeholders with the information for your environment:\ndashboard: frontend: cloud_provider_regions: gcp: - \u0026lt;GCP Region\u0026gt; client: infrastructure: gcp: project_name: \u0026lt;Full Project Name\u0026gt; zone: \u0026lt;Zone\u0026gt; application_credentials: | \u0026lt;JSON Key File Content\u0026gt; vault: api_key: \u0026lt;Provazio API Key\u0026gt; url: \u0026lt;Provazio vault URL\u0026gt; provisioning: whitelisted_services: [\u0026#34;*\u0026#34;]  GCP Region The GCP region, for example, \u0026quot;us-east1\u0026quot;.  Full Project Name The full project name that the platform will be deployed in.  Zone GCP zone , for example, \u0026quot;us-east1-b\u0026quot;.  JSON Key File Content The JSON key that was saved in Creating a GCP Service Account.  Provazio API Key A Provazio API key, received from Iguazio (see the installation prerequisites).  Provazio Vault URL A Provazio vault URL, received from Iguazio (see the installation prerequisites).   Step 3: Run the Platform Installer Run the platform installer, Provazio, by entering the following command from a command-line shell:\ndocker pull gcr.io/iguazio/provazio-dashboard:stable \u0026amp;\u0026amp; docker run --rm --name provazio-dashboard \\  -v /tmp/env.yaml:/tmp/env.yaml \\  -e PROVAZIO_ENV_SPEC_PATH=/tmp/env.yaml \\  -p 8060:8060 \\  gcr.io/iguazio/provazio-dashboard:stable Step 4: Access the Installer Dashboard In a web browser, browse to localhost:8060 to view the Provazio dashboard.    Select the plus-sign icon (+) to create a new system.\nStep 5: Choose the GCP Scenario In the Installation Scenario page, check GCP, and then click Next.\n   Step 6: Configure General Parameters On the General page, fill in the configuration parameters, and then click Next.     System Name A platform name (ID) of your choice (for example, \u0026quot;my-platform-0\u0026quot;). The installer prepends this value to the value of the System Domain parameter to create the full platform domain.\n Valid Values: A string of 1–12 characters; can contain lowercase letters (a–z) and hyphens (-); must begin with a lowercase letter.   Default Value: A randomly generated lowercase string.   Description A free-text string that describes the platform instance.  System Version The platform version. Insert the release build number that you received from Iguazio (for example, \u0026quot;3.0_b51_20210308021033\u0026quot;).  Owner Full Name An owner-name string, containing the full name of the platform owner, for bookkeeping.  Owner Email An owner-email string, containing the email address of the platform owner, for bookkeeping.  Username The username of a platform user to be created by the installation. This username will be used together with the configured password to log into platform dashboard. You can add additional users after the platform is provisioned.\n User Password A platform password for the user generated by the installation — to be used with the configured username to log into platform dashboard; see the password restrictions. You can change this password after the platform is provisioned.\n Region The region in which to install the platform.  System Domain A custom platform domain (for example, \u0026quot;customer.com\u0026quot;). The installer prepends the value of the System Name parameter to this value to create the full platform domain.\n Allocate Public IP Addresses Check this option to allocate public IP addresses to all of the platform nodes.  Step 7: Configure Cluster Parameters On the Clusters page, fill in the configuration parameters, and then select Next. For additional information and guidelines, see the GCP resource-calculation guide guide.    Common Parameters (Data and Application Clusters) The following parameters are set for both the data and application clusters. Node references in the parameter descriptions apply to the platform's data nodes for the data cluster and application nodes for the application cluster (GKE).\nData-Cluster Parameters  # of Nodes The number of nodes to allocate for the cluster.  Node Size The instance type, which determines the size of the clusters' nodes.  Root Block Device Size The size of the OS disk.   Application-Cluster Parameters The following parameters are applicable only to the platform's application cluster.\n Kubernetes Kind Leave this set to New GKE Cluster.  Root Block Device Size The size of the OS disk.  GKE Master Version The Kubernetes version that GCP is currently using by default to provision a GKE cluster. For instruction on how to get the current version, see the GKE page that describes checking versions.  Node Groups The installer predefines a default node group named \u0026quot;iniital\u0026quot;. You can configure the following parameters:\n    Name—the name of the node group Lifecycle—the lifecycle of the node group (spot or on-demand) # of instances—the number of instances of the node # of instances—the minimum number of instances of the node when # of instances—the maximum number of instances of the node when # of GPUs—the number of GPUs to be used in the node Custom Labels—user defined labels for the resources in the node Custom Tags—user defined tags for the resources in the node Size—the desired size of the node    Step 8: Configure Cloud Parameters On the Cloud page, fill in the configuration parameters, and then click Next.\n Project Name The full name of your GCP Project.  Region Name The GCP region, for example, \u0026quot;us-east1\u0026quot;.  Zone Name GCP zone, for example, \u0026quot;us-east1-b\u0026quot;.   VPC mode The cloud configuration configures the platform's virtual private cloud (VPC) networking. You can select between two alternative VPC modes:\n New — Create a new VPC and install the platform in this VPC. Existing — Install the platform in an existing VPC.   Whitelisted CIDRs A list of classless inter-domain routing (CIDR) addresses to be granted access to the platform's service port (for example, \u0026quot;200.40.0.1/32\u0026quot;). This parameter is typically relevant when the platform has public IP addresses. For a platform without public IP addresses, you can leave this parameter empty, assuming you have access to the VPC from your network.  Installer CIDR The CIDR of the machine on which you're running the platform installer (for example, \u0026quot;10.0.0.1/32\u0026quot;).  Allow Access from Iguazio Support Check this option to allow Iguazio's support team to access the platform nodes from the Iguazio network This parameter is applicable only when the platform has public IP addresses (see the Allocate Public IP Addresses general-configuration parameter).\n  New-VPC Configuration The following parameters are applicable only to the New VPC mode:\n    CIDR The CIDR of the VPC.  Subnet CIDRs The CIDRs of the VPC's subnets. The number of CIDRs translates to the number of subnets.   Existing-VPC Configuration The following parameters are applicable only to the Existing VPC mode:\n    VPC ID The ID of the VPC in which to install the platform.  Subnet IDs The IDs of the subnets within the VPC or of a subset of these subnets.   Step 9: Review the Settings On the Review page, review and verify your configuration; go back and make edits, as needed; and then select Create to provision a new instance of the platform.\n   Step 10: Wait for Completion It typically takes around 30–40 minutes to provision a new platform instance, regardless of the cluster sizes. You can download the provisioning logs, at any stage, by selecting Download logs from the instance's action menu.\n   You can also follow the installation progress by tracking the Provazio Docker container logs.\nWhen the installation completes, you should have a running instance of the platform in your cloud. You can use the Provazio dashboard to view the installed nodes. Then, proceed to the post-deployment steps.\nPost-Deployment Steps When the deployment completes, follow the post-deployment steps.\nSee Also  GCP Deployment How-Tos Post-Deployment How-Tos GCP Deployment Specifications   ","keywords":["gcp","installation,","gcp","cloud","installation,","gcp","outposts","installation,","gcp","deployment,","gcp","cloud","deployment,","provazio,","platform","installer,","gcp","roles,","iam","roles,","gcp","instance","profiles,","gcp","network,","gcp","vpcs,","vpcs,","gcp","ebs,","ebs,","cidrs"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/gcp-installation-guide/","title":"Installing the Platform on Google Cloud Platform (GCP)"},{"content":"Overview After following the platform's GCP cloud installation guide, you should have a running instance of the platform in your GCP cloud. In some cases, additional post-deployment steps are required before you can properly use the platform, as outlined in this guide.\nRegistering a Custom Platform Domain If you chose to install the platform under a custom domain, you must register a few DNS records. If you need assistance, contact Iguazio's support team. Creating an HTTPS Certificate In some cases, you might need to create an HTTPS certificate for your platform installation. For more information, contact Iguazio's support team. Importing IdP Users and Groups from an Active Directory To import users and groups from an external Microsoft Active Directory, see the platform's IdP documentation. For additional assistance, contact Iguazio's support team. See Also  GCP cloud installation guide  ","keywords":["gcp","cloud","post","deployment,","gcp","post","deployment,","gcp","post","installation,","custom","domain","regsitration,","domain","registration,","ip","addresses,","network,","dns,","idp,","microsoft","active","directory,","active","directory,","microsoft","ad,","idp","users,","http","certificates"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/howto/post-install-steps/","title":"Post-Installation Steps (GCP)"},{"content":" The management APIs are RESTful APIs that use a subset of the JSON API specification to represent request and response data.\nRequest Syntax The management-API operations are HTTP requests that you send to a dashboard endpoint using an appropriate HTTP method (such as GET or POST). Data parameters (where required) are passed within the request's HTTP body, in JSON format. The requests conform to the following general format:\n \u0026lt;method\u0026gt; /\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;management-APIs URL\u0026gt; Content-Type: application/json Cookie: session=\u0026lt;cookie\u0026gt; { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026lt;attributes\u0026gt; }, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;type\u0026gt;\u0026#34; } }  import requests url = \u0026#34;\u0026lt;management-APIs URL\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;session=\u0026lt;cookie\u0026gt;\u0026#34; } payload = { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026lt;attributes\u0026gt; }, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;type\u0026gt;\u0026#34; } } response = requests.\u0026lt;method\u0026gt;(url, json=payload, headers=headers) print(response.text)    Following is an explanation of the \u0026lt;...\u0026gt; placeholders used in the request syntax:\n \u0026lt;method\u0026gt; The HTTP method for the request — for example, GET (HTTP) or get (Python).  \u0026lt;resource\u0026gt; The full path to the operation's target resource. The path begins with the relevant API endpoint — for example, /api/containers for the Containers API — and is optionally followed by a forward slash (/) and URL resource parameters — for example, /1030 to set the container ID for a Delete Container operation.  \u0026lt;management-APIs URL\u0026gt; The URL of the management-APIs service of a platform tenant. Set this URL to the HTTPS URL of the platform dashboard; for example, https://dashboard.default-tenant.app.mycluster.iguazio.com. In bare-metal deployments, you can alternatively set the URL to http://\u0026lt;dashboard IP\u0026gt;:\u0026lt;port\u0026gt; where \u0026lt;dashboard IP\u0026gt; is the IP address or resolvable host domain name of the dashboard and \u0026lt;port\u0026gt; is 8001, which is the host port on which the management-APIs are served; for example, http://192.168.1.100:8001.\nNoteTo run the examples in this reference, you must replace the sample management-APIs URL in the examples with a tenant web-APIs URL for your platform environment.   \u0026lt;cookie\u0026gt; A session cookie that was received from a Create Session operation and is used to authenticate the sender of the request, and authorize the sender to perform management operations. For example, j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D. For more information, see Create Session.\nNoteTo run the examples in this reference, you must replace the sample session cookie in the examples with a valid session cookie for your cluster.   \u0026lt;attributes\u0026gt; Data attributes that serve as request parameters, where relevant.   Response Syntax A response to a management API request includes an HTTP status code and a related reason phrase (see RFC 7231 6. Response Status Codes). Some successful operations also return, within the data object of the response HTTP body, additional information relating to the operation that was performed, in JSON format. The data structure is operation-specific.\nSuccess Status Codes The following HTTP status codes are returned for successful operations:\nHTTP Method Operation Types Success Status Code   POST Create 200   GET List; Get 201   DELETE Delete 204   Error Information In the event of an error, in addition to the HTTP status code returned in the response, the operation returns, within the response HTTP body, an errors array of error-information JSON objects. Each error-information object has a status element that contains the operation's HTTP status code (for example, 400), and a detail string element that contains a textual description of the error:\n{ \u0026#34;errors\u0026#34;: [ { \u0026#34;status\u0026#34;: \u0026lt;HTTP status code\u0026gt;, \u0026#34;detail\u0026#34;: \u0026#34;\u0026lt;error description\u0026gt;\u0026#34; } ] } ","keywords":["management-api","structure,","api","structure,","management","apis,","management,","api","reference,","REST,","RESTful,","http,","http","requests,","http","headers,","request","header,","request","parameters,","http","responses,","http","operations,","host,","management-api","operations,","request","url,","url","resource","parameters,","json,","json","apis,","json","api","specification,","json","parameters,","json","elements,","python,","api","endpoints,","dashboard,","dashboard","endpoint,","dashboard","ip,","host,","ports,","dashboard","port,","security,","cookies,","session","cookies,","authentication,","http","authentication,","errors,","http","status,","http","status","codes,","debugging,","containers","management","api,","Delete","Container,","sessions","management","api,","Create","Session,","data","attributes,","http","POST,","POST","method,","POST,","http","PUT,","PUT","method,","PUT,","http","DELETE,","DELETE","method,","DELETE"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/general-api-structure/","title":"General Management-API Structure"},{"content":"Description Returns information about the endpoints of the platform cluster, which provide access to the platform's resources. The information includes the IP addresses of the endpoints.\nRequest Request Header Syntax  GET /api/cluster_info HTTP/1.1 Host: \u0026lt;management-APIs URL\u0026gt; Content-Type: application/json Cookie: session=\u0026lt;cookie\u0026gt;  url = \u0026#34;\u0026lt;management-APIs URL\u0026gt;/api/cluster_info\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;session=\u0026lt;cookie\u0026gt;\u0026#34; }    HTTP Method GET\nURL Resource ParametersNone\nRequest DataNone\nResponse Response Header Syntax HTTP/1.1 \u0026lt;status code; 200 on success\u0026gt; \u0026lt;reason phrase\u0026gt; Content-Type: application/json ... Response Data Syntax { \u0026#34;data\u0026#34;: [ { \u0026#34;attributes\u0026#34;: { \u0026#34;cluster_name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;endpoints\u0026#34;: { \u0026lt;cluster-endpoint arrays\u0026gt; } }, \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cluster_info\u0026#34; } ] } Elements The data object in the HTTP response body contains the requested cluster information, which includes these elements:\n attributes Cluster-information attributes.\n Type: A JSON object of cluster-information attributes  The following cluster-information attributes are returned:\n cluster_name The name of the cluster.\n Type: String   created_at The date and time at which the cluster was created.\n Type: String   endpoints Information about the cluster's endpoints, which provide access to the platform's resources. The returned information includes the endpoints' URLs — the IP addresses and port numbers for accessing the resources.\n Type: A JSON object containing endpoint-information arrays     id A unique cluster ID.\n Type: String   type The type of the data object. This value must be set to \u0026quot;cluster_info\u0026quot;.\n Type: String    Examples Request  GET /api/cluster_info HTTP/1.1 Host: https://dashboard.default-tenant.app.mycluster.iguazio.com Content-Type: application/json Cookie: session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D  import requests url = \u0026#34;https://dashboard.default-tenant.app.mycluster.iguazio.com/api/cluster_info\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D\u0026#34; } response = requests.get(url, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;data\u0026#34;: [ { \u0026#34;attributes\u0026#34;: { \u0026#34;cluster_name\u0026#34;: \u0026#34;igzc0\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2018-01-15T13:26:48.353000+00:00\u0026#34;, \u0026#34;endpoints\u0026#34;: { \u0026#34;api\u0026#34;: [ { \u0026#34;service_id\u0026#34;: \u0026#34;igz0.api.0\u0026#34;, \u0026#34;urls\u0026#34;: [ \u0026#34;10.0.0.1:8113\u0026#34; ] } ], \u0026#34;bridge\u0026#34;: [ { \u0026#34;rdma_urls\u0026#34;: [ \u0026#34;tcp://10.0.0.1:1234\u0026#34; ], \u0026#34;service_id\u0026#34;: \u0026#34;igz0.bridge.1\u0026#34;, \u0026#34;tcp_urls\u0026#34;: [ \u0026#34;10.0.0.1:8100\u0026#34; ] } ], \u0026#34;ui\u0026#34;: [ { \u0026#34;service_id\u0026#34;: \u0026#34;igz0.dashboard.0\u0026#34;, \u0026#34;urls\u0026#34;: [ \u0026#34;10.0.0.1:8000\u0026#34; ] } ], \u0026#34;web\u0026#34;: [] } }, \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cluster_info\u0026#34; } ], ... } ","keywords":["Get","Cluster","information,","api","endpoints,","http","GET,","GET","method,","GET"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/cluster-info-api/get-cluster-information/","title":"Get Cluster Information"},{"content":"Description Retrieves the requested attributes of a table item.\nGetItem Read OptimizationQuerying a table with GetItem is faster than a GetItems table scan, because GetItem searches for a specific object file on the data slice assigned to the item. See Working with NoSQL Data.  Request Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: GetItem \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetItem\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the item to retrieve. The path includes the table path and the item's name (primary key). You can optionally set the item name in the request's Key JSON parameter instead of in the URL; you can also optionally set the relative table path within the container, or part of the path, in the request's TableName JSON parameter. See Data-Service Web-API General Structure.\nRequest Data Syntax  { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;string\u0026#34; }  payload = { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;string\u0026#34; }    Parameters  TableName The table (collection) from which to retrieve the item — a relative table path within the configured data container or the end of such a path, depending on the resource configuration in the request URL. See Data-Service Web-API General Structure.\n Type: String   Requirement: Required if not set in the request URL   Key A primary-key attribute whose value is the item's primary-key value and name, which uniquely identifies the item within the table (see Item Name and Primary Key).\nNoteTo retrieve all items for an original sharding-key value that was recalculated during the ingestion (to achieve a more even workload distribution), you need to repeat the \u0026lt;api\u0026lt;GetItem request for each of the primary-key values that were used in the ingestion; (the primary-key value of the ingested item includes the recalculated sharding-key value). If the ingestion was done by using the even-distribution option of the NoSQL Spark DataFrame, you need to repeat the request with Key values that range from \u0026lt;original sharding key\u0026gt;_1.\u0026lt;sorting-key value\u0026gt; to \u0026lt;original sharding key\u0026gt;_\u0026lt;n\u0026gt;.\u0026lt;sorting-key value\u0026gt;, where \u0026lt;n\u0026gt; is the value of the v3io.kv.range-scan.hashing-bucket-num configuration property (default = 64); for example, johnd_1.20180602 .. johnd_64.20180602. For more information, see Recalculating Sharding-Key Values for Even Workload Distribution.\n   Type: Attribute object   Requirement: Required if the item's name (primary key) is not set in the URL   AttributesToGet The item attributes to return.\n Type: String   Requirement: Optional   Default Value: \u0026quot;*\u0026quot;  The attributes to return can be depicted in one of the following ways:\n  A comma-separated list of attribute names. Note: Currently, the delimiter commas cannot be surrounded by spaces.\nThe attributes can be of any attribute type — user, system, or hidden.\n  \u0026quot;*\u0026quot; — retrieve the item's user attributes, but not its system or hidden attributes. This is the default value.\n  \u0026quot;**\u0026quot; — retrieve all item attributes — user, system, and hidden attributes.\n  For an overview of the different attribute types, see Attribute Types.\n  Response Response Data Syntax { \u0026#34;Item\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;BOOL\u0026#34;: Boolean, \u0026#34;B\u0026#34;: \u0026#34;blob\u0026#34; } } } Elements  Item The requested item attributes. Only attributes that were requested in the request's AttributesToGet parameter are returned in the response.\n Type: An item JSON object that contains zero or more Attribute objects    Examples Retrieve from the MyDirectory/People table the Name, Age, and Country attributes of a person whose primary-key value is 1234:\nRequest  POST /mycontainer/MyDirectory/People/1234 HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: GetItem X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;AttributesToGet\u0026#34;: \u0026#34;Name,Age,Country\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/People/1234\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetItem\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = {\u0026#34;AttributesToGet\u0026#34;: \u0026#34;Name,Age,Country\u0026#34;} response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;Item\u0026#34;: { \u0026#34;Age\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;42\u0026#34;}, \u0026#34;Country\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Zimbabwe\u0026#34;}, \u0026#34;Name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Shmerel\u0026#34;} } } ","keywords":["GetItem,","get","item,","nosql,","tables,","table","items,","attributes,","item","names,","object","names,","primary","key,","AttributesToGet,","Key,","TableName,","Item"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/getitem/","title":"GetItem"},{"content":"Description Retrieves (reads) attributes of multiple items in a table or in a data container's root directory, according to the specified criteria.\nScan Optimization GetItems enables you to optimize table scans by using either of the following methods:\n Range Scan Parallel Scan  Note  You can't use both optimization methods together in the same GetItems request.\n  If you're looking for a specific item, use GetItem, which is faster than either of these GetItems optimized-scan methods because it searches for a specific object file on the relevant data slice. See Working with NoSQL Data.\n     Range Scan GetItems allows you to perform a range scan to retrieve items with a specific sharding-key value by setting the ShardingKey request parameter to the requested sharding-key value. You can also optionally restrict the query to a specific range of item sorting-key values by using the SortKeyRangeStart and/or SortKeyRangeEnd parameters. A range scan is more efficient than the default GetItems full table scan because of the way that the data is stored and accessed. For more information, see Working with NoSQL Data.\n Parallel Scan (Segmented Table Scan) GetItems scans table items in search for the requested items. By default, the scan is executed sequentially. However, you can optionally scan only a specific portion (segment) of the table: you can set the request's TotalSegment parameter to the number of segments into which you wish to divide the table, and set the request's Segment parameter to the ID of the segment that you wish to scan in the current operation. To improve performance, you can implement a parallel table scan by dividing the scan among multiple application instances (\u0026quot;workers\u0026quot;), assigning each worker a different segment to scan. Note that such an implementation requires that the workers all send GetItems requests with the same scan criteria and total-segments count but with different scan segments.\nThe following table depicts a parallel multi-worker scan of a segmented table with GetItems:\n  Partial Response The GetItems response might not return all the requested items, especially if the overall size of the requested data is considerable. In such cases, the value of the LastItemIncluded response element is \u0026quot;FALSE\u0026quot;. To retrieve the remaining requested items, send a new identical GetItems request, and set its Marker parameter to the value of the NextMarker element that was returned in the response of the previous request. When the Marker request parameter is set, the operation begins searching for matching items at the specified marker location.\nNote  The Limit request parameter defines the maximum number of items to return in the response object for the current API call. When issuing a GetItems request with a new marker, after receiving a partial response, consider recalculating the limit to subtract the items returned in the responses to the previous requests.\n   A GetItems response might contain less items than specified in the Limit request parameter even if there are additional table items that match the request (i.e., the value of the LastItemIncluded response element is \u0026quot;FALSE\u0026quot;). In such cases, you need to issue a new GetItems request to retrieve the remaining items, as explained above.\n  Requests that set the Marker parameter must perform a similar scan to that performed by the previous partial-response request — be it a parallel scan, a range scan, or a regular scan. For example, you cannot use the NextMarker response element returned for a previous range-scan request as the value of the Marker parameter of a parallel-scan request.\n    Request Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: GetItems X-v3io-session-key: \u0026lt;access key\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetItems\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters  To retrieve items from a specific table, set the relative table path within the configured container in the request URL or in the TableName JSON parameter, or split the path between the URL and the JSON parameter. See Data-Service Web-API General Structure. To retrieve items from the root directory of the configured container, omit the \u0026lt;resource\u0026gt; URL element — i.e., end the URL in the request header with \u0026lt;container\u0026gt;/ — and either don't set the request's TableName JSON parameter or set it to \u0026quot;/\u0026quot;.  Request Data Syntax  { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Limit\u0026#34;: number, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;FilterExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardingKey\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;SortKeyRangeStart\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;SortKeyRangeEnd\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Segment\u0026#34;: number, \u0026#34;TotalSegment\u0026#34;: number, \u0026#34;Marker\u0026#34;: \u0026#34;string\u0026#34; }  payload = { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Limit\u0026#34;: number, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;FilterExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardingKey\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;SortKeyRangeStart\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;SortKeyRangeEnd\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Segment\u0026#34;: number, \u0026#34;TotalSegment\u0026#34;: number, \u0026#34;Marker\u0026#34;: \u0026#34;string\u0026#34; }    Parameters  TableName To retrieve items from a specific table (collection), set the relative table path within the configured container in this parameter or in the request URL, or split the path between the URL and the JSON parameter. See Data-Service Web-API General Structure.\nTo retrieve items from the root directory of the configured container, end the URL in the request header with \u0026lt;container\u0026gt;/ and either don't set the TableName JSON parameter or set it to \u0026quot;/\u0026quot;.\n Type: String   Requirement: Optional   Limit The maximum number of items to return within the response (i.e., the maximum number of elements in the response object's Items array).\n Type: Number   Requirement: Optional   AttributesToGet The attributes to return for each item.\n Type: String   Requirement: Optional   Default Value: \u0026quot;*\u0026quot;  The attributes to return can be depicted in one of the following ways:\n  A comma-separated list of attribute names. Note: Currently, the delimiter commas cannot be surrounded by spaces.\nThe attributes can be of any attribute type — user, system, or hidden.\n  \u0026quot;*\u0026quot; — retrieve the item's user attributes and __name system attribute, but not other system attributes or hidden attributes. This is the default value.\n  \u0026quot;**\u0026quot; — retrieve all item attributes — user, system, and hidden attributes.\n  For an overview of the different attribute types, see Attribute Types.\n FilterExpression A filter expression that restricts the items to retrieve. Only items that match the filter criteria are returned. See filter expression.\n Type: String   Requirement: Optional   ShardingKey The sharding-key value of the items to get by using a range scan. The sharding-key value is the part to the left of the leftmost period in a compound primary-key value (item name). You can optionally use the SortKeyRangeStart and/or SortKeyRangeEnd request parameters to restrict the search to a specific range of sorting keys (SortKeyRangeStart \u0026gt;= \u0026lt;sorting key\u0026gt; \u0026lt; SortKeyRangeEnd).\nNoteTo retrieve all items for an original sharding-key value that was recalculated during the ingestion (to achieve a more even workload distribution), you need to repeat the GetItems request for each of the sharding-key values that were used in the ingestion. If the ingestion was done by using the even-distribution option of the NoSQL Spark DataFrame, you need to repeat the request with ShardingKey values that range from \u0026lt;original sharding key\u0026gt;_1 to \u0026lt;original sharding key\u0026gt;_\u0026lt;n\u0026gt;, where \u0026lt;n\u0026gt; is the value of the v3io.kv.range-scan.hashing-bucket-num configuration property (default = 64); for example, johnd_1 .. johnd_64. For more information, see Recalculating Sharding-Key Values for Even Workload Distribution.\n   Type: String   Requirement: Optional; required when either the SortKeyRangeStart or SortKeyRangeEnd request parameter is set\n   SortKeyRangeStart The minimal sorting-key value of the items to get by using a range scan. The sorting-key value is the part to the right of the leftmost period in a compound primary-key value (item name). This parameter is applicable only together with the ShardingKey request parameter. The scan will return all items with the specified sharding-key value whose sorting-key values are greater than or equal to (\u0026gt;=) the value of the SortKeyRangeStart parameter and less than (\u0026lt;) the value of the SortKeyRangeEnd parameter (if set).\n Type: String   Requirement: Optional   SortKeyRangeEnd The maximal sorting-key value of the items to get by using a range scan. The sorting-key value is the part to the right of the leftmost period in a compound primary-key value (item name). This parameter is applicable only together with the ShardingKey request parameter. The scan will return all items with the specified sharding-key value whose sorting-key values are greater than or equal to (\u0026gt;=) than the value of the SortKeyRangeStart parameter (if set) and less than (\u0026lt;) the value of the SortKeyRangeEnd parameter.\n Type: String   Requirement: Optional   Segment The ID of a specific table segment to scan — 0 to one less than TotalSegment. See Parallel Scan.\n Type: Number   Requirement: Required when TotalSegment is provided\n   TotalSegment The number of segments into which to divide the table scan — 1 to 1024. See Parallel Scan. The segments are assigned sequential IDs starting with 0.\n Type: Number   Requirement: Required when Segment is provided\n   Marker An opaque identifier that was returned in the NextMarker element of a response to a previous GetItems request that did not return all the requested items. This marker identifies the location in the table from which to start searching for the remaining requested items. See Partial Response and the description of the NextMarker response element.\n Type: String   Requirement: Optional    Response Response Data Syntax { \u0026#34;LastItemIncluded\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;NumItems\u0026#34;: number, \u0026#34;NextMarker\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Items\u0026#34;: [ { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;BOOL\u0026#34;: Boolean, \u0026#34;B\u0026#34;: \u0026#34;blob\u0026#34; } } ] } Elements  LastItemIncluded \u0026quot;TRUE\u0026quot; if the scan completed successfully — the entire table was scanned for the requested items and all relevant items were returned (possibly in a previous response — see Partial Response); \u0026quot;FALSE\u0026quot; otherwise.\n Type: Boolean string — \u0026quot;TRUE\u0026quot; or \u0026quot;FALSE\u0026quot;   NumItems The number of items in the response's Items array.\n Type: Number   NextMarker An opaque identifier that marks the location in the table at which to start searching for remaining items in the next call to GetItems. See Partial Response and the description of the Marker request parameter. When the response contains all the requested items, NextMarker is not returned.\n Type: String   Items An array of items containing the requested attributes. The array contains information only for items that satisfy the conditions of the FilterExpression request parameter. Each returned item object includes only the attributes requested in the AttributesToGet parameter, provided the item has these attributes.\n Type: An array of item JSON objects that contain Attribute objects    Examples Example 1 — Basic Filter-Expression Scan Retrieve from a \u0026quot;MyDirectory/Cars\u0026quot; table in a \u0026quot;mycontainer\u0026quot; container the __name, km, state, and manufacturer attributes (if exist) of up to 1,000 items whose km attribute value is greater than or equal to 10,000, and whose lastService attribute value is less than 10,000:\nRequest  POST /mycontainer/MyDirectory/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: GetItems X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;TableName\u0026#34;: \u0026#34;Cars\u0026#34;, \u0026#34;Limit\u0026#34;: 1000, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;__name,km,state,manufacturer\u0026#34;, \u0026#34;FilterExpression\u0026#34;: \u0026#34;(km \u0026gt;= 10000) AND (lastService \u0026lt; 10000)\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetItems\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;TableName\u0026#34;: \u0026#34;Cars\u0026#34;, \u0026#34;Limit\u0026#34;: 1000, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;__name,km,state,manufacturer\u0026#34;, \u0026#34;FilterExpression\u0026#34;: \u0026#34;(km \u0026gt;= 10000) AND (lastService \u0026lt; 10000)\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;LastItemIncluded\u0026#34;: \u0026#34;TRUE\u0026#34;, \u0026#34;NumItems\u0026#34;: 3, \u0026#34;Items\u0026#34;: [ { \u0026#34;__name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;7348841\u0026#34;}, \u0026#34;km\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;10000\u0026#34;}, \u0026#34;state\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;OK\u0026#34;} }, { \u0026#34;__name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;6924123\u0026#34;}, \u0026#34;km\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;15037\u0026#34;}, \u0026#34;state\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;OUT_OF_SERVICE\u0026#34;}, \u0026#34;manufacturer\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Honda\u0026#34;} }, { \u0026#34;__name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;7222751\u0026#34;}, \u0026#34;km\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;12503\u0026#34;} }, { \u0026#34;__name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;5119003\u0026#34;}, \u0026#34;km\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;11200\u0026#34;}, \u0026#34;manufacturer\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Toyota\u0026#34;} } ] } Example 2 — Range Scan This examples demonstrates two range-scan queries for a \u0026quot;mytaxis/rides\u0026quot; table in a \u0026quot;mycontainer\u0026quot; container. The table contains the following items:\n+---------+--------+---------+--------+----------------+------------------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers| avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+------------------+-------------------+ | 1|20180601| 25| 125.0| 40| 5.0| 1.6| | 1|20180602| 20| 106.0| 46| 5.3| 2.3| | 1|20180701| 28| 106.4| 42|3.8000000000000003| 1.5| | 16|20180601| 1| 224.2| 8| 224.2| 8.0| | 16|20180602| 10| 244.0| 45| 24.4| 4.5| | 16|20180701| 6| 193.2| 24|32.199999999999996| 4.0| | 24|20180601| 8| 332.0| 18| 41.5| 2.25| | 24|20180602| 5| 260.0| 11| 52.0| 2.2| | 24|20180701| 7| 352.1| 21|50.300000000000004| 3.0| +---------+--------+---------+--------+----------------+------------------+-------------------+ Request The first query scans for all attributes of the items whose sharding-key value is 1:\n POST /mycontainer/mytaxis/rides/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: GetItems X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;ShardingKey\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;*\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/mytaxis/rides/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetItems\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;ShardingKey\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;*\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    The second query scans for the driver_id, date, avg_ride_km, and avg_ride_passengers attributes of all items whose sharding-key value is 24 and whose sorting-key values are within the first six months of 2018:\n POST /mycontainer/mytaxis/rides/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: GetItems X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;ShardingKey\u0026#34;: \u0026#34;24\u0026#34;, \u0026#34;SortKeyRangeStart\u0026#34;: \u0026#34;20180101\u0026#34;, \u0026#34;SortKeyRangeEnd\u0026#34;: \u0026#34;20180701\u0026#34;, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;__name,driver_id,date,avg_ride_km,avg_ride_passengers\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/mytaxis/rides/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetItems\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;ShardingKey\u0026#34;: \u0026#34;24\u0026#34;, \u0026#34;SortKeyRangeStart\u0026#34;: \u0026#34;20180101\u0026#34;, \u0026#34;SortKeyRangeEnd\u0026#34;: \u0026#34;20180701\u0026#34;, \u0026#34;AttributesToGet\u0026#34;: \u0026#34;__name,driver_id,date,avg_ride_km,avg_ride_passengers\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response Response to the first query —\nHTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;LastItemIncluded\u0026#34;: \u0026#34;TRUE\u0026#34;, \u0026#34;NumItems\u0026#34;: 3, \u0026#34;Items\u0026#34;: [ { \u0026#34;__name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;1.20180601\u0026#34; }, \u0026#34;avg_ride_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;5\u0026#34; }, \u0026#34;total_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;40\u0026#34; }, \u0026#34;driver_id\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;avg_ride_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1.6\u0026#34; }, \u0026#34;total_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;125\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;20180601\u0026#34; }, \u0026#34;num_rides\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;25\u0026#34; } }, { \u0026#34;__name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;1.20180602\u0026#34; }, \u0026#34;avg_ride_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;5.3\u0026#34; }, \u0026#34;total_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;46\u0026#34; }, \u0026#34;driver_id\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;avg_ride_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;2.3\u0026#34; }, \u0026#34;total_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;106\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;20180602\u0026#34; }, \u0026#34;num_rides\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;20\u0026#34; } }, { \u0026#34;__name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;1.20180701\u0026#34; }, \u0026#34;avg_ride_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;3.8\u0026#34; }, \u0026#34;total_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;42\u0026#34; }, \u0026#34;driver_id\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;avg_ride_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1.5\u0026#34; }, \u0026#34;total_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;106.4\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;20180701\u0026#34; }, \u0026#34;num_rides\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;28\u0026#34; } } ] } Response to the second query —\nHTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;LastItemIncluded\u0026#34;: \u0026#34;TRUE\u0026#34;, \u0026#34;NumItems\u0026#34;: 2, \u0026#34;Items\u0026#34;: [ { \u0026#34;__name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;24.20180601\u0026#34; }, \u0026#34;driver_id\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;24\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;20180601\u0026#34; }, \u0026#34;avg_ride_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;41.5\u0026#34; }, \u0026#34;avg_ride_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;2.25\u0026#34; } }, { \u0026#34;__name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;24.20180602\u0026#34; }, \u0026#34;driver_id\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;24\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;20180602\u0026#34; }, \u0026#34;avg_ride_km\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;52\u0026#34; }, \u0026#34;avg_ride_passengers\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;2.2\u0026#34; } } ] } ","keywords":["GetItems,","get","items,","nosql,","tables,","table","items,","attributes,","table","scan,","items","scan,","partial","scan,","segmented","table","scan,","partial","response,","table","segments,","item","names,","object","names,","range","scan,","primary","key,","sharding","key,","sorting","key,","filter","expressions,","condition","expressions,","AttributesToGet,","FilterExpression,","Key,","Limit,","Marker,","Segment,","ShardingKey,","SortKeyRangeStart,","SortKeyRangeEnd,","TableName,","TotalSegment,","Items,","LastItemIncluded,","NextMarker,","NumItems"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/getitems/","title":"GetItems"},{"content":"Description Retrieves (consumes) records from a stream shard.\nDetermining the Start LocationBefore submitting a GetRecords request, you need to determine the location within the shard at which to begin the record consumption. Before the first request for a specific shard, send a Seek request to get the desired location (see the Seek Location) response element. In subsequent GetRecords requests, pass the NextLocation value that you received in a previous GetRecords response as the Location value for the current request. See the descriptions of Location and NextLocation, and the Stream Record Consumption overview.\n  Request Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: GetRecords \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetRecords\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the stream shard from which to retrieve the records. The path includes the stream path and the shard ID. You can optionally set the stream name and shard ID, or only the shard ID, in the request's StreamName and ShardId JSON parameters instead of in the URL.\nRequest Data Syntax  { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardId\u0026#34;: number, \u0026#34;Location\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;Limit\u0026#34;: number }  payload = { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardId\u0026#34;: number, \u0026#34;Location\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;Limit\u0026#34;: number }    Parameters  StreamName The name of the stream that contains the shard resource.\n Type: String   Requirement: Required if not set in the request URL   ShardId The ID of the shard from which to retrieve records. The shard ID is an integer between 0 and one less than the stream's shard count.\n Type: Number   Requirement: Required if not set in the request URL   Location The location within the shard at which to begin consuming records.\nNoteThe location must be exactly as received either in the Location element of a previous Seek response or in the NextLocation element of a previous GetRecords response. See Determining the Start Location in the operation description above. Do not attempt to calculate the location yourself.   Type: Blob — a Base64 encoded string   Requirement: Required   Limit The maximum number of records to return in the response. The minimum is 1. There's no restriction on the amount of returned records, but the maximum supported overall size of all the returned records is 10 MB and the maximum size of a single record is 2 MB, so calculate the limit accordingly.\n Type: Number   Requirement: Optional   Default Value: 1000    Response Response Data Syntax { \u0026#34;NextLocation\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;MSecBehindLatest\u0026#34;: number, \u0026#34;RecordsBehindLatest\u0026#34;: number, \u0026#34;Records\u0026#34;: [ { \u0026#34;ArrivalTimeSec\u0026#34;: number, \u0026#34;ArrivalTimeNSec\u0026#34;: number, \u0026#34;SequenceNumber\u0026#34;: number, \u0026#34;ClientInfo\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;PartitionKey\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;blob\u0026#34; } ] } Elements  NextLocation The location of the next shard record to consume (based on the records' sequence numbers). This value should be used as the value of the Location parameter in a subsequent GetRecords request. See also Determining the Start Location in the operation description above.\n Type: Blob — a Base64 encoded string   MSecBehindLatest The difference in the ingestion time of the last record returned in the response and the latest ingested record in the shard, in milliseconds.\n Type: Number   RecBehindLatest The difference between the last record returned in the response and the latest ingested record in the shard, in number of records. A value of 0 means that the latest record in the shard has been consumed.\n Type: Number   Records An array of the requested records.\n Type: Array of record JSON objects  The record JSON object contains these elements:\n ArrivalTimeSec The record's ingestion time (the time at which the record arrived at the platform), as a Unix timestamp in seconds. For example, 1511260205 indicates that the record was ingested on 21 Nov 2017 at 10:30:05 AM. The ArrivalTimeNSec response element holds the nanoseconds unit of the ingestion time.\n Type: Number   ArrivalTimeNSec The nanoseconds unit of the ArrivalTimeSec ingestion-time timestamp. For example, if ArrivalTimeSec is 1511260205 and ArrivalTimeNSec is 500000000, the record was ingested on 21 Nov 2017 at 10:30 AM and 5.5 seconds.\n Type: Number   SequenceNumber The record's sequence number, which uniquely identifies the record within the shard. This ID number can be used in a sequence-based Seek operation to get the location of a specific record within a given stream shard.\n Type: Number   ClientInfo Custom opaque information, if provided by the producer. This metadata can be used, for example, to save the data format of a record, or the time at which a sensor or application event was triggered. See Record Metadata.\n Type: Blob — a Base64 encoded string   PartitionKey The partition key associated with the record, if provided by the producer (see the PutRecords PartitionKey record request parameter, and Record Metadata). Records with the same partition key are assigned to the same shard. See Stream Sharding and Partitioning.\n Type: String   Data Record data, as provided by the producer (see PutRecords).\n Type: Blob — a Base64 encoded string      Errors In the event of an error, the response includes a JSON object with an ErrorCode element that contains a unique numeric error code, and an ErrorMessage element that contains one of the following API error messages: Error Message Description   IllegalLocation The specified record location does not exist in the shard. The requested records may have moved. Perform another Seek operation to get an updated location).    InvalidArgumentException A provided request parameter is not valid for this request.    Permission denied The sender of the request does not have the required permissions to perform the operation.    ResourceNotFoundException The specified resource does not exist.   ShardIDOutOfRangeException The specified shard does not exist in this stream.    Examples Retrieve the first two records from location \u0026quot;AQAAAAAAAAAAAAAAAAAAAA==\u0026quot; in shard 199 of a MyStream stream:\nRequest  POST /mycontainer/MyDirectory/MyStream/199 HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: GetRecords X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;Location\u0026#34;: \u0026#34;AQAAAAAAAAAAAAAAAAAAAA==\u0026#34;, \u0026#34;Limit\u0026#34;: 2 }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/MyStream/199\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;GetRecords\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = {\u0026#34;Location\u0026#34;: \u0026#34;AQAAAAAAAAAAAAAAAAAAAA==\u0026#34;, \u0026#34;Limit\u0026#34;: 2} response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;NextLocation\u0026#34;: \u0026#34;AQAAAAsAAAAFAEC2/+sAAA==\u0026#34;, \u0026#34;MSecBehindLatest\u0026#34;: 0, \u0026#34;RecordsBehindLatest\u0026#34;: 0, \u0026#34;Records\u0026#34;: [ { \u0026#34;ArrivalTimeSec\u0026#34;: 1485685671, \u0026#34;ArrivalTimeNSec\u0026#34;: 160186781, \u0026#34;SequenceNumber\u0026#34;: 15756, \u0026#34;PartitionKey\u0026#34;: \u0026#34;MyKey\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;ZGF0YQ0K\u0026#34; } ] } ","keywords":["GetRecords,","get","records,","streaming,","stream","records,","stream","consumption,","stream","shards,","sharding,","stream","seek,","Seek,","PutRecords,","stream","partitions,","partitioning,","record","metadata,","record","sequence","number,","record","arrival","time,","record","location,","ArrivalTimeNSec,","ArrivalTimeSec,","ClientInfo,","Location,","PartitionKey,","SequenceNumber,","ShardId"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/getrecords/","title":"GetRecords"},{"content":" Overview You can use the Apache Spark open-source data engine to work with data in the platform. This tutorial demonstrates how to run Spark jobs for reading and writing data in different formats (converting the data format), and for running SQL queries on the data. For more information about Spark, see the Spark v3.1.2 quick-start guide.\nBefore You Begin To follow this tutorial, you must first ingest some data, such as a CSV or Parquet file, into the platform (i.e., write data to a platform data container). For information about the available data-ingestion methods, see the Ingesting and Preparing Data and Ingesting and Consuming Files tutorials.\nData Formats The Spark jobs in this tutorial process data in the following data formats:\n  Comma Separated Value (CSV)\n  Parquet — an Apache columnar storage format that can be used in Apache Hadoop. For more information about Parquet, see https://parquet.apache.org/. For more information about Hadoop, see the Apache Hadoop web site.\n  NoSQL — the platform's NoSQL format. A NoSQL table is a collection of items (objects) and their attributes. \u0026quot;items\u0026quot; are the equivalent of NoSQL database rows, and \u0026quot;attributes\u0026quot; are the equivalent of NoSQL database columns. All items in the platform share one common attribute, which serves as an item's name and primary key. The value of this attribute must be unique to each item within a given NoSQL table. The primary key enables unique identification of specific items in the table, and efficient sharding of the table items. For more information, see Working with NoSQL Data.\nYou can use Spark Datasets API Reference, or the platform's NoSQL Web API Reference, to add, retrieve, and remove NoSQL table items. You can also use the platform's Spark API extensions or NoSQL Web API to extend the basic functionality of Spark Datasets (for example, to conditionally update an item in a NoSQL table). For more information, see the related API references.\n  Using a Web Notebook A common way to run Spark data jobs is by using web notebook for performing interactive data analytics, such as Jupyter Notebook. You create a web notebook with notes that define Spark jobs for interacting with the data, and then run the jobs from the web notebook. The code can be written in any of the supported language interpreters. This tutorial contains examples in Scala and Python. For more information about Jupyter Notebook, see the product documentation. See also Running Spark Jobs from a Web Notebook in the Spark reference overview. The examples in this tutorial were tested with Spark v3.1.2.\nSelecting the Programming Language and Creating a Spark Session In JupyterLab, select to create a new Python or Scala notebook. Scala Jupyter NotebooksVersion 3.2.1 of the platform doesn't support Scala Jupyter notebooks. See the Software Specifications and Restrictions.  Then, add the following code in your Jupyter notebook cell or Zeppelin note paragraph to perform required imports and create a new Spark session; you're encouraged to change the appName string to provide a more unique description:\n import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SaveMode import org.apache.spark.sql.types._ val spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate()  import sys from pyspark.sql import SparkSession from pyspark.sql import * from pyspark.sql.types import * from pyspark.sql.functions import * spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate()    At the end of your code flow, add a cell/paragraph with the following code to stop the Spark session and release its resources:  spark.stop()  spark.stop()    Sample Workflows Following are some possible workflows that use the Spark jobs outlined in this tutorial:\nWorkflow 1: Convert a CSV File into a Partitioned Parquet Table   Write a CSV file to a platform data container.\n  Convert the CSV file into a Parquet table.\n  Run SQL queries on the data in Parquet table.\n  Workflow 2: Convert a Parquet Table into a NoSQL Table   Write a Parquet table to a platform data container.\n  Convert the Parquet table into a NoSQL table.\n  Run SQL queries on the data in NoSQL table.\n  Reading the Data  Reading CSV Data Reading Parquet Data Reading NoSQL Data  Reading CSV Data Use the following code to read data in CSV format. You can read both CSV files and CSV directories.\nDefining the Table SchemaTo read CSV data using a Spark DataFrame, Spark needs to be aware of the schema of the data. You can either define the schema programmatically as part of the read operation as demonstrated in this section, or let Spark infer the schema as outlined in the Spark SQL and DataFrames documentation (e.g., option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;) in Scala or csv(..., inferSchema=\u0026quot;true\u0026quot;) in Python). (Note that inferSchema requires an extra pass over the data.)  NoteBefore running the read job, ensure that the referenced data source exists.  Syntax The header and delimiter options are optional.\n val \u0026lt;schema variable\u0026gt; = StructType(List( StructField(\u0026#34;\u0026lt;column name\u0026gt;\u0026#34;, \u0026lt;column type\u0026gt;, nullable = \u0026lt;Boolean value\u0026gt;), \u0026lt;additional StructField() columns\u0026gt; of the same format, for each column\u0026gt;)) val \u0026lt;DF variable\u0026gt; = spark.read.schema(\u0026lt;schema variable\u0026gt;) .option(\u0026#34;header\u0026#34;, \u0026#34;\u0026lt;true/false\u0026gt;\u0026#34;) .option(\u0026#34;delimiter\u0026#34;, \u0026#34;\u0026lt;delimiter\u0026gt;\u0026#34;) .csv(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to CSV data\u0026gt;\u0026#34;)  \u0026lt;schema variable\u0026gt; = StructType([ StructField(\u0026#34;\u0026lt;column name\u0026gt;\u0026#34;, \u0026lt;column type\u0026gt;, \u0026lt;is-nullable Boolean value\u0026gt;), \u0026lt;additional StructField() columns\u0026gt; of the same format, for each column\u0026gt;)]) \u0026lt;DF variable\u0026gt; = spark.read.schema(\u0026lt;schema variable\u0026gt;) \\ .option(\u0026#34;header\u0026#34;, \u0026#34;\u0026lt;true/false\u0026gt;\u0026#34;) \\ .option(\u0026#34;delimiter\u0026#34;, \u0026#34;\u0026lt;delimiter\u0026gt;\u0026#34;) \\ .csv(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to CSV data\u0026gt;\u0026#34;)    Example The following example reads a /mydata/nycTaxi.csv CSV file from the \u0026quot;projects\u0026quot; container into a myDF DataFrame variable.\n val schema = StructType(List( StructField(\u0026#34;pickup_time\u0026#34;, LongType, nullable = true), StructField(\u0026#34;dropoff_time\u0026#34;, LongType, nullable = true), StructField(\u0026#34;passenger_count\u0026#34;, LongType, nullable = true), StructField(\u0026#34;trip_distance\u0026#34;, DoubleType, nullable = true), StructField(\u0026#34;payment_type\u0026#34;, LongType, nullable = true), StructField(\u0026#34;fare_amount\u0026#34;, DoubleType, nullable = true), StructField(\u0026#34;tip_amount\u0026#34;, DoubleType, nullable = true), StructField(\u0026#34;tolls_amount\u0026#34;, DoubleType, nullable = true), StructField(\u0026#34;total_amount\u0026#34;, DoubleType, nullable = true) )) val myDF = spark.read.schema(schema) .option(\u0026#34;header\u0026#34;, \u0026#34;false\u0026#34;) .option(\u0026#34;delimiter\u0026#34;, \u0026#34;|\u0026#34;) .csv(\u0026#34;v3io://projects/mydata/nycTaxi.csv\u0026#34;)  schema = StructType([ StructField(\u0026#34;pickup_time\u0026#34;, LongType(), True), StructField(\u0026#34;dropoff_time\u0026#34;, LongType(), True), StructField(\u0026#34;passenger_count\u0026#34;, LongType(), True), StructField(\u0026#34;trip_distance\u0026#34;, DoubleType(), True), StructField(\u0026#34;payment_type\u0026#34;, LongType(), True), StructField(\u0026#34;fare_amount\u0026#34;, DoubleType(), True), StructField(\u0026#34;tip_amount\u0026#34;, DoubleType(), True), StructField(\u0026#34;tolls_amount\u0026#34;, DoubleType(), True), StructField(\u0026#34;total_amount\u0026#34;, DoubleType(), True) ]) myDF = spark.read.schema(schema) \\ .option(\u0026#34;header\u0026#34;, \u0026#34;false\u0026#34;) \\ .option(\u0026#34;delimiter\u0026#34;, \u0026#34;|\u0026#34;) \\ .csv(\u0026#34;v3io://projects/mydata/nycTaxi.csv\u0026#34;)    Reading Parquet Data Use the following code to read data as a Parquet database table.\nNoteBefore running the read job, ensure that the referenced data source exists.  Syntax  val \u0026lt;DF variable\u0026gt; = spark.read.parquet(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to Parquet data\u0026gt;\u0026#34;)  \u0026lt;DF variable\u0026gt; = spark.read.parquet(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to Parquet data\u0026gt;\u0026#34;)    Example The following example reads a /mydata/my-parquet-table Parquet database table from the \u0026quot;projects\u0026quot; container into a myDF DataFrame variable.\n val myDF = spark.read.parquet(\u0026#34;v3io://projects/mydata/my-parquet-table\u0026#34;)  myDf = spark.read.parquet(\u0026#34;v3io://projects/mydata/my-parquet-table\u0026#34;)    Reading NoSQL Data Use the following code to read data as a NoSQL table.\nDefining the Table SchemaWhen using a Spark DataFrame to read data that was written in the platform using a NoSQL Spark DataFrame, the schema of the table structure is automatically identified and retrieved (unless you select to explicitly define the schema for the read operation). However, to read NoSQL data that was written to a table in another way, you first need to define the table schema. You can either define the schema programmatically as part of the read operation as demonstrated in this section, or let the platform infer the schema by using the inferSchema option (option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;)). For more information, see Defining the Table Schema in the Spark NoSQL DataFrame reference.  NoteBefore running the read job, ensure that the referenced data source exists.  Syntax  val \u0026lt;schema variable\u0026gt; = StructType(List( StructField(\u0026#34;\u0026lt;column name\u0026gt;\u0026#34;, \u0026lt;column type\u0026gt;, nullable = \u0026lt;Boolean value\u0026gt;), \u0026lt;additional StructField() columns\u0026gt; of the same format, for each column\u0026gt;)) val \u0026lt;DF variable\u0026gt; = spark.read.schema(\u0026lt;schema variable\u0026gt;) .format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to a NoSQL table\u0026gt;\u0026#34;)  \u0026lt;schema variable\u0026gt; = StructType([ StructField(\u0026#34;\u0026lt;column name\u0026gt;\u0026#34;, \u0026lt;column type\u0026gt;, \u0026lt;is-nullable Boolean value\u0026gt;), \u0026lt;additional StructField() columns\u0026gt; of the same format, for each column\u0026gt;)]) \u0026lt;DF variable\u0026gt; = spark.read.schema(\u0026lt;schema variable\u0026gt;) \\ .format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to a NoSQL table\u0026gt;\u0026#34;)    Example The following example reads a /mydata/flights NoSQL table from the \u0026quot;projects\u0026quot; container into a myDF DataFrame variable.\n val schema = StructType(List( StructField(\u0026#34;id\u0026#34;, StringType, nullable = false), StructField(\u0026#34;origin_country\u0026#34;, StringType, nullable = true), StructField(\u0026#34;call_sign\u0026#34;, StringType, nullable = true), StructField(\u0026#34;velocity\u0026#34;, DoubleType, nullable = true), StructField(\u0026#34;altitude\u0026#34;, DoubleType, nullable = true), StructField(\u0026#34;__mtime_secs\u0026#34;, LongType, nullable = true) )) val myDF = spark.read.schema(schema) .format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://projects/mydata/flights\u0026#34;)  schema = StructType([ StructField(\u0026#34;id\u0026#34;, StringType(), False), StructField(\u0026#34;origin_country\u0026#34;, StringType(), True), StructField(\u0026#34;call_sign\u0026#34;, StringType(), True), StructField(\u0026#34;velocity\u0026#34;, DoubleType(), True), StructField(\u0026#34;altitude\u0026#34;, DoubleType(), True), StructField(\u0026#34;__mtime_secs\u0026#34;, LongType(), True) ]) myDF = spark.read.schema(schema) \\ .format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://projects/mydata/flights\u0026#34;)    Writing the Data (Converting the Format)  Writing Parquet Data Writing NoSQL Data Writing CSV Data  Writing Parquet Data Use the following code to write data as a Parquet database table.\nSyntax  \u0026lt;DF variable\u0026gt;.write.parquet(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to Parquet data\u0026gt;\u0026#34;)  \u0026lt;DF variable\u0026gt;.write.parquet(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to Parquet data\u0026gt;\u0026#34;)    Example The following example converts the data that is currently associated with the myDF DataFrame variable into a /mydata/my-parquet-table Parquet database table in the \u0026quot;projects\u0026quot; container.\n myDF.write.parquet(\u0026#34;v3io://projects/mydata/my-parquet-table\u0026#34;)  myDF.write.parquet(\u0026#34;v3io://projects/mydata/my-parquet-table\u0026#34;)    Writing NoSQL Data Use the following code to write data as a NoSQL table.\nSyntax  \u0026lt;DF variable\u0026gt;.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026lt;key column\u0026gt;) .save(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to a NoSQL table\u0026gt;\u0026#34;)  \u0026lt;DF variable\u0026gt;.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026lt;key column\u0026gt;) \\ .save(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to a NoSQL table\u0026gt;\u0026#34;)    Example The following example converts the data that is currently associated with the myDF DataFrame variable into a /mydata/my-nosql-table NoSQL table in the \u0026quot;projects\u0026quot; container.\n myDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;ID\u0026#34;).save(\u0026#34;v3io://projects/mydata/my-nosql-table\u0026#34;)  myDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;ID\u0026#34;).save(\u0026#34;v3io://projects/mydata/my-nosql-table\u0026#34;)    Writing CSV Data Use the following code to write data in CSV format. You can write both CSV files and CSV directories.\nSyntax The header and delimiter options are optional.\n \u0026lt;DF variable\u0026gt;.write .option(\u0026#34;header\u0026#34;, \u0026#34;\u0026lt;true/false\u0026gt;\u0026#34;) .option(\u0026#34;delimiter\u0026#34;, \u0026#34;\u0026lt;delimiter\u0026gt;\u0026#34;) .csv(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to CSV data\u0026gt;\u0026#34;)  \u0026lt;DF variable\u0026gt;.write \\ .option(\u0026#34;header\u0026#34;, \u0026#34;\u0026lt;true/false\u0026gt;\u0026#34;) \\ .option(\u0026#34;delimiter\u0026#34;, \u0026#34;\u0026lt;delimiter\u0026gt;\u0026#34;) \\ .csv(\u0026#34;v3io://\u0026lt;container name\u0026gt;/\u0026lt;path to CSV data\u0026gt;\u0026#34;)    Example The following example converts the data that is currently associated with the myDF DataFrame variable into /mydata/my-csv-data CSV data in the \u0026quot;projects\u0026quot; container.\n myDF.write.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).option(\u0026#34;delimiter\u0026#34;, \u0026#34;,\u0026#34;) .csv(\u0026#34;v3io://projects/mydata/my-csv-data\u0026#34;)  myDF.write.option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).option(\u0026#34;delimiter\u0026#34;, \u0026#34;,\u0026#34;) \\ .csv(\u0026#34;v3io://projects/mydata/my-csv-data\u0026#34;)    Running SQL Data Queries Use the following syntax to run an SQL query on your data.\nSyntax The call to show is optional.\n \u0026lt;DF variable\u0026gt;.createOrReplaceTempView(\u0026#34;\u0026lt;SQL table name\u0026gt;\u0026#34;) spark.sql(\u0026#34;\u0026lt;SQL query string\u0026gt;\u0026#34;).show()  \u0026lt;DF variable\u0026gt;.createOrReplaceTempView(\u0026#34;\u0026lt;SQL table name\u0026gt;\u0026#34;) spark.sql(\u0026#34;\u0026lt;SQL query string\u0026gt;\u0026#34;).show()    Example The following example creates a temporary myTable SQL table for the database associated with the myDF DataFrame variable, and runs an SQL query on this table:\n myDF.createOrReplaceTempView(\u0026#34;myTable\u0026#34;) spark.sql(\u0026#34;select column1, count(1) as count from myTable where column2=\u0026#39;xxx\u0026#39; group by column1\u0026#34;).show()  myDF.createOrReplaceTempView(\u0026#34;myTable\u0026#34;) spark.sql(\u0026#34;select column1, \\ count(1) as count from myTable where column2=\u0026#39;xxx\u0026#39; group by column1\u0026#34;) \\ .show()    See Also  The Spark Service Ingesting and Preparing Data Spark Datasets API Reference  The NoSQL Spark DataFrame NoSQL Table Schema Reference    ","keywords":["data","ingestion,","spark,","spark","apis,","spark","dataframes,","getting","started,","quick-start,","web","notebook,,","jupyter,","jupyter","notebook,","hadoop,","csv,","parquet,","nosql,","key","value,","KV,","nosql","dataframe,","nosql","tables,","tables,","table","items,","item","names,","primary","key,","attributes,","scala,","python,","table","schema,","schema,","infer","schema,","inferSchema"],"path":"https://github.com/jasonnIguazio/data-layer/spark-data-ingestion-qs/","title":"Getting Started with Data Ingestion Using Spark"},{"content":"This section contains installation and how-to guides for installing (deploying) and configuring the MLOps Platform (\u0026quot;the platform\u0026quot;) on Google Cloud, also known as Google Cloud Platform (GCP).\n","keywords":["google","cloud","deployment,","google","cloud","platform","deployment,","gcp","deployment,","google","cloud","installation,","google","cloud","platform","installation,","gcp","installation,","google","cloud","setup,","gcp","setup,","google","cloud,","google","cloud","platform,","gcp"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/gcp/","title":"Google Cloud Deployment"},{"content":" Overview The MLOps Platform (\u0026quot;the platform\u0026quot;) provides enterprises with the flexibility to run applications on-premises (\u0026quot;on-prem\u0026quot;) or in an edge or hybrid cloud architecture. The platform was built from the ground up to maximize CPU utilization and leverage the benefits of non-volatile memory, 100 GbE remote direct memory access (RDMA), flash memory, and dense storage. This design enables the platform to achieve extreme performance while maintaining data consistency, at the lowest cost.\nHardware Configurations The platform is available in two configurations, which differ in a variety of aspects, including the performance capacity, footprint, storage size, and scale capabilities:\n Development Kit A single data-node and single application-node cluster implementation. This configuration is designed mainly for evaluation trials and doesn't include high availability (HA) or performance testing. Operational Cluster A scalable cluster implementation that is composed of multiple data and application nodes. This configuration was designed to achieve superior performance that enables real-time execution of analytics, machine-learning (ML), and artificial-intelligence (AI) applications in a production pipeline. The minimum requirement for HA support is three data nodes and three application nodes.  Both configurations also support an additional backup node for backing up the platform instance.\nDeployment Methods The platform supports the following alternative deployment methods:\n Cloud Deployment on an Amazon Web Services (AWS) or Microsoft Azure cloud — either as part of your virtual private cloud (VPC) or virtual network (VNet) or as a software as a service (SaaS) in Iguazio's cloud account. See AWS Cloud Deployment and Azure Cloud Deployment. On-Prem On-premises deployment. The data nodes are deployed on virtual machines (VMs), and the application nodes can optionally be deployed either on VMs or on physical machines (bare-metal). See On-Prem Deployment.  Notes  All capacity calculations in the hardware specifications are performed using the base-10 (decimal) number system. For example, 1 TB = 1,000,000,000,000 bytes.  See Also  AWS Cloud Deployment Azure Cloud Deployment On-Prem Deployment Software Specifications and Restrictions Support and Certification Matrix High Availability (HA)  ","keywords":["hardware","configuration,","deployment","options,","hardware","specifications,","hardware","specs,","specification,","spec,","development","kit,","operational","cluster,","cloud,","on-premises,","on-prem,","bare","metal,","hybrid","cloud,","edge","cloud,","virtual","machines,","vms"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cfg-options/","title":"Hardware-Configuration and Deployment Options"},{"content":" Overview The Operational Cluster configuration of the MLOps Platform (\u0026quot;the platform\u0026quot;) is a high-availability (HA) cluster. It was designed from the ground up as a shared-nothing (SN) architecture — a distributed architecture with no single point of failure (SPOF). Every cluster has at least three data nodes and supports data replication, ensuring high availability.\nData-Replication and Failover The platform's data-distribution architecture ensures that there are always two copies of each data element. The system provides strong consistency at the object level: only one consistent state can be observed by all users at any given point in time. The provided solution uses high-end and enterprise-grade hardware in terms of reliability. The platform uses Non-Volatile Memory Express (NVMe) drives with 2M hours of mean time between failure (MTBF) and 5 drive writes per day (DWPD) endurance.\nThe data distribution is done by using a variant of the consistent hashing algorithm with a fixed number of partitions. Each data node in the cluster is divided into virtual nodes (VNs). Each data container is divided into slices (\u0026quot;partitions\u0026quot;), which are the basic units for data distribution and recovery. Each slice is handled by both a primary and a secondary VN. Each pair of primary and secondary VNs has a very limited set of shared slices. Consequently, in the event of a software or hardware failure, the service remains available without interruption while maintaining load balancing. The platform is designed to support CAP-theorem CP (consistent but not available under network partitions) over AP (available but not consistent under network partitions). This means that in the possible event of partitioning, data availability is sacrificed to preserve consistency.\nThe platform's data-distribution implementation consists of two planes— a control plane and a data plane.\nThe Control Plane The control plane determines the cluster members and creates a slice-mapping table. This plane\n Uses the Raft consensus algorithm to define for each container the slice distribution among the VNs. Uses a variation of a consistent-hashing algorithm to distribute the container data across the VNs.  The Data Plane The data plane provides strong consistency. This plane\n Replicates I/O between the primary VN and its secondary VN. Ensures that a minimal number of hops is required to complete an I/O operation by sending the I/O request to the primary VN and handling retries for different failure scenarios. Distributes the data among the VNs according to the distribution rules defined by the control plane.  The data plane uses the V3IO library, which serves as the platform's I/O interface and controls access to data. This library uses a mathematical function to identify the parent slice of each data element and map the slice to its primary VN by using the control plane's slice-mapping table. This mapping is dynamically updated as a result of relevant changes to the cluster, such as in the event of a scale-out scenario or upon a system failure. The V3IO library is linked to the user application and its implementation is entirely transparent, so users don't need to be aware of the internal distribution mechanism.\nSlice Distribution Following are some key principles of the platform's slice-distribution algorithm:\n Same number of primary and secondary roles per VN — to support load balancing. Balanced peer to peer pairing — to minimize rebuild times. Per-container slice table — to ensure that the same object name will fall under different slices for different containers.  The following image demonstrates slice distribution:    Write I/O Flow The following diagram describes a write I/O flow, demonstrating these concepts:\n Strong consistency guarantees that every acknowledged write can be viewed immediately and upon any failure. Atomic updates guarantee that only the old or new data image is visible at any given point in time, enabling multiple parallel uncoordinated application updates.     See Also  Deployment and Specifications  ","keywords":["HA,","high","availability,","failover,","failback,","architecture,","virtual","nodes,","container","slices"],"path":"https://github.com/jasonnIguazio/intro/high-availability/","title":"High Availability (HA)"},{"content":"exists exists(ATTRIBUTE) Checks whether an attribute with the specified name (ATTRIBUTE — see Attribute Variables) exists in the evaluated item.\nFor example, exists(Vendor) returns true if the evaluated item has a Vendor attribute.\nif_not_exists if_not_exists(ATTRIBUTE, VALUE) If the specified attribute (ATTRIBUTE — see Attribute Variables) exists, the function returns its value. Otherwise, the function returns the default value specified in the VALUE parameter.\nFor example, a \u0026quot;SET lastName = if_not_exists(lastName, '')\u0026quot; update expression creates a lastName attribute and initializes its value to an empty string, provided the attribute doesn't already exist; if the attribute exists, it will be reassigned its current value (i.e., remain unchanged).\nNoteif_not_exists is supported in update expressions.  See Also  Update Expression Attribute Variables  ","keywords":["if-exists","function,","exists","function,","if-not-exists","function,","if_not_exists","function,","if_not_exists,","attribute","variables,","update","expressions"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/functions/exists-n-not-exists-functions/","title":"If Exists/Not-Exists Expression Functions"},{"content":" Get ready to start your free evaluation of the MLOps Platform ... .\nOverview Iguazio trial is a free evaluation of the MLOps Platform (\u0026quot;the platform\u0026quot;) from within a cloud environment that doesn't require any user installations or configurations. When you register to the trial, you receive your own cluster — a dedicated cloud instance of the platform. To register for a free evaluation, click here or select the Try Iguazio for free button on the Iguazio web site.\nThe trial allows you to test most of the functions and features of the platform, including full access to its APIs and graphical and command-line interfaces. However, note that\n The trial environment currently uses only a single node and isn't provisioned for performance, and its capacity is restricted to 900 GB.  The free evaluation it typically restricted to 14 days. You should not change the password of your cluster's predefined system user (typically named \u0026quot;admin\u0026quot;). This is a trial-only restriction for the purpose of simplifying the evaluation process.  For general information about the Iguazio trial offering and the registration process, contact trial@iguazio.com.  For technical questions and assistance, contact Iguazio's customer-success engineers, who are available to assist and guide you throughout your evaluation.\nNoteThe trial runs in the cloud. For the rare instances in which the documentation differentiates among the different deployment types, follow the cloud guidelines.  Getting Started It's recommended that you start your evaluation by following these steps:\n  Watch the introductory video (available also on YouTube). Note that there might be some differences between your environment and what you see in the video, depending on the version that you are using:     Read the platform introduction — Introducing the Platform. The introduction also contains useful links for continuing your evaluation.\n  Log into your cluster's dashboard using the URL and login credentials that you received, and then log into the predefined Jupyter Notebook service or create a new service (see the instructions on the welcome pop-up page or in the Working with Services tutorial.)  The tutorials include a welcome.ipynb notebook (available also as a README.md Markdown file) — which is similar to the platform introduction that you read in the previous step — and many other useful tutorials.\n  What's Next?  Follow the getting-started tutorial Jupyter tutorial, which demonstrates key features and benefits of the platform. Read the product introduction and in-depth overview, and browse the introductory content in the sections that best fits your development needs (see the table-of-contents menu). You can also find useful links on the documentation home page. Review and run other Jupyter tutorial notebooks. You can find full end-to-end use-case application and how-to demos in the demos tutorials directory. For more information, see Introducing the Platform.  ","keywords":["Iguazio","trial,","cloud","trial,","trial,","free","trial,","evaluation,","product","evaulation,","getting","started,","quick-start,","introduction,","v3io","tutorials,","platform","tutorials,","tutorials,","api","examples,","code","examples,","code","walkthroughs,","jupyter,","jupypter","notebooks,","dashboard,","UI,","demos,","v3io"],"path":"https://github.com/jasonnIguazio/intro/trial-qs/","title":"Iguazio Trial Quick-Start"},{"content":"OPERAND-A IN (OPERAND-B) The IN operator checks whether OPERAND-A is found in OPERAND-B, where OPERAND-B is a comma-separated list of 1–1024 values (or expressions). OPERAND-A and the members of OPERAND-B should be of the same type — either strings or integers.\nFor example, \u0026quot;genre IN ('Rock', 'Blues', 'R\u0026amp;B')\u0026quot; checks whether the value of a \u0026quot;genre\u0026quot; string attribute is identical to one of the genre strings in OPERAND-B (\u0026quot;Rock\u0026quot;, \u0026quot;Blues\u0026quot;, or \u0026quot;R\u0026amp;B\u0026quot;).\n","keywords":["expression","IN","operator,","IN","operator,","expression","operators,","attribute","variables,"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/in-operator/","title":"IN Operator"},{"content":" Overview This document provides an in-depth overview of the MLOps Platform (\u0026quot;the platform\u0026quot;) and how to use it to implement a full data science workflow.\nNoteStart out by first reading the Introducing the Platform introduction and high-level overview.  The platform uses Kubernetes (k8s) as the baseline cluster manager, and deploys various application microservices on top of Kubernetes to address different data science tasks. Most of the provided services support scaling out and GPU acceleration and have a secure and low-latency access to the platform's shared data store and file system, enabling high performance and scalability with maximum resource efficiency.\nThe platform makes extensive use of Nuclio serverless functions to automate various tasks — such as data collection, extract-transform-load (ETL) processes, model serving, and batch jobs. Nuclio functions describe the code and include all the required resource definitions and configuration for running the code. The functions auto scale and can be versioned. The platform supports various methods for generating Nuclio functions — using the graphical dashboard, Docker, Git, or Jupyter Notebook — as demonstrated in the platform tutorials. The following sections detail how you can use the platform to implement all stages of a data science workflow from research to production.\nCollecting and Ingesting Data There are many ways to collect and ingest data from various sources into the platform:\n Streaming data in real time from sources such as Kafka, Kinesis, Azure Event Hubs, or Google Pub/Sub.  Loading data directly from external databases using an event-driven or periodic/scheduled implementation. See the explanation and examples in the read-external-db tutorial.  Loading files (objects), in any format (for example, CSV, Parquet, JSON, or a binary image), from internal or external sources such as Amazon S3 or Hadoop. See, for example, the file-access tutorial.  Importing time-series telemetry data using a Prometheus compatible scraping API.  Ingesting (writing) data directly into the system using RESTful AWS-like simple-object, streaming, or NoSQL APIs. See the platform's Web-API References.   For more information and examples of data collection, ingestion, and preparation with the platform, see the basic-data-ingestion-and-preparation tutorial Jupyter notebook.\nExploring and Processing Data The platform includes a wide range of integrated open-source data query and exploration tools, including the following:\n Apache Spark data-processing engine — including the Spark SQL and Datasets, MLlib, R, and GraphX libraries — with real-time access to the platform's NoSQL data store and file system. Presto distributed SQL query engine, which can be used to run interactive SQL queries over platform NoSQL tables or other object (file) data sources. pandas Python analysis library, including structured DataFrames. Dask parallel-computation Python library, including scaled pandas DataFrames. Iguazio V3IO Frames — Iguazio's open-source data-access library, which provides a unified high-performance API for accessing NoSQL, stream, and time-series data in the platform's data store and features native integration with pandas and NVIDIA RAPIDS. See, for example, the frames tutorial. Built-in support for ML packages such as scikit-learn, Pyplot, NumPy, PyTorch, and TensorFlow.  All these tools are integrated with the platform's Jupyter Notebook service, allowing users to access the same data from Jupyter through different interfaces with minimal configuration overhead. Users can easily install additional Python packages by using the Conda binary package and environment manager and the pip Python package installer, which are both available as part of the Jupyter Notebook service. This design, coupled with the platform's unified data model, enables users to store and access data using different formats — such as NoSQL (\u0026quot;key/value\u0026quot;), time series, stream data, and files (simple objects) — and leverage different tools and APIs for accessing and manipulating the data, all from a single development environment (namely, Jupyter Notebook).\nNoteYou can deploy and manage application services, such as Spark and Jupyter Notebook, from the Services page of the platform dashboard. For more information, see Working with Services.  For more information and examples of data exploration with the platform, see the getting-started and basic-data-ingestion-and-preparation tutorials. Building and Training Models You can develop and test data science models in the platform's Jupyter Notebook service or in your preferred external editor. When your model is ready, you can train it in Jupyter Notebook or by using scalable cluster resources such as Nuclio functions, Dask, Spark ML, or Kubernetes jobs. You can find model-training examples in the following platform demos; for more information and download instructions, see the platform introduction.\n The NetOps demo demonstrates predictive infrastructure-monitoring using scikit-learn. The image-classification demo demonstrates image recognition using TensorFlow and Horovod with MLRun.  If you're are a beginner, you might find the following ML guide useful — Machine Learning Algorithms In Layman's Terms.\nExperiment Tracking One of the most important and challenging areas of managing a data science environment is the ability to track experiments. Data scientists need a simple way to track and view current and historical experiments along with the metadata that is associated with each experiment. This capability is critical for comparing different runs, and eventually helps to determine the best model and configuration for production deployment. The platform leverages the open-source MLRun library to help tackle these challenges. You can find examples of using MLRun in the MLRun demos. See the information about getting additional demos in the platform introduction.\nDeploying Models to Production The platform allows you to easily deploy your models to production in a reproducible way by using the open-source Nuclio serverless framework. You provide Nuclio with code or Jupyter notebooks, resource definitions (such as CPU, memory, and GPU), environment variables, package or software dependencies, data links, and trigger information. Nuclio uses this information to automatically build the code, generate custom container images, and connect them to the relevant compute or data resources. The functions can be triggered by a wide variety of event sources, including the most commonly used streaming and messaging protocols, HTTP APIs, scheduled (cron) tasks, and batch jobs.\nNuclio functions can be created from the platform dashboard or by using standard code IDEs, and can be deployed on your platform cluster. A convenient way to develop and deploy Nuclio functions is by using Jupyter Notebook and Python tools. For detailed information about Nuclio, visit the Nuclio website and see the product documentation.\nNoteNuclio functions aren't limited to model serving: they can automate data collection, serve custom APIs, build real-time feature vectors, drive triggers, and more.  For an overview of Nuclio and how to develop, document, and deploy serverless Python Nuclio functions from Jupyter Notebook, see the nuclio-jupyter documentation. You can also find examples in the platform tutorials. For example, the NetOps demo demonstrates how to deploy a network-operations model as a function; for more information about this demo and how to get it, see the platform introduction.\nVisualization, Monitoring, and Logging Data in the platform — including collected data, internal or external telemetry and logs, and program-output data — can be analyzed and visualized in different ways simultaneously. The platform supports multiple standard data analytics and visualization tools, including SQL, Prometheus, Grafana, and pandas. For example, you can plot or chart data within Jupyter Notebook using Matplotlib; use your favorite BI visualization tools, such as Tableau, to query data in the platform over a Java database connectivity connector (JDBC); or build real-time dashboards in Grafana.\nThe data analytics and visualization tools and services generate telemetry and log data that can be stored using the platform's time-series database (TSDB) service or by using external tools such as Elasticsearch. Platform users can easily instrument code and functions to collect various statistics or logs, and explore the collected data in real time.\nThe Grafana open-source analytics and monitoring framework is natively integrated into the platform, allowing users to create dashboards that provide access to platform NoSQL tables and time-series databases from different dashboard widgets. You can also create Grafana dashboards programmatically (for example, from Jupyter Notebook) using wizard scripts. For information on how to create Grafana dashboards to monitor and visualize data in the platform, see Adding Grafana Dashboards.\nSee Also  Introducing the Platform Data Layer Platform Services APIs Overview Support and Certification Matrix Software Specifications and Restrictions and release notes (registered users only)  ","keywords":["in-depth","platform","overview,","platform","overview,","overview,","introduction,","getting","started,","data","science,","data","science","workflow,","data","colleciton,","data","ingestion,","data","exploration,","data","models,","model","training,","model","deployment,","deployment","to","production,","data","visualization,","visualization,","data","analytics,","data","monitoring,","data","logging,","mlrun,","features,","architecture,","apis,","web","apis,","data,","simple","objects,","data","services,","application","services,","data","management,","streaming,","data","streams,","sql,","nosql,","key","value,","KV,","scala,","python,","nuclio,","v3io","tsdb,","tsdb,","time","series,","v3io","frames,","frames,","presto,","spark,","grafana,","tensorflow,","keras,","pytortch,","pandas,","dask,","matplotlib,","pyplot,","numpy,","nvidia,","nvidia,","data","frames,","dataframes,","aws,","amazon,","operating","system,","web","server,","web","gateway,","nginx,","anaylics,","AI,","artificial","intelligence,","ML,","machine","learning,","mlib,","jupyter,","jupyterlab,","jupyter","notebook,","prometheus,","REST"],"path":"https://github.com/jasonnIguazio/intro/product-overview/","title":"In-Depth Platform Overview"},{"content":"\u0026lt;!-- JillG-9-9-21 Zeppelin removed from SW, no plan to return it. The #convert-csv-to-nosql section in this file now points to https://github.com/v3io/tutorials/blob/3.x/data-ingestion-and-preparation/basic-data-ingestion-and-preparation.ipynb Overview This tutorial demonstrates how to ingest (write) a new file object to a data container in the platform, and consume (read) an ingested file, either from the dashboard or by using the Simple-Object Web API. The tutorial also demonstrates how to convert a CSV file to a NoSQL table by using the Spark SQL and DataFrames API.\nThe examples are for CSV and PNG files that are copied to example directories in the platform's predefined data containers. You can use the same methods to process other types of files, and you can also modify the file and container names and the data paths that are used in the tutorial examples, but it's up to you to ensure the existence of the elements that you reference.\nBefore You Begin   Follow the Working with Data Containers tutorial to learn how to create and delete container directories and access container data.\n  Note that to send web-API requests, as demonstrated in this tutorial, you need to have the URL of the parent tenant's webapi service, and either a platform username and password or an access key for authentication. To learn more and to understand how to structure and secure the web-API requests, see the web-APIs reference, and especially Data-Service Web-API General Structure and Securing Your Web-API Requests. (The tutorial Postman examples use the username-password authentication method, but if you prefer, you can replace this with access-key authentication, as explained in the documentation.) To understand how data paths are set in web-API requests, see RESTful Web and Management API Data Paths.\n  Using the Dashboard to Ingest and Consume Files You can easily ingest and consume files from the dashboard.\nIngesting Files Using the Dashboard Follow these steps to ingest (upload) files to the platform from the dashboard:\n  On the Data page, select a container (for example, \u0026quot;bigdata\u0026quot;) to display the container Browse data tab (default), which allows you to browse the contents of the container. To upload a file to a specific container directory, navigate to this directory as explained in the Working with Data Containers tutorial.\n  Use either of the following methods to upload a file to the container:\n  Simply drag and drop a file into the main-window area (which displays the container or directory contents), as demonstrated in the following image:      Select the \u0026quot;upload\u0026quot; icon () from the action toolbar and then, when prompted, browse to the location of the file in your local file system.\n  For example, you can download the example bank.csv file and upload it to a mydata directory in the \u0026quot;bigdata\u0026quot; container. Creating the directory is as simple as selecting the new-folder icon in the dashboard () and entering the directory name; for detailed instructions, see the Working with Data Containers tutorial.\n  When the upload completes, you should see your file in the dashboard, as demonstrated in the following image:    Consuming Files Using the Dashboard Follow these steps to retrieve (download) an uploaded file from the dashboard: on the Data page, select a container (for example, bigdata) to display the container Browse data tab (default), and then use the side navigation tree to navigate to the directory that contains the file. Then, check the check box next to the file that you want to download, select the download icon () from the action toolbar, and select the location to which to download the file.\nUsing the Simple-Object Web API to Ingest and Consume Files You can ingest and consume files by sending Simple-Object Web API HTTP requests using your preferred method, such as Postman or curl.\nIngesting Files Using the Simple-Object Web API You can use Postman, for example, to send a Simple-Object Web API PUT Object request that uploads a file object to the platform:\n  Create a new request and set the request method to PUT.\n  In the request URL field, enter the following; replace \u0026lt;web-APIs URL\u0026gt; with the URL of the parent tenant's webapi service, replace \u0026lt;container name\u0026gt; with the name of the container to which you want to upload the data, and replace \u0026lt;image-file path\u0026gt; with the relative path within this container to which you want to upload the file:\n\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container name\u0026gt;/\u0026lt;image-file path\u0026gt; For example, the following URL sends a request to web-API service URL https://default-tenant.app.mycluster.iguazio.com:8443 to upload a file named igz_logo.png file to a mydata directory in the \u0026quot;bigdata\u0026quot; \u0026quot; container:\nhttps://default-tenant.app.mycluster.iguazio.com:8443/bigdata/mydata/igz_logo.png Any container directories in the specified \u0026lt;image-file path\u0026gt; path that don't already exist will be created automatically, but the container directory must already exist.\n  In the Authorization tab, set the authorization type to \u0026quot;Basic Auth\u0026quot; and enter your username and password in the respective credential fields.\n  In the Body tab —\n  Select the file format that matches the uploaded file. For an image file, select binary.\n  Select Choose Files and browse to the file to upload in your local file system.\n    Select Send to send the request, and then check the response.\n  For a successful request, you should be able to see the uploaded image file from the dashboard: in the side navigation menu, select Data and then select the container to which you uploaded the file (for example, \u0026quot;bigdata\u0026quot;). In the container's Browse tab, navigate from the left navigation tree to the container directory in which you selected to save the file (for example, mydata), and verify that the directory contains the uploaded file (for example, igz_logo.png).\nConsuming Files Using the Simple-Object Web API After you ingest a file, you can send a Simple-Object Web API GET Object request to retrieve (consume) it from the platform. For example, to retrieve the image file that you uploaded in the previous steps, define the following Postman request:\n  Set the request method to GET.\n  Enter the following as the request URL, replacing the \u0026lt;...\u0026gt; placeholders with the same data that you used in Step #2 of the ingest example:\n\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container name\u0026gt;/\u0026lt;image-file path\u0026gt; For example:\nhttps://default-tenant.app.mycluster.iguazio.com:8443/bigdata/mydata/igz_logo.png   In the Authorization tab, set the authorization type to \u0026quot;Basic Auth\u0026quot; and enter your username and password in the respective credential fields.\n  Select Send to send the request, and then check the response Body tab. The response body should contain the contents of the uploaded file.\n  Converting a CSV File to a NoSQL Table The unified data model of the platform allows you to ingest a file in one format and consume it in another format. The [basic-data-ingestion-and-preparation] (https://github.com/v3io/tutorials/tree/v3.0.13/data-ingestion-and-preparation/basic-data-ingestion-and-preparation.ipynb) platform tutorial Jupyter notebook presents a Python example code for converting a CSV file to a NoSQL table.\nNote To run the Jupyter Notebook example, you first need to create the respective notebook service. See Creating a New Service.    For more information and examples of using Spark to ingest and consume data in different formats and convert data formats, see the Getting Started with Data Ingestion Using Spark tutorial.\n","keywords":["data","ingestion,","data","consumption,","files,","objects,","ingest","file,","injest","object,","put","object,","copy","file,","upload","file,","consume","file,","consume","object,","get","file,","get","object,","getting","started,","quick-start,","dashboard,","UI,","simple-object","web","api,","S3,","PUT","Object,","GET","Object,","web","apis,","postman,","curl,","zeppelin"],"path":"https://github.com/jasonnIguazio/data-layer/objects/ingest-n-consume-files/","title":"Ingesting and Consuming Files"},{"content":"Learn about different methods for ingesting data into the MLOps Platform, analyzing the data, and preparing it for the next step in your data pipeline.\nNoteA version of this tutorial is available also as a README notebook and Markdown file in the data-ingestion-and-preparation directory of the platform's tutorial Jupyter notebooks, which also contains most of the tutorial notebooks referenced in this tutorial. After reading the tutorial, review and run from a Jupyter Notebook service the tutorial notebooks that are most relevant to your development needs. A good place to start is the basic-data-ingestion-and-preparation.ipynb notebook. If you don't already have a Jupyter Notebook service, create one first.  Overview The MLOps Platform (\u0026quot;the platform) allows storing data in any format. The platform's multi-model data layer and related APIs provide enhanced support for working with NoSQL (\u0026quot;key-value\u0026quot;), time-series, and stream data. Various steps of the data science life cycle (pipeline) might require different tools and frameworks for working with data, especially when it comes to the different mechanisms required during the research and development phase versus the operational production phase. The platform features a wide set of methods for manipulating and managing data, of different formats, in each step of the data life cycle, using a variety of frameworks, tools, and APIs — such as Spark SQL and DataFrames, Spark Streaming, Presto SQL queries, pandas DataFrames, Dask, the V3IO Frames Python library, and web APIs.\nThis tutorial provides an overview of various methods for collecting, storing, and manipulating data in the platform, and refers to sample tutorial notebooks that demonstrate how to use these methods.\nFor an in-depth overview of the platform and how it can be used to implement a full data science workflow, see In-Depth Platform Overview. For information about the available full end-to-end use-case application and how-to demos, see Introducing the Platform.\n   Basic Flow The virtual-env tutorial walks you through basic scenarios of ingesting data from external sources into the platform's data store and manipulating the data using different data formats. The tutorial includes an example of ingesting a CSV file from an AWS S3 bucket; converting it into a NoSQL table using Spark DataFrames; running SQL queries on the table; and converting the table into a Parquet file.\nThe Platform's Data Layer The platform features an extremely fast and secure data layer  (a.k.a. \u0026quot;data store\u0026quot;) that supports storing data in different formats — SQL, NoSQL, time-series databases, files (simple objects), and streaming. The data is stored within data containers can be accessed using a variety of APIs — including simple-object, NoSQL (\u0026quot;key-value\u0026quot;), and streaming APIs. For detailed information, see the data-layer APIs overview and the data-layer API references.\nPlatform Data Containers Data is stored within data containers in the platform's distributed file system (DFS), which makes up the platform's data layer (a.k.a. \u0026quot;data store\u0026quot;). There are predefined containers, such as the \u0026quot;users\u0026quot; container, and you can also create additional custom containers. For detailed information about data containers and how to use them and reference data in containers, see Data Containers and API Data Paths.\nThe Simple-Object Platform API The platform's Simple-Object API enables performing simple data-object and container operations that resemble the Amazon Web Services (AWS) Simple Storage Service (S3) API. In addition to the S3-like capabilities, the Simple-Object Web API enables appending data to existing objects. See Data Objects and the Simple-Object Web API Reference. For more information and API usage examples, see the v3io-objects tutorial.\nThe NoSQL (Key-Value) Platform API The platform's NoSQL (a.k.a. key-value/KV) API provides access to the platform's NoSQL data store (database service), which enables storing and consuming data in a tabular format. See Working with NoSQL Data. For more information and API usage examples, see the v3io-kv tutorial.\nThe Streaming Platform API The platform's Streaming API enables working with data in the platform as streams. See the Streaming Web API Reference. For more information and API usage examples, see the v3io-streams tutorial. In addition, see the Working with Streams section in the current tutorial for general information about different methods for working with data streams in the platform.\nReading Data from External Databases You can use different methods to read data from external databases into the platform's data store, such Spark over JDBC or SQLAlchemy.\nUsing Spark over JDBC Spark SQL includes a data source that can read data from other databases using Java database connectivity (JDBC). The results are returned as a Spark DataFrame that can easily be processed using Spark SQL, or joined with other data sources. The spark-jdbc tutorial includes several examples of using Spark JDBC to ingest data from various databases — such as MySQL, Oracle, and PostgreSQL.\nUsing SQLAlchemy The read-external-db tutorial outlines how to ingest data using SQLAlchemy — a Python SQL toolkit and Object Relational Mapper, which gives application developers the full power and flexibility of SQL — and then use Python DataFrames to work on the ingested data set.\nWorking with Spark The platform has a default pre-deployed Spark service that enables ingesting, analyzing, and manipulating data using different Spark APIs:\n Using Spark SQL and DataFrames Using the Spark Streaming API — see Using Streaming Streaming under \u0026quot;Working with Spark\u0026quot;.  Using Spark SQL and DataFrames Spark lets you write and query structured data inside Spark programs by using either SQL or a familiar DataFrame API. DataFrames and SQL provide a common way to access a variety of data sources. You can use the Spark SQL and DataFrames API to ingest data into the platform, for both batch and micro-batch processing, and analyze and manipulate large data sets, in a distributed manner.\nThe platform's custom NoSQL Spark DataFrame implements the Spark data-source API to support a custom data source that enables reading and writing data in the platform's NoSQL store using Spark DataFrames, including enhanced features such as data pruning and filtering (predicate push down); queries are passed to the platform's data store, which returns only the relevant data. This allows accelerated and high-speed access from Spark to data stored in the platform.\nThe spark-sql-analytics tutorial demonstrates how to use Spark SQL and DataFrames to access objects, tables, and unstructured data that persists in the platform's data store.\nFor more information and examples of data ingestion with Spark DataFrames, see Getting Started with Data Ingestion Using Spark.\nFor more about running SQL queries with Spark, see Running Spark SQL Queries under \u0026quot;Running SQL Queries on Platform Data\u0026quot;.\nWorking with Streams The platform supports various methods for working with data streams, including the following:\n Using Nuclio to Get Data from Common Streaming Engines Using the Platform's Streaming Engine Using Spark Streaming  Using Nuclio to Get Data from Common Streaming Engines The platform has a default pre-deployed Nuclio service that uses Iguazio's Nuclio serverless-framework, which provides a mechanism for analyzing and processing real-time events from various streaming engines. Nuclio currently supports the following streaming frameworks — Kafka, Kinesis, Azure Event Hubs, platform streams (a.k.a. V3IO streams), RabbitMQ, and MQTT.\nUsing Nuclio functions to retrieve and analyze streaming data in real time is a very common practice when building a real-time data pipeline. You can stream any type of data — such as telemetry (NetOps) metrics, financial transactions, web clicks, or sensors data — in any format, including images and videos. You can also implement your own logic within the Nuclio function to manipulate or enrich the consumed stream data and prepare it for the next step in the pipeline.\nNuclio serverless functions can sustain high workloads with very low latencies, thus making them very useful for building an event-driven applications with strict latency requirements.\nFor more information about Nuclio, see the Nuclio Serverless Functions.\nUsing the Platform's Streaming Engine The platform features a custom streaming engine and a related stream format — a platform stream (a.k.a. V3IO stream). You can use the platform's streaming engine to write data into a queue in a real-time data pipeline, or as a standard streaming engine (similar to Kafka and Kinesis), so you don't need to use an external engine.\nThe platform's streaming engine is currently available via the platform's Streaming Web API.\nIn addition, the platform's Spark-Streaming Integration API enables using the Spark Streaming API to work with platform streams, as explained in the next section (Using Spark Streaming).\nThe v3io-streams tutorial demonstrates basic usage of the streaming API.\nUsing Spark Streaming You can use the Spark Streaming API to ingest, consume, and analyze data using data streams. The platform features a custom Spark-Streaming Integration API to allow using the Spark Streaming API with platform streams.\nRunning SQL Queries on Platform Data You can run SQL queries on NoSQL and Parquet data in the platform's data store, using any of the following methods:\n Running full ANSI Presto SQL queries using SQL magic Running Spark SQL queries Running SQL queries from Nuclio functions  Running Full ANSI Presto SQL Queries The platform has a default pre-deployed Presto service that enables using the Presto open-source distributed SQL query engine to run interactive SQL queries and perform high-performance low-latency interactive analytics on data that's stored in the platform. To run a Presto query from a Jupyter notebook, all you need is to use an SQL magic command — %sql followed by your Presto query. Such queries are executed as distributed queries across the platform's application nodes. The basic-data-ingestion-and-preparation tutorial demonstrates how to run Presto queries using SQL magic.\nNote that for running queries on Parquet tables, you need to work with Hive tables. The csv-to-hive tutorial includes a script that converts a CSV file into a Hive table.\nRunning Spark SQL Queries The spark-sql-analytics tutorial demonstrates how to run Spark SQL queries on data in the platform's data store.\nFor more information about the platform's Spark service, see Working with Spark in this tutorial.\nRunning SQL Queries from Nuclio Functions In some cases, you might need to run SQL queries as part of an event-driven application. The nuclio-read-via-presto tutorial demonstrates how to run an SQL query from a serverless Nuclio function.\nRunning SQL Queries from MLRun Jobs In some cases, you might need to run SQL queries as part of an MLRun job. The mlrun-read-via-presto tutorial demonstrates how to run an SQL query from an MLRun job using Presto.\nWorking with Parquet Files Parquet is a columnar storage format that provides high-density high-performance file organization.\nThe parquet-read-write tutorial demonstrates how to create and write data to a Parquet table in the platform and read data from the table.\nAfter you ingest Parquet files into the platform, you might want to create related Hive tables and run SQL queries on these tables.\nThe parquet-to-hive tutorial demonstrates how you can do this using Spark DataFrames.\nAccessing Platform NoSQL and TSDB Data Using the Frames Library V3IO Frames (\u0026quot;Frames\u0026quot;) is a multi-model open-source data-access library, developed by Iguazio, which provides a unified high-performance DataFrame API for working with data in the platform's data store. Frames currently supports the NoSQL (key-value) and time-series (TSDB) data models via its NoSQL (nosql|kv) and TSDB (tsdb) backends. The frames tutorial provides an introduction to Frames and demonstrates how to use it to work with NoSQL and TSDB data in the platform. See also the Frames API reference.\nGetting Data from AWS S3 Using curl A simple way to ingest data from the Amazon Simple Storage Service (S3) into the platform's data store is to run a curl command that sends an HTTP request to the relevant AWS S3 bucket, as demonstrated in the following code cell. For more information and examples, see the basic-data-ingestion-and-preparation tutorial. %%sh CSV_PATH=\u0026#34;/User/examples/stocks.csv\u0026#34; curl -L \u0026#34;https://s3.wasabisys.com/iguazio/data/stocks/2018-03-26_BINS_XETR08.csv\u0026#34; \u0026gt; ${CSV_PATH} Running Distributed Python Code with Dask Dask is a flexible library for parallel computation in Python, which is useful for computations that don't fit into a DataFrame. Dask exposes low-level APIs that enable you to build custom systems for in-house applications. This helps parallelize Python processes and dramatically accelerates their performance. The dask-cluster tutorial demonstrates how to use Dask with platform data.\nDask is pre-deployed in the platform's Jupyter Notebook service. For more information about using Dask in the platform, see the Dask application service.\nRunning DataFrames on GPUs using NVIDIA cuDF The platform allows you to use NVIDIA's RAPIDS open-source libraries suite to execute end-to-end data science and analytics pipelines entirely on GPUs. cuDF is a RAPIDS GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data. This library features a pandas-like API that will be familiar to data engineers and data scientists, who can use it to easily accelerate their workflows without going into the details of CUDA programming. The gpu-cudf-vs-pd tutorial demonstrates how to use the cuDF library and compares performance benchmarks with pandas and cuDF.\nNoteTo use the cuDF library, you need to create a RAPIDS Conda environment. For more information, see the virtual-env tutorial.  For more information about the platform's GPU support, see Running Applications over GPUs.\nVisualizing Data with Grafana The platform has a Grafana service  with predefined dashboards that leverage the monitoring service to display monitoring data, such as performance statistics, for application services. You can also define custom Grafana  dashboards for monitoring, visualizing, and understanding data stored in the platform, such as time-series metrics and NoSQL data. You can read and analyze data from the platform's data store and visualize it on Grafana dashboards in the desired formats, such as tables and graphs. This can be done by using the custom iguazio data source, or by using a Prometheus data source for running Prometheus queries on platform TSDB tables. You can also issue data alerts and create, explore, and share dashboards.\nYou can use Iguazio's https://github.com/v3io/grafwiz Python library to create an deploy Grafana dashboards programmatically, as demonstrated in the grafana-grafwiz tutorial. See Also  Platform Services Data Layer  Working with NoSQL Data Working with Data Streams (\u0026quot;Streaming\u0026quot;) Time-Series Databases (TSDB) Getting Started with Data Ingestion Using Spark Using Presto Data-Layer References    ","keywords":["data","ingestion,","data","preparation,","data","pipelines,","data","consumption,","files,","ingest","file,","copy","file,","upload","file,","consume","file,","get","file,","getting","started,","quick-start,","AWS","S3,","S3,","spark","dataframes,","spark","streaming,","spark"],"path":"https://github.com/jasonnIguazio/data-layer/data-ingestion-and-preparation/","title":"Ingesting and Preparing Data"},{"content":" Welcome to the Iguazio Data Science Platform An initial introduction to the Iguazio Data Science platform and the platform tutorials\n Platform Overview Data Science Workflow The Tutorial Notebooks Getting-Started Tutorial End-to-End Use-Case Application and How-To Demos Installing and Updating the MLRun Python Package Data Ingestion and Preparation Additional platform Resources Miscellaneous  \nPlatform Overview The Iguazio Data Science platform (\u0026quot;the platform\u0026quot;) is a fully integrated and secure data science platform as a service (PaaS), which simplifies development, accelerates performance, facilitates collaboration, and addresses operational challenges. The platform incorporates the following components:\n A data science workbench that includes Jupyter Notebook, integrated analytics engines, and Python packages The MLRun open-source MLOps orchestration framework for ML model management with experiments tracking and pipeline automation Managed data and machine-learning (ML) services over a scalable Kubernetes cluster A real-time serverless functions framework for model serving (Nuclio) An extremely fast and secure data layer that supports SQL, NoSQL, time-series databases, files (simple objects), and streaming Integration with third-party data sources such as Amazon S3, HDFS, SQL databases, and streaming or messaging protocols Real-time dashboards based on Grafana  \n\nData Science Workflow The platform provides a complete data science workflow in a single ready-to-use platform that includes all the required building blocks for creating data science applications from research to production:\n Collect, explore, and label data from various real-time or offline sources Run ML training and validation at scale over multiple CPUs and GPUs Deploy models and applications into production with serverless functions Log, monitor, and visualize all your data and services  \nThe Tutorial Notebooks The home directory of the platform's running-user directory (/User/\u0026lt;running user\u0026gt;) contains pre-deployed tutorial Jupyter notebooks with code samples and documentation to assist you in your development — including a demos directory with end-to-end use-case applications (see the next section) and a data-ingestion-and-preparation] directory with documentation and examples for performing data ingestion and preparation tasks.\n Note:\n To view and run the tutorials from the platform, you first need to create a Jupyter Notebook service. The welcome.ipynb notebook and main README.md file provide the same introduction in different formats.   \nGetting-Started Tutorial Start out by running the getting-started tutorial to familiarize yourself with the platform and experience firsthand some of its main capabilities.\n\nYou can also view the tutorial on GitHub.\n\nEnd-to-End Use-Case Application and How-To Demos Iguazio provides full end-to-end use-case application and how-to demos that demonstrate how to use the platform, its MLRun service, and related tools to address data science requirements for different industries and implementations. These demos are available in the MLRun demos repository. Use the provided update-demos.sh script to get updated demos from this repository. By default, the script retrieves the files from the latest release that matches the version of the installed mlrun package (see Installing and Updating the MLRun Python Package). The files are copied to the /v3io/users/\u0026lt;username\u0026gt;/demos directory, where \u0026lt;username\u0026gt; is the name of the running user ($V3IO_USERNAME) unless you set the -u|--user flag to another username.\n Note: Before running the script, close any open files in the demos directory.\n # Get additional demos !/User/update-demos.sh For full usage instructions, run the script with the -h or --help flag:\n!/User/update-demos.sh --help \nEnd-to-End Use-Case Application Demos Demo  Description   News Article Summarization and Keyword Extraction via NLP View on GitHub  Demonstrates how to create an NLP pipeline that will summarize and extract keywords from a news article URL. We will be using state-of-the-art transformer models such as BERT to perform these NLP tasks.    Mask Detection View on GitHub  Demonstrates how to use MLRun to create a mask detection app. We'll train a model that classifies an image of a person as wearing a mask or not, and serve it to an HTTP endpoint.    Fraud Prevention - Iguazio Feature Store View on GitHub  Demonstrates how to develop a fraud prevention pipeline using the Iguazio feature store. The demo creates a full end to end flow starting with creating feature sets,then deploying them as operational feature sets, then train the models, then deploy it as an online serving function, and then close the loop with model monitoring.    \nHow-To Demos Demo  Description   How-To: Converting existing ML code to an MLRun project View on GitHub  Demonstrates how to convert existing ML code to an MLRun project. The demo implements an MLRun project for taxi ride-fare prediction based on a Kaggle notebook with an ML Python script that uses data from the New York City Taxi Fare Prediction competition.    How-To: Running a Spark job for reading a CSV file View on GitHub  Demonstrates how to run a Spark job that reads a CSV file and logs the data set to an MLRun database.    How-To: Running a Spark job for analyzing data View on GitHub  Demonstrates how to create and run a Spark job that generates a profile report from an Apache Spark DataFrame based on pandas profiling.    How-To: Running a Spark Job with Spark Operator View on GitHub  Demonstrates how to use Spark Operator to run a Spark job over Kubernetes with MLRun.    \nInstalling and Updating the MLRun Python Package The demo applications and many of the platform tutorials use MLRun — Iguazio's end-to-end open-source MLOps solution for managing and automating your entire analytics and machine-learning life cycle, from data ingestion through model development to full pipeline deployment in production. MLRun is available in the platform via a default (pre-deployed) shared platform service (mlrun). However, to use MLRun from Python code (such as in the demo and tutorial notebooks), you also need to install the MLRun Python package (mlrun). The version of the installed package must match the version of the platform's MLRun service and must be updated whenever the service's version is updated.\nThe platform provides an align_mlrun.sh script for simplifying the MLRun package installation and version synchronization with the MLRun service. The script is available in the running-user directory (your Jupyter home directory), which is accessible via the /User data mount. Use the following command to run this script for the initial package installation (after creating a new Jupyter Notebook service) and whenever the MLRun service is updated; (the command should be run for each Jupyter Notebook service):\n!/User/align_mlrun.sh \nData Ingestion and Preparation The platform allows storing data in any format. The platform's multi-model data layer and related APIs provide enhanced support for working with NoSQL (\u0026quot;key-value\u0026quot;), time-series, and stream data. Various steps of the data science life cycle (pipeline) might require different tools and frameworks for working with data, especially when it comes to the different mechanisms required during the research and development phase versus the operational production phase. The platform features a wide array of methods for manipulating and managing data, of different formats, in each step of the data life cycle, using a variety of frameworks, tools, and APIs — such as as the following:\n Spark SQL and DataFrames Spark Streaming Presto SQL queries pandas DataFrames Dask V3IO Frames Python library V3IO SDK Web APIs  The Ingesting and Preparing Data tutorial README (data-ingestion-and-preparation/README.ipynb/.md) provides an overview of various methods for collecting, storing, and manipulating data in the platform, and references to sample tutorial notebooks that demonstrate how to use these methods. ▶ Open the README notebook / Markdown file\n\nAdditional platform Resources You can find more information and resources in the MLRun documentation: ▶ View the MLRun documentation\nYou might also find the following resources useful:\n Introduction video In-depth platform overview with a break down of the steps for developing a full data science workflow from development to production platform Services platform data layer, including references nuclio-jupyter SDK for creating and deploying Nuclio functions with Python and Jupyter Notebook  \nMiscellaneous \nCreating Virtual Environments in Jupyter Notebook A virtual environment is a named, isolated, working copy of Python that maintains its own files, directories, and paths so that you can work with specific versions of libraries or Python itself without affecting other Python projects. Virtual environments make it easy to cleanly separate projects and avoid problems with different dependencies and version requirements across components. See the virtual-env tutorial notebook for step-by-step instructions for using conda to create your own Python virtual environments, which will appear as custom kernels in Jupyter Notebook.\n\nUpdating the Tutorial Notebooks You can use the provided igz-tutorials-get.sh script to get updated platform tutorials from the tutorials GitHub repository. By default, the script retrieves the files from the latest release that matches the current platform version. For details, see the update-tutorials.ipynb notebook.\n\nThe v3io Directory The v3io directory that you see in the file browser of the Jupyter UI displays the contents of the v3io data mount for browsing the platform data containers. For information about the platform's data containers and how to reference data in these containers, see platform Data Containers.\n\nSupport The Iguazio support team will be happy to assist with any questions.\n","keywords":["introduction,","overview,","getting","started,","data","science,","data","science","workflow,","data","colleciton,","data","ingestion,","data","exploration,","data","models,","model","training,","model","deployment,","deployment","to","production,","data","visualization,","visualization,","data","analytics,","data","monitoring,","data","logging,","data","pipelines,","ML","pipelines,","automated","pipelines,","worflow","pipelines,","operational","pipelines,","data-engineering","pipelines,","data","science","pipelines,","features,","architecture,","apis,","web","apis,","data,","simple","objects,","data","services,","application","services,","data","management,","streaming,","data","streams,","sql,","nosql,","key","value,","KV,","scala,","python,","nuclio,","v3io","tsdb,","tsdb,","time","series,","v3io","frames,","frames,","presto,","spark,","grafana,","tensorflow,","keras,","pytortch,","pandas,","dask,","matplotlib,","pyplot,","numpy,","nvidia,","nvidia,","data","frames,","dataframes,","aws,","amazon,","operating","system,","web","server,","web","gateway,","nginx,","anaylics,","AI,","artificial","intelligence,","ML,","machine","learning,","mlib,","jupyter,","jupyterlab,","jupyter","notebook,","prometheus,","REST"],"path":"https://github.com/jasonnIguazio/intro/introduction/","title":"Introducing the Platform"},{"content":"This section contains a high-level introduction to the MLOps Platform (version 3.2.1). The introduction includes basic product architecture, terminology, and setup and configuration information, and an overview of the main programming interfaces.\n","keywords":["introduction,","terminology"],"path":"https://github.com/jasonnIguazio/intro/","title":"Introduction"},{"content":"Known Issues Application Services | Backup and Recovery | Dashboard (UI | File System | Frames | Hadoop | High Availability (HA) | Hive | Jupyter | MLRun | Logging and Monitoring Services | Nuclio | Presto | Prometheus | Security | Spark | TSDB | User Management | Web APIs\nManaged Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  V3IO Frames Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   The write client method's \u0026quot;overwriteTable\u0026quot; save mode isn't supported for partitioned tables [Tech Preview].\n  MLRun   The default-project directory (default) is shared for read-only and execution.\n  Beginning with platform version 3.0.0 / MLRun version 0.6.0, project-name restrictions are enforced for MLRun projects both from the dashboard (UI) and the MLRun API. Project names must conform to the RFC 1123 DNS label-name requirements (DNS-1123 label) — 1–63 characters, contain only lowercase alphanumeric characters (a–z, 0–9) or hyphens (-), and begin and end with a lowercase alphanumeric character. Older MLRun projects whose names don't conform to these restrictions will be in a degraded mode and won't have Nuclio functionality (no serverless functions or API gateways). To fix this you need to replace the old projects (you cannot simply rename them). For assistance in upgrading old projects, contact Iguazio's support team.\n  Nuclio Serverless Functions   Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  Time-Series Databases (TSDBs) Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n   Invalid Presto configuration files or connector definitions might cause the Presto service and all dependent services to fail. This is the expected behavior. To recover, delete the problematic configuration or manually revert relevant changes to the default Presto configuration files, save the changes to the Presto service, and select Apply Changes on the dashboard Services page.   In rare cases, after failing to deploy incorrect edits to a default configuration file, it might not be possible to revert the default Presto configuration from the dashboard.\n  Running the Hive CLI from a Jupyter Notebook service when there's no web-shell service in the cluster might fail if another user had previously run the CLI from another instance of Jupyter. To resolve this, ensure that there's at least one web-shell service in the cluster.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Spark Operator   Spark jobs whose names exceed 63 characters might fail because of a Spark bug when running on Kubernetes — see Spark Bug SPARK-24894, which was resolved in Spark version 3.0.0.\n  Web APIs Simple-Object Web API   GET Object and HEAD Object don't return correct ETag and Content-Type metadata.\n  A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.\n  NoSQL Web API   Range-scan requests following recreation of a data container or a container directory might return a 513 error; in such cases, reissue the request.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Logging and Monitoring Services   Data-cluster statistics are not available when the platform is in offline mode.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Security and User Management   Users who are members of a user group with the IT Admin management policy but are not assigned this policy directly cannot access the application-cluster status dashboard. To bypass this issue, assign the IT Admin management policy directly to any user who require such access..\n  Changing the token-expiration period for the authenticator service (id_tokens_expiration configuration) doesn't affect existing tokens. For further assistance, contact Iguazio's support team.\n  Backup, Recovery, and High Availability (HA)   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Dashboard (UI)  NoteSee the MLRun and Nuclio known issues for dashboard issues related specifically to these frameworks.  Notes   Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["known","issues"],"path":"https://github.com/jasonnIguazio/release-notes/known_issues/","title":"Known Issues"},{"content":"Description Returns information about all containers that are visible to the user who sent the request, according to its tenant (default), or about a specific container (see the \u0026lt;container\u0026gt; URL resource parameter).\nRequest Request Header Syntax  GET /api/containers/[\u0026lt;container\u0026gt;] HTTP/1.1 Host: \u0026lt;management-APIs URL\u0026gt; Content-Type: application/json Cookie: session=\u0026lt;cookie\u0026gt;  url = \u0026#34;\u0026lt;management-APIs URL\u0026gt;/api/containers/[\u0026lt;container\u0026gt;]\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;session=\u0026lt;cookie\u0026gt;\u0026#34; }    HTTP Method GET\nURL Resource Parameters  -- \u0026lt;container\u0026gt; The ID of a specific container for which to retrieve information.\n Type: Integer   Requirement: Optional   Default Behavior: By default, when the \u0026lt;container\u0026gt; parameter is not set, the operation returns information for all containers of the user's tenant.    Request DataNone\nResponse Response Header Syntax HTTP/2.1 \u0026lt;status code; 200 on success\u0026gt; \u0026lt;reason phrase\u0026gt; Content-Type: application/json ... Response Data Syntax  { \u0026#34;data\u0026#34;: [ { \u0026#34;attributes\u0026#34;: { \u0026#34;admin_status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;cost\u0026#34;: number, \u0026#34;created_at\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;id\u0026#34;: number, \u0026#34;imports\u0026#34;: [], \u0026#34;mapping\u0026#34;: { \u0026#34;cmds_index\u0026#34;: number, \u0026#34;container_id\u0026#34;: number, \u0026#34;mds_instances\u0026#34;: [ { \u0026#34;service_context\u0026#34;: { \u0026#34;internal_xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;rest_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;shared_memory_objects\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;shutdown\u0026#34;: { \u0026#34;phase\u0026#34;: number, \u0026#34;timeout\u0026#34;: number }, \u0026#34;version_info\u0026#34;: { \u0026#34;external\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;git\u0026#34;: \u0026#34;string \u0026#34;, \u0026#34;offline\u0026#34;: \u0026#34;string \u0026#34; }, \u0026#34;xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ] }, \u0026#34;service_id\u0026#34;: { \u0026#34;node_name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;service_instance\u0026#34;: number, \u0026#34;service_name\u0026#34;: \u0026#34;string\u0026#34; } } ], \u0026#34;num_slices\u0026#34;: 0 }, \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;properties\u0026#34;: [], \u0026#34;updated_at\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;id\u0026#34;: number, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; } ], ... }  { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;admin_status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;cost\u0026#34;: number, \u0026#34;created_at\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;id\u0026#34;: number, \u0026#34;imports\u0026#34;: [], \u0026#34;mapping\u0026#34;: { \u0026#34;cmds_index\u0026#34;: number, \u0026#34;container_id\u0026#34;: number, \u0026#34;mds_instances\u0026#34;: [ { \u0026#34;service_context\u0026#34;: { \u0026#34;internal_xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;rest_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;shared_memory_objects\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;shutdown\u0026#34;: { \u0026#34;phase\u0026#34;: number, \u0026#34;timeout\u0026#34;: number }, \u0026#34;version_info\u0026#34;: { \u0026#34;external\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;git\u0026#34;: \u0026#34;string \u0026#34;, \u0026#34;offline\u0026#34;: \u0026#34;string \u0026#34; }, \u0026#34;xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;string\u0026#34; } ] }, \u0026#34;service_id\u0026#34;: { \u0026#34;node_name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;service_instance\u0026#34;: number, \u0026#34;service_name\u0026#34;: \u0026#34;string\u0026#34; } } ], \u0026#34;num_slices\u0026#34;: 0 }, \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;properties\u0026#34;: [], \u0026#34;updated_at\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;id\u0026#34;: number, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; }, ... }    Elements The data object in the HTTP response body contains an array of container-information JSON objects (default) or a single object (for a specific-container request). The returned container information includes the container's name (name), ID (id), creation time (created_at), relevant addresses, and version information (version_info). Full the full list of returned data elements, see response-data syntax above.\nExamples Example 1 Return information about all visible containers for the user's tenant:\nRequest  GET /api/containers HTTP/1.1 Host: https://dashboard.default-tenant.app.mycluster.iguazio.com Content-Type: application/json Cookie: session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D  import requests url = \u0026#34;https://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers\u0026#34; headers = {\u0026#34;Cookie\u0026#34;: \u0026#34;session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D\u0026#34;} response = requests.get(url, headers=headers) print(response.text)    Response The response includes an array with a single container-information object:\nHTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;data\u0026#34;: [ { \u0026#34;attributes\u0026#34;: { \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;cost\u0026#34;: 0.976715087890625, \u0026#34;created_at\u0026#34;: \u0026#34;2021-01-15T08:17:19.904000+00:00\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;id\u0026#34;: 1028, \u0026#34;imports\u0026#34;: [], \u0026#34;mapping\u0026#34;: { \u0026#34;cmds_index\u0026#34;: 0, \u0026#34;container_id\u0026#34;: 0, \u0026#34;mds_instances\u0026#34;: [ { \u0026#34;service_context\u0026#34;: { \u0026#34;internal_xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp:// 10.0.0.1:5000\u0026#34; } ], \u0026#34;rest_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;10.0.0.1:8154\u0026#34; } ], \u0026#34;shared_memory_objects\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_metadata\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsMetadata\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_values\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsValues\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;place_holder_for_log_shm\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;log\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_names\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsMetricNames\u0026#34; } ], \u0026#34;shutdown\u0026#34;: { \u0026#34;phase\u0026#34;: 0, \u0026#34;timeout\u0026#34;: 0 }, \u0026#34;version_info\u0026#34;: { \u0026#34;external\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;git\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;offline\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp:// 10.0.0.1:5001\u0026#34; } ] }, \u0026#34;service_id\u0026#34;: { \u0026#34;node_name\u0026#34;: \u0026#34;igz0\u0026#34;, \u0026#34;service_instance\u0026#34;: 1, \u0026#34;service_name\u0026#34;: \u0026#34;mds\u0026#34; } } ], \u0026#34;num_slices\u0026#34;: 0 }, \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;properties\u0026#34;: [], \u0026#34;updated_at\u0026#34;: \u0026#34;2021-01-30T14:30:00.367000+00:00\u0026#34; }, \u0026#34;id\u0026#34;: 1028, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; } ], ... } Example 2 Return information about a specific container with ID 1028:\nRequest  GET /api/containers/1028 HTTP/1.1 Host: https://dashboard.default-tenant.app.mycluster.iguazio.com Content-Type: application/json Cookie: session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D  import requests url = \u0026#34;https://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers/1028\u0026#34; headers = {\u0026#34;Cookie\u0026#34;: \u0026#34;session=j%3A%7B%22sid%22%3A%20%22a9ce242a-670f-47a8-9c8b-c6730f2794dc%22%7D\u0026#34;} response = requests.get(url, headers=headers) print(response.text)    Response The response includes a single container-information object for the requested container (ID = 1028):\nHTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;cost\u0026#34;: 0.976715087890625, \u0026#34;created_at\u0026#34;: \u0026#34;2021-01-15T08:17:19.904000+00:00\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;id\u0026#34;: 1028, \u0026#34;imports\u0026#34;: [], \u0026#34;mapping\u0026#34;: { \u0026#34;cmds_index\u0026#34;: 0, \u0026#34;container_id\u0026#34;: 0, \u0026#34;mds_instances\u0026#34;: [ { \u0026#34;service_context\u0026#34;: { \u0026#34;internal_xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp://10.0.0.1:5000\u0026#34; } ], \u0026#34;rest_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;10.0.0.1:8154\u0026#34; } ], \u0026#34;shared_memory_objects\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_metadata\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsMetadata\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_values\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsValues\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;place_holder_for_log_shm\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;log\u0026#34; }, { \u0026#34;address\u0026#34;: \u0026#34;/dev/shm/mds.1_stats_names\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;statsMetricNames\u0026#34; } ], \u0026#34;shutdown\u0026#34;: { \u0026#34;phase\u0026#34;: 0, \u0026#34;timeout\u0026#34;: 0 }, \u0026#34;version_info\u0026#34;: { \u0026#34;external\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;git\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;offline\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;xio_addresses\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp://10.0.0.1:5001\u0026#34; } ] }, \u0026#34;service_id\u0026#34;: { \u0026#34;node_name\u0026#34;: \u0026#34;igz0\u0026#34;, \u0026#34;service_instance\u0026#34;: 1, \u0026#34;service_name\u0026#34;: \u0026#34;mds\u0026#34; } } ], \u0026#34;num_slices\u0026#34;: 0 }, \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;properties\u0026#34;: [], \u0026#34;updated_at\u0026#34;: \u0026#34;2021-01-30T14:30:00.367000+00:00\u0026#34; }, \u0026#34;id\u0026#34;: 1028, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; }, ... } ","keywords":["List","Containers,","management,","containers,","http","GET,","GET","method,","GET"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/containers-api/list-containers/","title":"List Containers"},{"content":" The platform features a default (pre-deployed) shared single-instance tenant-wide log-forwarder service (log-forwarder) for forwarding application-service logs to an instance of the Elasticsearch open-source search and analytics engine by using the open-source Filebeat log-shipper utility.\nIn addition, the platform has a default (pre-deployed) shared single-instance tenant-wide monitoring service (monitoring) for monitoring Nuclio serverless functions and gathering performance statistics and additional data. The platform visualizes data gathered by the monitoring service in predefined Grafana dashboards. For more information, see Data Monitoring and Visualization Services and Tools.\nFor detailed information about these services and how to use them, as well as information about additional platform monitoring, logging, and debugging tools, see Logging, Monitoring, and Debugging and Monitoring Platform Services.\nSee Also  Working with Services Logging, Monitoring, and Debugging Monitoring Platform Services Adding Grafana Dashboards  ","keywords":["logging","and","monitoring","services,","logging","services,","applciation","logging,","logging,","monitoring","services,","monitoring","tools,","application","monitoring,","data","monitoring,","monitoring,","grafana,","debugging","tools,","application","debugging,","debugging,","log","forwarder,","log","forwarding,","filebeat,","elasticsearch,","monitoring","service"],"path":"https://github.com/jasonnIguazio/services/app-services/logging-and-monitoring-services/","title":"Logging and Monitoring Services"},{"content":" Overview There are a variety of ways in which you can log and debug the execution of platform application services, tools, and APIs.\nNoteTo learn how to use the platform's default monitoring service and pre-deployed Grafana dashboards to monitor application services, see Monitoring Platform Services.   Logging application services (Log forwarder and Elasticsearch) Kubernetes tools Event logs Cluster support logs API error information  For further troubleshooting assistance, visit Iguazio Support. Logging Application Services The platform has a default tenant-wide log-forwarder application service (log-forwarder) for forwarding application-service logs. The logs are forwarded to an instance of the Elasticsearch open-source search and analytics engine by using the open-source Filebeat log-shipper utility. The log-forwarder service is disabled by default. To enable it, on the Services dashboard page, select to edit the log-forwarder service; in the Custom Parameters tab, set the Elasticsearch URL parameter to an instance of Elasticsearch that will be used to store and index the logs; then, save and apply your changes to deploy the service.\nTypically, the log-forwarder service should be configured to work with your own remote off-cluster instance of Elasticsearch. Note The default transfer protocol, which is used when the URL doesn't begin with \u0026quot;http://\u0026quot; or \u0026quot;https://\u0026quot;, is HTTPS. The log-forwarder service doesn't support off-cluster Elasticsearch user authentication, regardless of whether you use an HTTP or HTTPS URL. The default port, which is used when the URL doesn't end with \u0026quot;:\u0026lt;port number\u0026gt;\u0026quot;, is port 80 for HTTP and port 443 for HTTPS.    In cloud platform environments you can also configure the log-forwarder service to work with a tenant-wide Elasticsearch service that uses a pre-deployed on-cluster vanilla installation of Elasticsearch. To do so, you first need to create such an Elasticsearch service: from the Services dashboard page, select to create a new service of type \u0026quot;Elasticsearch\u0026quot;, optionally edit the default configuration, and apply your changes to deploy the service. You can then configure the log-forwarder service to forward the application-service logs to this service by selecting your Elasticsearch service from the drop-down list of the Elasticsearch parameter. After enabling the log-forwarder service, you can view the forwarded logs in the Logs page of the platform dashboard (including filtering options), provided you have the Service Admin management policy. The Logs column on the Services dashboard page has links to filtered log views for each service. NoteNote that the Logs page displays all logs from the configured log-forwarder Elasticsearch instance, regardless of their origin. It's therefore recommended that you use a dedicated Elasticsearch instance for each platform cluster that enables log forwarding.  WarningNote that you cannot delete an Elasticsearch service while it's being used by the log-forwarder service.  Kubernetes Tools You can use the Kubernetes kubectl CLI from a platform web-shell or Jupyter Notebook application service to monitor, log, and debug your Kubernetes application cluster:\n Use the get pods command to display information about the cluster's pods: kubectl get pods  Use the logs command to view the logs for a specific pod; replace POD with the name of one of the pods returned by the get command: kubectl logs POD  Use the top pod command to view pod resource metrics and monitor resource consumption; replace [POD] with the name of one of the pods returned by the get command or remove it to display logs for all pods: kubectl top pod [POD]   NoteTo run kubectl commands from a web-shell service, the service must be configured with an appropriate service account; for more information about the web-shell service accounts, see The Web-Shell Service.\n The get pods and logs commands require the \u0026quot;Log Reader\u0026quot; service account or higher. The top pod command requires the \u0026quot;Service Admin\u0026quot; service account.    For more information about the kubectl CLI, see the Kubernetes documentation.\n\nIGZTOP - Performance Reporting Tool IGZTOP is a small tool which is intended to display useful information about pods in the default-tenant namespace. It is a performance troubleshooting tool providing insights on how your MLrun jobs are consuming resources in the cluster. and enhances the standard set of provided Kubernetes tools.\nPrerequisites To use the IGZTOP feature you will need the to have a shell service configured with service admin rights. To setup a new service see Creating a New Service. To run the shell service:\n In the side navigation menu, select Services. On the Services page, press the Shell Service name (link) from the list of services. A shell window opens.  How to Run IGZTOP Use the following command in the shell service window. Use the flags (options) to customize your results.\nigztop [--sort=\u0026lt;KEY\u0026gt;] [--limit=\u0026lt;KEY\u0026gt;] [--wide] [--all] [--kill-job=\u0026lt;JOB\u0026gt;] [--no-borders] [--check-permissions] [--update] Options Description    -h --help\nDisplays the command help    -v --version\n Displays the current version    -s --sort=\u0026lt;KEY\u0026gt;\nSort by key (for example --sort memory)    -l --limit=\u0026lt;KEY\u0026gt;\nfilter using a key:value pair (for example 'node=k8s-node1', 'name=presto')    -w --wide\nShow additional fields including “requests” and “limits”    -a --all\nInclude pods which are not currently running    -b --no-borders\nPrint the table without borders    -k --kill-job=\u0026lt;JOB\u0026gt;\nKill one or more MLRun job pods (use * to kill multiple matching pods    -c --check-permissions\nCheck the permissions of the user running this Service    -u --update\nFetch the latest version of IGZTOP   Examples The following is an example of the default output.\n$ igztop +--------------------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | NAME | CPU(m) | MEMORY(Mi) | NODE | STATUS | GPUs | GPU Util. | MLRun Proj. | MLRun Owner | +--------------------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | authenticator-fb7dc6df7-vg9tl | 1 | 6 | k8s-node2 | Running | | | | | | docker-registry-676df9d6f6-x788h | 1 | 6 | k8s-node2 | Running | | | | | | framesd-8b4c489f-sx9rw | 1 | 17 | k8s-node1 | Running | | | | | | grafana-5b456dc59-vjszb | 1 | 31 | k8s-node1 | Running | | | | | | jupyter-58c5bf598f-z86qm | 9 | 641 | k8s-node1 | Running | 1/1 | 0.00% | | | | metadata-envoy-deployment-786b65df7d-lldp2 | 3 | 11 | k8s-node2 | Running | | | | | | metadata-grpc-deployment-7cfc8d9b8-prdr8 | 1 | 1 | k8s-node2 | Running | | | | | | metadata-writer-7bfd6bf6dd-x8fj9 | 1 | 140 | k8s-node2 | Running | | | | | | metrics-server-exporter-75b49dd6b8-k9r5q | 5 | 14 | k8s-node2 | Running | | | | | | ml-pipeline-565cb8497d-wkx2b | 5 | 16 | k8s-node2 | Running | | | | | | ml-pipeline-persistenceagent-746949f667-822ch | 1 | 9 | k8s-node1 | Running | | | | | | ml-pipeline-scheduledworkflow-6fd754dfcf-slrtk | 1 | 9 | k8s-node1 | Running | | | | | | ml-pipeline-ui-7694dc85ff-4cjc5 | 4 | 52 | k8s-node1 | Running | | | | | | ml-pipeline-viewer-crd-6fbf9f8fbf-9cwvq | 2 | 10 | k8s-node2 | Running | | | | | | ml-pipeline-visualizationserver-66ccfd8f98-l6g86 | 5 | 80 | k8s-node1 | Running | | | | | | mlrun-api-68cfd974db-cgph4 | 4 | 317 | k8s-node2 | Running | | | | | | mlrun-ui-88cbcffc4-8s76d | 0 | 11 | k8s-node1 | Running | | | | | | monitoring-prometheus-server-78d5bbc9d4-vgrhm | 5 | 42 | k8s-node2 | Running | | | | | | mpi-operator-7589fbff58-87xr7 | 1 | 13 | k8s-node1 | Running | | | | | | mysql-kf-67b6cb589d-dvh94 | 4 | 460 | k8s-node1 | Running | | | | | | nuclio-controller-5bcbd8bcf6-6hqvv | 1 | 10 | k8s-node2 | Running | | | | | | nuclio-dashboard-84ddf6f9bd-c4scb | 1 | 43 | k8s-node1 | Running | | | | | | nuclio-dlx-86cbc4cdb9-wcds2 | 1 | 5 | k8s-node2 | Running | | | | | | nuclio-scaler-7b96584d57-l9cwj | 1 | 10 | k8s-node1 | Running | | | | | | oauth2-proxy-5bd48c96d8-ghvqq | 1 | 4 | k8s-node2 | Running | | | | | | presto-coordinator-65d6d64b5f-vc7gh | 17 | 1319 | k8s-node1 | Running | | | | | | presto-worker-76d774b948-dxxjr | 8 | 1229 | k8s-node2 | Running | | | | | | presto-worker-76d774b948-vlxcn | 13 | 1236 | k8s-node1 | Running | | | | | | provazio-controller-controller-c9bff698c-xk6r9 | 3 | 50 | k8s-node2 | Running | | | | | | shell-7b47994578-z7s6k | 50 | 108 | k8s-node2 | Running | | | | | | spark-operator-658bdfb6f5-bl2lb | 2 | 10 | k8s-node1 | Running | | | | | | v3io-webapi-b9b9f | 8 | 1147 | k8s-node1 | Running | | | | | | v3io-webapi-p4kbq | 8 | 1431 | k8s-node2 | Running | | | | | | v3iod-fztqm | 13 | 2541 | k8s-node2 | Running | | | | | | v3iod-jg68d | 14 | 2536 | k8s-node1 | Running | | | | | | v3iod-locator-599b9d5c9f-76vrr | 0 | 5 | k8s-node1 | Running | | | | | | workflow-controller-7d59d94444-5n7r7 | 1 | 12 | k8s-node2 | Running | | | | | +--------------------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | SUM | 197m | 13582Mi | | | | | | | +--------------------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ The following examples compares Presto memory usage across all cluster nodes. $ igztop --sort memory --limit name=presto +-------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | NAME | CPU(m) | MEMORY(Mi) | NODE | STATUS | GPUs | GPU Util. | MLRun Proj. | MLRun Owner | +-------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | presto-coordinator-65d6d64b5f-vc7gh | 17 | 1320 | k8s-node1 | Running | | | | | | presto-worker-76d774b948-vlxcn | 12 | 1237 | k8s-node1 | Running | | | | | | presto-worker-76d774b948-dxxjr | 12 | 1230 | k8s-node2 | Running | | | | | +-------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | SUM | 41m | 3787Mi | | | | | | | +-------------------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ The following example displays which pods are using GPUs.\n$ igztop --sort memory --limit gpu +--------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | NAME | CPU(m) | MEMORY(Mi) | NODE | STATUS | GPUs | GPU Util. | MLRun Proj. | MLRun Owner | +--------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | jupyter-58c5bf598f-z86qm | 10 | 644 | k8s-node1 | Running | 1/1 | 0.00% | | | +--------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ | SUM | 10m | 644Mi | | | | | | | +--------------------------+--------+------------+-----------+---------+------+-----------+-------------+-------------+ The following example displays the --all flag and includes pods which are not running.\n$ igztop -a +---------------------------------------------------------------+--------+------------+-----------+-----------+--------------------------------+-------------+ | NAME | CPU(m) | MEMORY(Mi) | NODE | STATUS | MLRun Proj. | MLRun Owner | +---------------------------------------------------------------+--------+------------+-----------+-----------+--------------------------------+-------------+ | v3io-webapi-dcknw | 19 | 5069 | k8s-node1 | Running | | | | presto-coordinator-68df99bdf9-8swbp | 20 | 1855 | k8s-node1 | Running | | | | ml-pipeline-viewer-crd-656775dc5c-n4tzl | 2 | 11 | k8s-node1 | Running | | | | prep-data-5vpsb | | | k8s-node1 | Completed | getting-started-tutorial-admin | admin | | nuclio-dlx-555d6c4cc5-64mns | 1 | 5 | k8s-node1 | Running | | | | workflow-controller-64fdf54f56-8wq2v | 1 | 11 | k8s-node1 | Running | | | | zeppelin-68fdc8bdb5-fnc5v | 58 | 685 | k8s-node1 | Running | | | | shell-service-admin-6f4bb85cc8-5d9v7 | 56 | 89 | k8s-node1 | Running | | | | nuclio-dashboard-5bcff7dc4c-b7r4j | 7 | 43 | k8s-node1 | Running | | | | prep-data-fblfn | | | k8s-node1 | Completed | getting-started-tutorial-admin | admin | | v3iod-x5fmt | 16 | 5080 | k8s-node1 | Running | | | | prep-data-77k4m | | | k8s-node1 | Completed | getting-started-tutorial-admin | admin | | spark-m9zrumpebc-rdxs6-worker-8647d4466f-rjk9h | 9 | 302 | k8s-node1 | Running | | | | metadata-grpc-deployment-6475f89f9b-t2gns | 1 | 1 | k8s-node1 | Running | | | | metadata-writer-85ddb586d-wzz8v | 1 | 140 | k8s-node1 | Running | | | | spark-m9zrumpebc-rdxs6-master-5fd5d6964b-bk527 | 8 | 361 | k8s-node1 | Running | | | | nuclio-controller-dd6fcbc74-4pt82 | 1 | 13 | k8s-node1 | Running | | | | spark-operator-7f698b78dc-8x2sh | 2 | 10 | k8s-node1 | Running | | | | prep-data-g6dps | | | k8s-node1 | Completed | getting-started-tutorial-admin | admin | | mpi-operator-68dbcc88cc-z8nls | 1 | 14 | k8s-node1 | Running | | | | spark-operator-init-zxp8r | | | k8s-node1 | Completed | | | | mlrun-ui-657cb7f889-rpn7t | 0 | 11 | k8s-node1 | Running | | | | prep-data-bgwg2 | | | k8s-node1 | Completed | getting-started-tutorial-admin | admin | | metadata-envoy-deployment-5f6c59895d-59qk9 | 4 | 11 | k8s-node1 | Running | | | | mysql-kf-77b69d9d9-4wwkc | 3 | 463 | k8s-node1 | Running | | | | framesd-7b89dc9cfb-t4p7x | 1 | 17 | k8s-node1 | Running | | | | ml-pipeline-86dc8d885-z6zpm | 5 | 16 | k8s-node1 | Running | | | | spark-master-7cf99697df-9dprz | 7 | 408 | k8s-node1 | Running | | | | ml-pipeline-visualizationserver-78ccd99c4b-zqwzb | 4 | 76 | k8s-node1 | Running | | | | ml-pipeline-scheduledworkflow-868c7467f5-6jxst | 1 | 8 | k8s-node1 | Running | | | | metrics-server-exporter-594f9c9c97-hvnsd | 8 | 12 | k8s-node1 | Running | | | | mlrun-api-5dbb8899cb-5zwh5 | 28 | 403 | k8s-node1 | Running | | | | shell-none-6d64c8675c-tndv4 | 5 | 66 | k8s-node1 | Running | | | | provazio-controller-controller-77866b6b98-n7psm | 4 | 50 | k8s-node1 | Running | | | | monitoring-prometheus-server-5c959fc895-hfqg2 | 6 | 43 | k8s-node1 | Running | | | | v3iod-locator-597b4c7c87-jn2gx | 0 | 5 | k8s-node1 | Running | | | | spark-worker-664878f4b6-mwq2r | 8 | 404 | k8s-node1 | Running | | | | prep-data-pq8jm | | | k8s-node1 | Completed | getting-started-tutorial-admin | admin | | docker-registry-6d8b657f8d-h5297 | 1 | 45 | k8s-node1 | Running | | | | hive-5bfbc84589-5l9rg | 7 | 284 | k8s-node1 | Running | | | | ml-pipeline-persistenceagent-56f77bdb5f-vn9ms | 1 | 9 | k8s-node1 | Running | | | | shell-app-admin-57474fc864-dkzct | 5 | 95 | k8s-node1 | Running | | | | presto-worker-758dcccf6f-hst2f | 12 | 1809 | k8s-node1 | Running | | | | jupyter-77bfdd798d-vbt6r | 9 | 585 | k8s-node1 | Running | | | | mariadb-74d5c8cb74-zjq82 | 4 | 76 | k8s-node1 | Running | | | | shell-log-reader-7d67d7f567-vbldj | 5 | 52 | k8s-node1 | Running | | | | ml-pipeline-ui-7db966bb54-znch6 | 4 | 46 | k8s-node1 | Running | | | | oauth2-proxy-85b8c78fb7-8ktzx | 1 | 3 | k8s-node1 | Running | | | | nuclio-getting-started-tutorial-admin-serving-6d8fbdc9d-r4x2w | 1 | 165 | k8s-node1 | Running | | | | authenticator-6647dd8467-r2g9n | 1 | 7 | k8s-node1 | Running | | | | nuclio-scaler-78cc778b6c-kr7m9 | 1 | 10 | k8s-node1 | Running | | | +---------------------------------------------------------------+--------+------------+-----------+-----------+--------------------------------+-------------+ | SUM | 339m | 18868Mi | | | | | +---------------------------------------------------------------+--------+------------+-----------+-----------+--------------------------------+-------------+ To delete MLRun use igztop with the -k --kill-job option followed by a job name. Use '*' as a wildcard at the start or end of a job name to kill multiple job pods.\n$ igztop -k \u0026#39;*data*\u0026#39; pod \u0026#34;prep-data-5vpsb\u0026#34; deleted pod \u0026#34;prep-data-77k4m\u0026#34; deleted pod \u0026#34;prep-data-bgwg2\u0026#34; deleted pod \u0026#34;prep-data-fblfn\u0026#34; deleted pod \u0026#34;prep-data-g6dps\u0026#34; deleted pod \u0026#34;prep-data-pq8jm\u0026#34; deleted Event Logs The Events page of the dashboard displays different platform event logs:\n The Event Log tab displays system event logs. The Alerts tab displays system alerts. The Audit tab displays a subset of the system events for audit purposes — security events (such as a failed login) and user actions (such as creation and deletion of a container).  The Events page is visible to users with the IT Admin management policy — who can view all event logs — or to users with the Security Admin management policy — who can view only the Audit tab.\nCluster Support Logs Users with the IT Admin management policy can collect and download support logs for the platform clusters from the dashboard. Log collection is triggered for a data cluster, but the logs are collected from both the data and application cluster nodes.\nYou can trigger collection of cluster support-logs from the dashboard in one of two ways; (note that you cannot run multiple collection jobs concurrently):\n On the Clusters page, open the action menu () for a data cluster in the clusters table (Type = \u0026quot;Data\u0026quot;); then, select the Collect logs menu option. On the Clusters page, select to display the Support Logs tab for a specific data cluster — either by selecting the Support logs option from the cluster's action menu () or by selecting the data cluster and then selecting the Support Logs tab; then, select Collect Logs from the action toolbar.  You can view the status of all collection jobs and download archive files of the collected logs from the data-cluster's Support Logs dashboard tab.\nAPI Error Information The platform APIs return error codes and error and warning messages to help you debug problems with your application. See, for example, the Error Information documentation in the Data-Service Web-API General Structure reference documentation.\nSee Also  Logging and Monitoring Services Monitoring Platform Services  ","keywords":["logging","and","monitoring","services,","logging","services,","applciation","logging,","logging,","monitoring","services,","monitoring","tools,","application","monitoring,","data","monitoring,","monitoring,","grafana,","debugging","tools,","application","debugging,","debugging,","log","forwarder,","log","forwarding,","filebeat,","elasticsearch,","monitoring","service,","kubectl,","kubernetes","logging,","events,","event","logs,","api","error","information,","jupyter,","jupyter","notebook,","web","shell"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/logging-n-debugging/","title":"Logging, Monitoring, and Debugging"},{"content":"The following binary logical operators are supported in expressions. The operands are Boolean.\nNote Use the literal operator names — AND, OR, or NOT — and not the operator signs that are commonly associated with these operators. The names of the logical expression operators are reserved names in the platform. For more information, see Reserved Names.    Binary Logical Operators Logical AND Operator OPERAND-A AND OPERAND-B The logical AND operator returns true if both operands are true; otherwise, returns false.\nLogical OR Operator OPERAND-A OR OPERAND-B The logical OR operator returns true if either of the operands is true; otherwise, returns false.\nUnary Logical Operators Logical Negation (NOT) Operator NOT OPERAND The logical negation operator (NOT) negates the value of the operand: if the operand evaluates to true, the operator returns false; if the operand evaluates to false, the operator returns true.\n","keywords":["expression","logical","operators,","logical","operators,","expression","operators,","attribute","variablese,","binarly","logical","operators,","binary","operators,","logical","AND","operator,","AND","expression","operator,","AND","operator,","logical","OR","operator,","OR","operator,","unary","logical","operators,","unary","operators,","logical","negation","operator,","negation","operator,","logical","NOT","operator,","NOT","operator"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/operators/logical-operators/","title":"Logical Operators"},{"content":"The platform's cluster-management REST APIs (\u0026quot;the management APIs\u0026quot;) provide an alternative to using the dashboard to perform management operations — such as containers management, system administration, and security operations (including management of users, groups, tenants, and data-access polices). The management-APIs are exposed as /api/\u0026lt;API ID\u0026gt; endpoints (for example, /api/containers).\nThis section describes the general structure of management-API requests and responses, and provides a reference of the following management APIs:\n  Sessions API for supporting user authentication and authorization.\n  Containers API for creating and managing containers.\n  Cluster-Information API for retrieving cluster information.\n  There are also additional management APIs that are currently undocumented. For more information, contact your Iguazio representative.\n Beta NoteThe management APIs are provided as a beta feature and are not officially supported in this release.  ","keywords":["management","apis,","beta,","management,","REST,","RESTful,","management-api","structure,","api","structure,","api","endpoints,","/api,","/api/containers,","security,","user","management,","users,","user","groups,","containers,","containers","mangement,","sessions,","session","management,","authentication,","authorization,","data-access","policies,","tenants,","system","administration,","administration,","cluster","management,","cluster","information,","http,","python,","sessions","management","api,","containers","management","api,","cluster-information","management","api"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/overview/","title":"Overview of the Cluster-Management REST APIs"},{"content":" Browse reference documentation for the MLOps Platform's cluster-management REST APIs (\u0026quot;the management APIs\u0026quot;) for performing administrative management operations such as containers, users, and security management. Note that currently the reference doesn't cover all the management APIs. ","keywords":["clutster-management","REST","apis","reference,","management","REST","apis","reference,","api","reference,","cluster-management","REST","apis,","management","REST","apis,","cluster-management","apis,","mangement","apis,","cluster","management,","management,","beta,","REST","apis,","REST,","RESTful,","http","apis,","http,","https,","python"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/","title":"Cluster-Management REST APIs Reference"},{"content":" Note  min and max are supported in update expressions.\n  Regarding the use of the min and max functions with arrays, note that in the current release this is restricted to the web APIs.\n    max The max function supports the following variations:\n Simple-values comparison Array-elements comparison  Compare Simple Values max(a, b) When max receives two parameters, it returns the highest (maximal) parameter value. The function supports comparison of Boolean, numeric, and string values. The values can also be provided as expressions (for example, attr1+attr1 or attr1==attr2).\nFor example, given attr1, attr2, and attr3 attributes with the respective values 5, 1, and 6, \u0026quot;max(attr1, attr2+attr3)\u0026quot; returns the sum of attr2 and attr3 — 7.\nNote Strings are compared using lexicographic string comparison. Numeric values of different data types are compared using numeric promotion. The type of the return value is that of the largest data type from among the compared values. For example, \u0026quot;max(1.0, 9) returns 9.0 (double). Boolean values are implicitly converted to numbers to allow comparison — true = 1 and false = 0. However, the data type of the function's return value in the case of a Boolean operand is the winning data type — i.e., the original data type of the returned value. For example, \u0026quot;max(1==1, 1==2)\u0026quot; and \u0026quot;max(1==1, 0)\u0026quot; both return true (Boolean), but \u0026quot;max(1==1, 3)\u0026quot; returns 3 (integer).    Compare Array Elements max(ARRAY) When max receives a single array or array-slice parameter (ARRAY), it returns the highest (maximal) value from among the values of all elements of the specified array or array slice. The parameter can be the name of an array attribute (ARRAY-ATTRIBUTE), or a slice of an array attribute that indicates the zero-based index range of the array elements to compare (ARRAY-ATTRIBUTE[istart..iend]).\nFor example, for a myArray integer-array attribute with the values 7, 13, 5, and 1, \u0026quot;max(myArray)\u0026quot; returns 13 and \u0026quot;max(myArray[2..3])\u0026quot; returns 5.\nmin The min function supports the following variations:\n Simple-values comparison Array-elements comparison  Compare Simple Values min(a, b) When min receives two parameters, it returns the lowest (minimal) parameter value. The function supports comparison of Boolean, numeric, and string values. The values can also be provided as expressions (for example, attr1+attr1 or attr1==attr2).\nFor example, given attr1, attr2, and attr3 attributes with the respective values 5, 1, and 6, \u0026quot;min(attr1, attr2+attr3)\u0026quot; returns the value of attr1 — 5.\nNote Strings are compared using lexicographic string comparison. Numeric values of different data types are compared using numeric promotion. The type of the return value is that of the largest data type from among the compared values. For example, \u0026quot;min(1, 9.0) returns 1.0 (double). Boolean values are implicitly converted to numbers to allow comparison — true = 1 and false = 0. However, the data type of the function's return value in the case of a Boolean operand is the winning data type — i.e., the original data type of the returned value. For example, \u0026quot;min(1==1, 1==2)\u0026quot; and \u0026quot;min(1==2, 1)\u0026quot; both return false (Boolean), but \u0026quot;min(1==1, 0)\u0026quot; returns 0 (integer).    Compare Array Elements min(ARRAY) When min receives a single array or array-slice parameter (ARRAY), it returns the lowest (minimal) value from among the values of all elements of the specified array or array slice. The parameter can be the name of an array attribute (ARRAY-ATTRIBUTE), or a slice of an array attribute that indicates the zero-based index range of the array elements to compare (ARRAY-ATTRIBUTE[istart..iend]).\nFor example, for a myArray integer-array attribute with the values 7, 13, 5, and 1, \u0026quot;min(myArray)\u0026quot; returns 1 and \u0026quot;min(myArray[1..2])\u0026quot; returns 5.\nSee Also  Update Expression Attribute Variables  ","keywords":null,"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/functions/min-max-functions/","title":"Maximal-/Minimal-Value Expression Functions"},{"content":" The Monitoring Application Service and Grafana Dashboards The platform has a default (pre-deployed) tenant-wide monitoring application service (monitoring) for monitoring application services and gathering performance statistics and additional data. This service is enabled by default.\nAll Grafana user services in the platform that are created or restarted after the monitoring service is enabled have a private dashboards directory that contains the following predefined dashboards. The dashboards visualize data gathered by the monitoring service for application services of the parent tenant:\n Application Services Monitoring — displays information for all the managed application services. Nuclio Functions Monitoring - Overview — displays information for all the deployed serverless Nuclio functions. Nuclio Functions Monitoring - Per Function — displays information for a specific Nuclio function, as set in the dashboard filter.  The name of the monitoring service on the Services platform dashboard page (monitoring) links to the UI of the related Prometheus service.\nThe platform also has a shared single-instance tenant-wide application-cluster Grafana service with monitoring dashboards for the entire Kubernetes application cluster (rather than a specific Kubernetes namespace like the dashboards for the Grafana user service). The dashboards are located under a private directory.  This service is accessible from the platform dashboard to users with the IT Admin management policy via a Status dashboard link on the Clusters | Application tab. The link opens the Kubernetes Cluster Status dashboard, but you can find additional dashboards in the parent private dashboards directory. NoteSeveral platform features rely on the monitoring service of the parent tenant, including the following:\n  The application-services and Nuclio-functions monitoring data that's displayed in the platform dashboard and in the platform's Grafana monitoring dashboards. Note that the predefined monitoring Grafana dashboards display information only when the monitoring service is enabled; the service is enabled by default.\n  Auto scaling of Nuclio functions.\n  You can also use the Kubernetes kubectl CLI from a platform web-shell or Jupyter Notebook application service to monitor your Kubernetes application cluster. For details, see Kubernetes monitoring tools.\n    See Also  Logging and Monitoring Services Data Monitoring and Visualization Services and Tools Logging, Monitoring, and Debugging  Kubernetes monitoring tools    ","keywords":["monitoring","services,","monitoring","application","services,","monitoring,","monitoring","service,","grafana","monitoring","dashboards,","grafana","dashboards,","grafana,","kubectl,","kubernetes"],"path":"https://github.com/jasonnIguazio/services/monitoring-services/","title":"Monitoring Platform Services"},{"content":" Overview As part of the platform installation, a set of security groups are configured to control access at the network level (\u0026quot;network security groups\u0026quot;), in addition to the platform's application-level authentication. These groups can be reconfigured manually at any time after the installation. When the platform is created with private IPs only, security groups are usually not much of a concern. However, it's crucial to properly set up the security groups when the platform is assigned public IP addresses.\nDuring the installation, you can provide the following, which affects the configuration of the network security groups; for more information, see the AWS installation guide:\n A CIDRs whitelist — a list of classless inter-domain routing (CIDR) addresses to be granted access to the platform's service ports. Iguazio access permission — allow Iguazio's support team to access the platform nodes from the Iguazio network. An installer CIDR — the CIDR of the machine on which you're running the platform installer. (This machine needs to connect to the platform during the installation to perform various actions.) A security groups' installer-CIDR rule can be deleted after the installation.  Security-Group Rules The installation automatically creates two network security groups — one for the data cluster and one for the application cluster — and configures appropriate rules based on the installation parameters.\n Data-cluster security-group rules Application-cluster security-group rules  Data-Cluster Security-Group Rules  Service Source CIDRs Destination Ports Condition Notes   Platform dashboard Whitelisted CIDRs 80, 443 A CIDRs whitelist is provided. Allows direct access to the platform dashboard from the data nodes.    Access to Iguazio support Iguazio office All The Iguazio network is allowed access. There are several ways to allow Iguazio support personnel to access the platform. One way is to allow access to the platform nodes from the Iguazio network (two /32 CIDR addresses).    SSH access for the installation Installer CIDR 22 Always The installer CIDR (configurable) is required because during the installation, the installer connects to the platform nodes through SSH. After the installation completes, this rule can be deleted..   Inter-cluster communication Application-cluster public IP addresses All Always Allows the data and application clusters to occasionally communicate with each other through their public IP addresses.   All services VPC All Always Allows the platform nodes to freely communicate with each other through their private IP addresses.   Application-Cluster Security-Group Rules  Service Source CIDRs Destination Ports Condition Notes   HTTP/S ingress Whitelisted CIDRs 80, 443 A CIDRs whitelist is provided. Allows access to the platform dashboard and to various application services (such as Jupyter Notebook and Grafana) from the platform application nodes.    Kubernetes API server Whitelisted CIDRs 6443 A CIDRs whitelist is provided. Allows crafting a kubeconfig file and running kubectl CLI commands from outside of the cluster (in addition to programmatic API access).    Web APIs (HTTP interface to the platform's data layer) Whitelisted CIDRs 8081, 8443 A CIDRs whitelist is provided. Accessing the platform's data layer through web-APIs service ports is much faster than through the HTTP/S ingress ports (80, 443), because the web-APIs ports are mapped directly to the platform's web-APIs service.    Kubernetes node ports Whitelisted CIDRs 30000\u0026ndash;32000 A CIDRs whitelist is provided. These ports are usually used by Nuclio functions. Consuming Nuclio functions through the Kubernetes node ports is faster than through the HTTP/S ingress ports (80, 443), because it eliminates the need to go through the ingress.    Access to Iguazio support Iguazio office All The Iguazio network is allowed access. There are several ways to allow Iguazio support personnel to access the platform. One way is to allow access to the platform nodes from the Iguazio network (two /32 CIDR addresses).   SSH access for the installation Installer CIDR 22 Always The installer CIDR (configurable) is required because during the installation, the installer connects to the platform nodes through SSH. After the installation completes, this rule can be deleted..   Inter-cluster communication Data-cluster public IP addresses All Always Allows the data and application clusters to occasionally communicate with each other through their public IP addresses.   All services VPC All Always Allows the platform nodes to freely communicate with each other through their private IP addresses.   Adding and Removing a Security-Group Rule You can modify the network security-group rules after the platform installation, through the AWS console and CLI, to enable or disable access to the platform. You should be able to modify most rules safely without affecting platform behavior, but do not modify the VPC CIDR or the application/data-cluster public IP addresses. For more information, see the AWS documentation.\nGranting Access to IguazioSupport If you allowed access to the platform from Iguazio's network during the installation, Iguazio's support personnel should be able to access the platform and assist you, provided the platform has public IP addresses. You can disable this access permission by simply removing the applicable rule in the security groups of both clusters (see the \u0026quot;Access to Iguazio support\u0026quot; service rules). If you'd like to provide access to Iguazio support regardless of whether the platform has public IP addresses), contact Iguazio's support team for instructions and a list of the latest CIDR addresses that you need to whitelist to allow this.\nSee Also  AWS cloud installation guide Configuring VPC Subnet Allocation of Public IP Addresses (AWS)  ","keywords":["security","groups","configuration,","aws","network","security","groups,","network","security","groups,","aws","security","groups,","security","groups,","security","group,","security,","network,","network","configuration,","securtiy","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/aws/howto/network-security-groups-cfg/","title":"Network Security Groups Configuration (AWS)"},{"content":" Overview As part of the platform installation, a set of security groups are configured to control access at the network level (\u0026quot;network security groups\u0026quot;), in addition to the platform's application-level authentication. These groups can be reconfigured manually at any time after the installation. When the platform is created with private IPs only, security groups are usually not much of a concern. However, it's crucial to properly set up the security groups when the platform is assigned public IP addresses.\nDuring the installation, you can provide the following, which affects the configuration of the network security groups; for more information, see the Azure installation guide:\n A CIDRs whitelist — a list of classless inter-domain routing (CIDR) addresses to be granted access to the platform's service ports. Iguazio access permission — allow Iguazio's support team to access the platform nodes from the Iguazio network. An installer CIDR — the CIDR of the machine on which you're running the platform installer. (This machine needs to connect to the platform during the installation to perform various actions.) A security groups' installer-CIDR rule can be deleted after the installation.  Security-Group Rules The installation automatically creates two network security groups — one for the data cluster and one for the application cluster — and configures appropriate rules based on the installation parameters.\n Data-cluster security-group rules Application-cluster security-group rules  Data-Cluster Security-Group Rules  Service Source CIDRs Destination Ports Condition Notes   Platform dashboard Whitelisted CIDRs 80, 443 A CIDRs whitelist is provided. Allows direct access to the platform dashboard from the data nodes.    Access to Iguazio support Iguazio office All The Iguazio network is allowed access. There are several ways to allow Iguazio support personnel to access the platform. One way is to allow access to the platform nodes from the Iguazio network (two /32 CIDR addresses).    SSH access for the installation Installer CIDR 22 Always The installer CIDR (configurable) is required because during the installation, the installer connects to the platform nodes through SSH. After the installation completes, this rule can be deleted..   Inter-cluster communication Application-cluster public IP addresses All The platform has public IP addresses Allows the data and application clusters to occasionally communicate with each other through their public IP addresses.   All services VNet All Always Allows the platform nodes to freely communicate with each other through their private IP addresses.   Application-Cluster Security-Group Rules  Service Source CIDRs Destination Ports Condition Notes   HTTP/S ingress Whitelisted CIDRs 80, 443 A CIDRs whitelist is provided. Allows access to the platform dashboard and to various application services (such as Jupyter Notebook and Grafana) from the platform application nodes.    Kubernetes API server Whitelisted CIDRs 6443 A CIDRs whitelist is provided. Allows crafting a kubeconfig file and running kubectl CLI commands from outside of the cluster (in addition to programmatic API access).    Web APIs (HTTP interface to the platform's data layer) Whitelisted CIDRs 8081, 8443 A CIDRs whitelist is provided. Accessing the platform's data layer through web-APIs service ports is much faster than through the HTTP/S ingress ports (80, 443), because the web-APIs ports are mapped directly to the platform's web-APIs service.    Kubernetes node ports Whitelisted CIDRs 30000\u0026ndash;32000 A CIDRs whitelist is provided. These ports are usually used by Nuclio functions. Consuming Nuclio functions through the Kubernetes node ports is faster than through the HTTP/S ingress ports (80, 443), because it eliminates the need to go through the ingress.    Access to Iguazio support Iguazio office All The Iguazio network is allowed access. There are several ways to allow Iguazio support personnel to access the platform. One way is to allow access to the platform nodes from the Iguazio network (two /32 CIDR addresses).   SSH access for the installation Installer CIDR 22 Always The installer CIDR (configurable) is required because during the installation, the installer connects to the platform nodes through SSH. After the installation completes, this rule can be deleted..   Inter-cluster communication Data-cluster public IP addresses All The platform has public IP addresses Allows the data and application clusters to occasionally communicate with each other through their public IP addresses.   All services VNet All Always Allows the platform nodes to freely communicate with each other through their private IP addresses.   Adding and Removing a Security-Group Rule You can modify the network security-group rules after the platform installation, through the Azure portal and CLI, to enable or disable access to the platform. You should be able to modify most rules safely without affecting platform behavior, but do not modify the VNet CIDR or the application/data-cluster public IP addresses. For more information, see the Azure documentation.\nGranting Access to IguazioSupport If you allowed access to the platform from Iguazio's network during the installation, Iguazio's support personnel should be able to access the platform and assist you, provided the platform has public IP addresses. You can disable this access permission by simply removing the applicable rule in the security groups of both clusters (see the \u0026quot;Access to Iguazio support\u0026quot; service rules). If you'd like to provide access to Iguazio support regardless of whether the platform has public IP addresses), contact Iguazio's support team for instructions and a list of the latest CIDR addresses that you need to whitelist to allow this.\nSee Also  Azure cloud installation guide  ","keywords":["security","groups","configuration,","azure","network","security","groups,","network","security","groups,","azure","security","groups,","security","groups,","security","group,","security,","network,","network","configuration,","securtiy","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/howto/network-security-groups-cfg/","title":"Network Security Groups Configuration (Azure)"},{"content":" Overview To support reading and writing NoSQL data using structured-data interfaces — such as Spark DataFrames, Presto, and V3IO Frames (\u0026quot;Frames\u0026quot;) — the platform uses a schema file that defines the schema of the data structure. When writing NoSQL data in the platform using a Spark or Frames DataFrame, the schema of the data table is automatically identified and saved and then retrieved when using a structure-data interface to read data from the same table (unless you explicitly define the schema for the read operation). However, to use a structure-data interface to read NoSQL data that was not written in this manner, you first need to define the table schema. The schema is stored as a JSON file (.#schema). You don't typically need to manually create or edit this file. Instead, use any of the supported methods to define or update the schema of a NoSQL table:\n  Spark — do one of the following as part of a NoSQL Spark DataFrame read operation. For more information, see Defining the Table Schema in the Spark NoSQL DataFrame reference:\n Use the custom inferSchema option to infer the schema (recommended). Define the schema programmatically. NoteProgrammatically created table schemas don't support range-scan or even-distribution table queries.      Presto — use the custom v3io.schema.infer Presto CLI command to generate a schema file. For more information, see Defining the NoSQL Table Schema in the Presto reference.\n  Frames — use the infer_schema or infer command of the NoSQL backend's execute client method to generate a schema file.\n  The Item-Attributes Schema Object ('fields') The NoSQL-table schema JSON file contains a fields array object with one or more objects that describe the table's attributes (columns). The attribute object has three fields:\n name The name of the attribute (column). For example, \u0026quot;id\u0026quot; or \u0026quot;age\u0026quot;.\n Type: String   type The attribute's data type (i.e., the type of the data that is stored in the column). The type can be one of the following string values — \u0026quot;boolean\u0026quot;, \u0026quot;double\u0026quot;, \u0026quot;long\u0026quot;, \u0026quot;null\u0026quot;, \u0026quot;string\u0026quot;, or \u0026quot;timestamp\u0026quot;. The platform implicitly converts integer and short values to long values (\u0026quot;long\u0026quot;) and floating-point values to double-precision values (\u0026quot;double\u0026quot;).\n Type: String in the schema file; Spark SQL data type when defining the schema programmatically using a Spark DataFrame  Spark DataFrame Programmatic Schema Definition NoteWhen defining the table shcema programmatically as part of a Spark DataFrame read operation, use the Spark SQL data types that match the supported schema-file attribute types (such as StringType for \u0026quot;string\u0026quot; or LongType for \u0026quot;long\u0026quot;). When writing the data to the NoSQL table, the platform will translate the Spark data types into the relevant attribute data types and perform any necessary type conversions.   nullable Indicates whether the value is \u0026quot;nullable\u0026quot;. If true, the attribute value can be null.\n Type: Boolean    The Item-Key Schema Objects ('key' and 'sortingKey') The NoSQL-table schema JSON file contains a key object that identifies the table's sharding-key attribute, and optionally also a sortingKey object that identifies the table's sorting-key attribute. These attributes are used to determine an item's name and primary-key value, which uniquely identifies items in the table. See Object Names and Primary Keys.\n key The name of the table's sharding-key attribute, which together with the sorting-key attribute (sortingKey), if defined, determines the primary-key values of the table items. For example, \u0026quot;id\u0026quot;.\n Type: String   sortingKey The name of the table's sorting-key attribute, if defined, which together with the sharding-key attribute (key) determines the primary-key values of the table items. For example, \u0026quot;date\u0026quot;.\n Type: String    Faster Primary-Key QueriesThe schema's key objects enable supporting faster NoSQL table reads (queries). See the Presto read-optimization and the NoSQL Spark DataFrame range-scans reference documentation.  See Also  Frames NoSQL-backend reference — Table Schema Presto reference — Defining the NoSQL Table Schema Spark NoSQL DataFrame reference — Defining the Table Schema Object Names and Primary Keys Frames Attribute Data Types Spark DataFrame Data Types Getting Started with Data Ingestion Using Spark \u0026gt; Reading NoSQL Data  ","keywords":["table","schema,","schema,","infer","schema,","inferSchema,","data","types,","reference,","attributes,","nosql,","boolean,","blob,","number,","string,","integer,","floating","point,","spark,","presto,","v3io,","v3io","GitHub,","iguazio_api_examples"],"path":"https://github.com/jasonnIguazio/data-layer/reference/nosql-table-schema/","title":"NoSQL Table Schema Reference"},{"content":"Introduction The platform's NoSQL Web API provides access to the NoSQL database service, which enables storing and consuming data in a tabular format. For more information, see Working with NoSQL Data. The platform's unified data model enables you to use the NoSQL Web API to fetch and update any object in the system.\n  To add an item to a table or replace an existing item, use PutItem.\nTo create a table, just begin adding items. The table will be created automatically with the first item that you add. See Working with NoSQL Data.\n  To update an existing item in a table, use UpdateItem. .\n  To retrieve attributes of a table item, use GetItem.\n  To retrieve attributes of multiple items in a table or in a data container's root directory, use GetItems.\nGetItems supports scan optimizations either by performing range scans that use the items' sharding and sorting keys to locate items more quickly, or by dividing the table-scan operation into multiple segments that can be assigned to different workers for implementing a parallel scan. See Scan Optimization in the GetItems documentation.\n  To delete a table, either delete (remove) the table directory by using a file-system command, or use the delete client method of the V3IO Frames NoSQL backend. See Working with NoSQL Data.\n  In the NoSQL Web API, an attribute is represented by an Attribute JSON object. This is true for all attribute types.\nThe NoSQL Web API is part of the data-service web APIs and conforms to the related structure and syntax rules — see Data-Service Web-API General Structure.\nNoteThe maximum JSON body size for the NoSQL web-API requests is 2 MB.  Item Name and Primary Key A table item is a data object, and as such needs to be assigned a unique primary-key value, which serves as the item's name and is stored by the platform in the __name system attribute — see Object Names and Primary Keys. When adding a new item by using the PutItem or UpdateItem operation, you can provide the item's primary-key value (name) either in the request URL or as the name of the request's Key attribute JSON request parameter (see Setting the Resource Path). If you select to use the Key JSON parameter, the platform defines a primary-key user attribute with the specified name and value, in addition to the __name system attribute that is always defined for a new item; (see Attribute Types for information about user and system attributes). Table Schema The NoSQL Web API doesn't enforce a specific table schema: i.e., the items don't all need to contain the same attributes and items can have different data types for the same attribute. However, be advised that if the table items don't follow a consistent attributes schema you might fail to read data from the table using a structured-data interface such as Spark Datasets, Presto, and V3IO Frames. See Also  Data-Service Web-API General Structure Securing Your Web-API Requests Working with NoSQL Data Objects Attributes Data Objects  Object Names and Primary Keys   NoSQL Web API software specifications and restrictions  ","keywords":["nosql","web","api,","nosql","databases,","nosql","dbs,","nosql","tables,","tables,","table","items,","attributes,","table","scan,","items","scan,","GetItem,","GetItems,","PutItem,","UpdateItem,","data-service","web","apis,","primary","key,","item","names,","names,","__name,","http","requests,","json,","json","parameters"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/overview/","title":"Overview of the NoSQL Web API"},{"content":"Introduction The platform's Streaming Web API enables working with data in the platform as streams. For more information, see Working with Data Streams (\u0026quot;Streaming\u0026quot;).\n  To create and configure a new stream, use CreateStream.\n  To get the description of a stream's configuration, use DescribeStream.\n  To update a stream's configuration (change its shard count), use UpdateStream.\n  To add records to a stream, use PutRecords.\nYou can optionally control the assignment of records to specific shards. For more information, see Stream Sharding and Partitioning.\n  To retrieve a specific location within a shard, to be used in a subsequent GetRecords operation, use Seek.\n  To retrieve records from a specific stream shard, use GetRecords.\nGetRecords receives as a parameter the location within the specified stream shard from which to begin consuming records. Use the Seek operation to retrieve the location of the first record to consume, according to the desired location type. GetRecords returns the location of the next record in the shard to consume, which you can use as the location parameter of a subsequent GetRecords operation. For more information, see Stream Record Consumption.\n    To delete a stream, delete (remove) the stream directory by using a file-system command. See Working with Data Streams (\u0026quot;Streaming\u0026quot;).  The Streaming Web API is part of the data-service web APIs and conforms to the related structure and syntax rules — see Data-Service Web-API General Structure.\nSee Also  Data-Service Web-API General Structure Securing Your Web-API Requests Working with Data Streams (\u0026quot;Streaming\u0026quot;) Streaming Web API software specifications and restrictions  ","keywords":["streaming","web","api,","streaming,","streams,","stream","shards,","shards,","sharding,","stream","records,","stream","partitions,","partitioning,","stream","consumption,","CreateStream,","create","stream,","DescribeStream,","describe","stream,","UpdateStream,","update","stream,","PutRecords,","add","records,","Seek,","stream","seek,","seek","records,","stream","delete,","data-service","web","apis"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/overview/","title":"Overview of the Streaming Web API"},{"content":"","keywords":["nosql","web","api,","api","reference,","nosql,","key","value,","KV"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/","title":"NoSQL Web API Reference"},{"content":" Overview Iguazio's Nuclio Enterprise Edition serverless-functions framework — a leading open-source project for converging and simplifying data management — is integrated into the platform. Nuclio is a high-performance low-latency framework that supports execution over CPUs or GPUs and a wide array of tools and event triggers, providing users with a complete cloud experience of data services, ML, AI, and serverless functionality — all delivered in a single integrated and self-managed offering at the edge, on-premises (\u0026quot;on-prem\u0026quot;), or in a hosted cloud.\nYou can use Nuclio functions, for example, to\n  Collect and ingest data into the platform and consume (query) the data on an ongoing basis. Nuclio offers built-in function templates for collecting data from common sources, such as Apache Kafka streams or databases, including examples of data enrichment and data-pipeline processing.\n  Run machine-learning models in the serving layer, supporting high throughput on demand and elastic resource allocation.\n  Nuclio can be easily integrated with Jupyter Notebook, enabling users to develop their entire code (model, feature vector, and application) in Jupyter Notebook and use a single command to deploy the code as a serverless function that runs in the serving layer. For examples, see the platform's tutorial Jupyter notebooks.\nNuclio is provided as a default (pre-deployed) shared single-instance tenant-wide platform service (nuclio). Users can disable, enable, and restart this service, as well as configure the Docker Registry for storing the Nuclio function images. See also the Nuclio restrictions in the Software Specifications and Restrictions documentation.\nConfiguring the Docker Registry for the Nuclio Service By default, the Nuclio service on the default tenant is configured to work with a predefined default tenant-wide Docker Registry service (docker-registry), which uses a pre-deployed local on-cluster Docker Registry. However, you can create your own Docker Registry service for working with a remote off-cluster Docker Registry, and change the configuration of the Nuclio service to work with your Docker Registry service and use your registry to store the Nuclio function images. Configuring the Node Selector The pods of a Nuclio function can only run on nodes whose labels match the node selector entries configured for the specific function. You can configure the key-value node selector pairs in the Custom Parameters tab of the service. If there is no specified node selector, it defaults to the Kubernetes default behavior, and runs on a random node.\nIntroduction to Nuclio and Serverless Functions In recent years, the concept of serverless development has been rapidly gaining traction. Serverless solutions accelerate and facilitate the development process by eliminating tedious server orchestration tasks and allowing developers to concentrate their efforts on the development logic. Developers can embed flexible computation within their data flow without worrying about the server infrastructure provisioning and DevOps considerations.\nTo provide users of the MLOps Platform (\u0026quot;the platform\u0026quot;) with a serverless solution on top of the platform, Iguazio developed Nuclio — a standalone open-source self-service application-development environment. The Nuclio Enterprise edition is integrated into the platform as part of the default installation. Nuclio allows developers to build and run auto-scaling applications, in their preferred programming language, without worrying about managing servers. Nuclio is currently the fastest serverless framework in the market. It allows running functions over CPUs or GPUs; supports a large variety of event sources (triggers); can be deployed in the cloud or on-premises (\u0026quot;on-prem\u0026quot;); and provides many other significant advantages. Combining the platform with Nuclio provides users with a complete cloud experience of data services, machine learning (ML) and artificial intelligence (AI), and serverless functionality — all delivered in a single integrated and self-managed offering at the edge, on-prem, or in a hosted cloud.\nNuclio comes with powerful UI (dashboard) and CLI (nuctl) applications and is simple to install and use. When using Nuclio from the platform, its dashboard is available in the Projects area of the platform dashboard. See specifically the Functions project page. In the Quick Links section of the project-overview sidebar, you can find Real-time functions (Nuclio) and API gateways (Nuclio) links to the Functions page's Functions and API Gateways tabs.\nNote that viewing, managing, and developing Nuclio functions is supported only for users with the Function Admin management policy.\nFor detailed information about Nuclio, including comprehensive documentation, visit the Nuclio website and GitHub repository. For information about creating and deploying Python Nuclio functions from Jupyter Notebook, see the Nuclio Jupyter package — nuclio-jupyter.\nSee Also  Working with Services Data Science and MLOps Nuclio software specifications and restrictions  ","keywords":["nuclio,","serverless","functions,","serverless,","jupyter,","jupyter","notebook,","docker","registry,","docker,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/nuclio/","title":"The Nuclio Serverless-Functions Service"},{"content":" Introduction When adding a new data object to the platform, you provide the object's name or the required components of the name. The platform stores the object name in the __name system attribute, which is automatically created for each object, and uses it as the value of the object's primary key, which uniquely identifies the object within a collection (such as a NoSQL table).\nSharding and Sorting Keys Primary keys affect the way that objects are stored in the platform, which in turn affects performance. The platform supports two types of object primary keys:\n Simple primary key A simple primary key is composed of a single logical key whose value uniquely identifies the object. This key is known as the object's sharding key. For example, a collection with a simple username primary key might have an object with the primary-key value \u0026quot;johnd\u0026quot;, which is also the value of the object's sharding key. Compound primary key A compound primary key is composed of two logical keys — a sharding key and a sorting key — whose combined values uniquely identify the object. The value of a compound primary key is of the format \u0026lt;sharding-key value\u0026gt;.\u0026lt;sorting-key value\u0026gt;. All characters before the leftmost period in an object's primary-key value define the object's sharding-key value, and the characters to the right of this period (if exist) define its sorting-key value. For example, a collection with a compound primary key that is made up of a username sharding key and a date sorting key might have an object with the sharding-key value \u0026quot;johnd\u0026quot;, the sorting-key value \u0026quot;20180602\u0026quot;, and the combined unique compound primary-key value \u0026quot;johnd.20180602\u0026quot;.  The platform divides the physical data storage into multiple units — data slices (also known as data shards or buckets). When a new data object is added, a hash function is applied to the value of its sharding key and the result determines on which slice the object is stored. All objects with the same sharding-key value are stored in a cluster on the same slice, sorted in ascending lexicographic order according to their sorting-key values (if exist). This design enables the support for faster NoSQL table queries that include a sharding-key and optionally also a sorting-key filter (see NoSQL read optimization).\nFor best-practice guidelines for defining primary keys, optimizing data and workload distribution, and improving performance, see Best Practices for Defining Primary Keys and Distributing Data Workloads.\nNote  The value of a sharding key cannot contain periods, because the leftmost period in an object's primary-key value (name) is assumed to be a separator between sharding and sorting keys.\n  To work with a NoSQL table using Spark DataFrames or Presto, the table items must have a sharding-key user attribute, and in the case of a compound primary-key also a sorting-key user attribute; for more efficient range scans, use a sorting-key attribute of type string (see Best Practices for Defining Primary Keys and Distributing Data Workloads for more information). To work with a NoSQL table using V3IO Frames, the table items must have a primary-key user attribute. The values of such key user attributes must match the value of the item's primary key (name) and shouldn't be modified after the initial item ingestion. (The NoSQL Web API doesn't require such attributes and doesn't attach any special meaning to them if they exist.) To change an item's primary key, delete the existing item and create a new item with the desired combination of sharding and sorting keys and matching user key attributes, if required.\n    Object-Name Restrictions The names of all data objects in the platform (such as items and files) are subject to the general file-system naming restrictions, including a maximum length of 255 characters. In addition —\n A period in an object name indicates a compound name of the format \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt;. See Sharding and Sorting Keys.   See Also  Best Practices for Defining Primary Keys and Distributing Data Workloads Objects Attributes Working with NoSQL Data  Read Optimization   Object-name software specifications and restrictions  ","keywords":["object","names,","object","keys,","object","primary","key,","simple","primary","key,","compound","primary","key,","sharding","key,","sorting","key,","object-name","restrictions,","attribute-name","restrictions,","attribute","names,","naming","restrictions,","object","attributes,","item","attributes,","attributes,","data","slices,","slices,","shards,","buckets,","object","metadata,","metadata,","objects,","simple","objects,","files,","items,","nosql","items,","nosql,","nosql","tables,","spark","dataframes,","spark,","dataframes,","nosql","dataframe,","v3io","frames,","frames,","presto,","range","scan,","read","optimization,","workload-distribution","optiomization,","workload","distribution,","performance","optimization,","performance"],"path":"https://github.com/jasonnIguazio/data-layer/objects/object-names-and-keys/","title":"Object Names and Primary Keys"},{"content":" Introduction All data objects in the platform have attributes. An attribute provides information (metadata) about an object. NoSQL table-item attributes in the platform are the equivalent of columns in standard NoSQL databases. See Working with NoSQL Data, including a terminology comparison. For the supported attribute data types, see the Attribute Data Types Reference.\nAttribute Types Attributes are classified into three logical types:\n User attributes Attributes that the user assigns to a data object using the platform APIs.  System attributes Attributes that the platform automatically assigns to all objects. The system attributes contain the object's name (__name) and miscellaneous information that was identified for the object by the system, such as the UID (__uid) and GID (__gid) of the object's owner, and the object's last-modification time __mtime_secs. For a full list of the supported system attributes, see the System-Attributes Reference.  Hidden attributes Attributes that the user or platform optionally assign to an object and are used to store internal information that is not meant to be exposed.   NoteThe platform's naming convention is to prefix the names of system and hidden attributes with two underscores (__) to differentiate them from user attributes.  Attribute Names Attribute names are subject to the general file-system naming restrictions and the following additional restrictions:\n  Contain only the following characters:\n Alphanumeric characters (a–z, A–Z, 0–9) Underscores (_)    Begin either with a letter (a–z, A–Z) or with an underscore (_)\n  Not identical to a reserved name — see Reserved Names\n  Length of 1–255 characters\n  NoteSpaces in attribute names are currently not supported.  See Also  System-Attributes Reference Attribute Data Types Reference Reserved Names Object Names and Primary Keys Working with NoSQL Data Attribute-name software specifications and restrictions  ","keywords":["object","attributes,","item","attributes,","attributes,","object","metadata,","file","metadata,","metadata,","attribute","types,","system","attributes,","user","attributes,","hidden","attributes,","attribute","names,","attribute-name","restrictions,","naming","restrictions,","objects,","simple","objects,","items,","nosql","items,","nosql,","key-value,","kv,","nosql","databases,","nosql","tables,","table","items,","table","columns,","columns"],"path":"https://github.com/jasonnIguazio/data-layer/objects/attributes/","title":"Objects Attributes"},{"content":" This section contains specifications, guides, and how-to tutorials for installing (deploying) and configuring the MLOps Platform on-premises (\u0026quot;on-prem\u0026quot;).\n","keywords":["on-prem","installation","and","setup,","on-prem","installation,","on-prem","setup,","on-prem,","on-premises,","on-prem","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/","title":"On-Premises Deployment"},{"content":" Overview This document lists the hardware specifications for on-premises (\u0026quot;on-prem\u0026quot;) deployment of version 3.2.1 of the MLOps Platform (\u0026quot;the platform\u0026quot;).\nCapacity CalculationsAll capacity calculations in the hardware specifications are performed using the base-10 (decimal) number system. For example, 1 TB = 1,000,000,000,000 bytes.  Hardware Configurations The platform is available in two configurations, which differ in a variety of aspects, including the performance capacity, footprint, storage size, and scale capabilities:\n Development Kit A single data-node and single application-node cluster implementation. This configuration is designed mainly for evaluation trials and doesn't include high availability (HA) or performance testing. Operational Cluster A scalable cluster implementation that is composed of multiple data and application nodes. This configuration was designed to achieve superior performance that enables real-time execution of analytics, machine-learning (ML), and artificial-intelligence (AI) applications in a production pipeline. The minimum requirement for HA support is three data nodes and three application nodes.  Both configurations also support an additional backup node for backing up the platform instance.\nFor both configurations, data nodes in on-prem deployments are always deployed on virtual machines (VMs) while application nodes can be deployed on either VMs or local machines (bare-metal).\nVM Deployment Notes Warning   Provisioning (deployment) of the platform's node VMs is done by using a dedicated virtualization package, provided by Iguazio.  Don't attempt to provision the servers yourself prior to the deployment.\n  Platform shutdown should be coordinated with Iguazio's support team.  Don't shut down the data-node VMs non gracefully, as this might erase the data.\n    When deploying on virtual machines, notify Iguazio's support team whenever VMware Enhanced vMotion Compatibility (EVC) mode is enabled, as a low EVC level might disable required CPU features.\nVM Hypervisor Host Specifications Hypervisor host machines in VM platform deployments must fulfill the following hardware specification requirements:\n Component  Specification   Network interfaces  A single-port 1 Gb (minimum) NIC for the management network  For hosting data-node VMs \u0026mdash; a dual-port 10 Gb (minimum) NIC for the data-path (client) and interconnect networks  For hosting application-node VMs only \u0026mdash; a single-port 10 Gb (minimum) NIC for the data-path (client) network     Hypervisor VMware vSphere ESXi 6.5, 6.7, 7.0, or Proxmox VE (PVE) 6.x or 7.0   VM Data-Node Specifications Data nodes in on-prem platform deployments are VMs that must fulfill the following hardware specification requirements.\nNoteFor some components, the specification differentiates between small and large data nodes. Large data nodes provide greater processing capabilities. Note that you cannot mix the specifications of these two alternative configurations.   Component  Specification   Memory 64 GB (POC node) / 128 GB (small node) / 256 GB (large node)  Cores 4 (POC node) / 8 (small node) / 16 (large node)  VM boot disk 400 GB (minimum) image (hosted on an enterprise-grade SSD-based data store)  Data disks 2, 4, 6, or 12 (POC and small node) / 2, 4, 6, 8, or 12 (large node) NVMe or enterprise-grade SSD data disks (drives) of 1 TB (minimum) each, which are mapped exclusively to the data-node VM using direct attach storage (DAS), such as raw device mapping (RDM).   Application-Node Specifications In on-prem deployments you can select whether to deploy the application nodes on virtual machines (VMs) or on local machines (bare-metal), provided the same method is used on all nodes. VM Application-Node Specifications Application nodes in VM platform deployments are VMs that must fulfill the following hardware specification requirements.\nNoteFor some components, the specification differentiates between small and large application nodes. Large application nodes provide greater processing capabilities. Note that you cannot mix the specifications of these two alternative configurations.   Component  Specification   Memory 64 GB (small node) / 128 GB (large node)  Cores 8 (small node) / 16 (large node)  VM boot disk 400 GB (minimum) image (hosted on an enterprise-grade SSD-based data store)  GPU (Optional) GPUs supported by the NVIDIA Linux display driver 450.57.   Bare-Metal Application-Node Specifications Application nodes in bare-metal platform deployments are supplied by the customer and must fulfill the following hardware specification requirements:\n Cores 8 (minimum)  Memory 64 GB of RAM (minimum)  OS boot disk 400 GB (minimum) enterprise-grade SSDs  Network interfaces  Single port 1 Gb (minimum) NIC for the management network Single port 10 Gb (minimum) NIC for the data-path (client) network    GPU (Optional) GPUs supported by the NVIDIA Linux display driver 450.57.   Backup-Node Specifications (Optional) If you wish to back up your instance of the platform, you need an additional VM or bare-metal backup node, which is used only for backups. The backup node must fulfill the following hardware specification requirements: NoteIt's strongly recommended that you back up your data on a regular basis.   Component  Specification   Memory 64 GB  Cores 8  VM boot disk 400 GB (minimum) image (hosted on an enterprise-grade SSD-based data store)  Backup storage 2 TB (minimum) of network or direct attached storage or a storage area network (NAS/DAS/SAN). The exact amount of required storage depends on the amount of data that's being used in the platform; consult Iguazio's support team.   See Also  High Availability (HA) Software Specifications and Restrictions Support and Certification Matrix  ","keywords":["VM","hardware","specifications,","virtual","machine,","VM,","hardware","specs,","hardware","configuration,","hardware,","specification,","spec,","on-prem,","on-prem","spec"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/on-prem-hw-spec/","title":"On-Premises Deployment Specifications"},{"content":"The platform's Cluster-Information Management API (\u0026quot;the Cluster-Information API\u0026quot;) supports a Get Cluster Information operation for retrieving information about the endpoints of the platform cluster, which provide access to the platform's resources. The returned information includes the endpoints' IP addresses.\nThe Cluster-Information API is part of the cluster-management APIs and conforms to the related structure and syntax rules — see General Management-API Structure. The API's endpoint is /api/cluster_info.\n","keywords":["cluster-information","management","api,","cluster-information","api,","management,","cluster","management,","cluster","information,","Get","Cluster","Information,","/api/cluster_info,","api","endpoints"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/cluster-info-api/overview/","title":"Overview of the Cluster-Information Management API"},{"content":"The platform's Containers Management API (\u0026quot;the Containers API\u0026quot;) is used to create and manage data containers.\n  To create a new container, use Create Container.\n  To return information about all the containers that are visible to the user who sent the request, or about a specific container, use List Containers.\n  To delete a container, use Delete Container.\n  The Containers API is part of the cluster-management APIs and conforms to the related structure and syntax rules — see General Management-API Structure. The API's endpoint is /api/containers.\nThe Containers API identifies containers by their IDs. When creating a new container, you set the container name and get back the container ID. See Container Names and IDs.\n","keywords":["containers","management","api,","containers","api,","management,","containers","management,","containers,","Create","Container,","Delete","Container,","List","Containers,","container","names,","container","IDs,","names,","/api/containers,","api","endpoints"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/containers-api/overview/","title":"Overview of the Containers Management API"},{"content":"The platform's Sessions Management API (\u0026quot;the Sessions API\u0026quot;) is used to support user authentication and authorization. The management-API operations (except where otherwise specified) authenticate the sender by requiring a session cookie, and authorize the sender to perform the operation based on the authentication results. You acquire this cookie by using the Session API's Create Session operation.\nThe Sessions API is part of the cluster-management APIs and conforms to the related structure and syntax rules — see General Management-API Structure. The API's endpoint is /api/sessions.\nSee Also  Security  ","keywords":["sessions","management","api,","sessions","api,","management,","session","management,","sesions,","security,","authentication,","authorization,","session","cookies,","cookies,","Create","Session,","/api/sesions,","api","endpoints"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/sessions-api/overview/","title":"Overview of the Sessions Management API"},{"content":"Fully Qualified Name org.apache.spark.streaming.v3io.PayloadWithMetadata\nDescription A case class that contains decoded stream record data and related metadata.\nSummary Instance Constructors\ncase class PayloadWithMetadata[V]( topicName: String = \u0026#34;\u0026#34;, partitionId: Short = -1, createTimeMs: Long = -1, createTimeNano: Long = -1, recordKey: Long = -1, payload: V = null.asInstanceOf[V]) Instance Constructors Syntax PayloadWithMetadata[V]( topicName: String, partitionId: Short, createTimeMs: Long, createTimeNano: Long, recordKey: Long1, payload: V) Parameters and Data Members NoteThe parameters have default initialization values (such as -1), but any parameter that is intended to be used must be set explicitly. The RecordAndMetadata.payloadWithMetadata method sets all parameters for the returned object.   topicName The name of the stream that contains the record.\n Type: String   partitionId The ID of the stream shard that contains the record (see Stream Sharding and Partitioning).\n Type: Short   createTimeMs The record's ingestion time (the time at which the record arrived at the platform), as a Unix timestamp in milliseconds. For example, 1511260205000 indicates that the record was ingested on 21 Nov 2017 at 10:30:05 AM. The createTimeNano parameter holds the nanoseconds unit of the ingestion time.\n Type: Long   createTimeNano The nanoseconds unit of the createTimeMs ingestion-time timestamp. For example, if createTimeMs is 1511260205000 and createTimeNano is 500000000, the record was ingested on 21 Nov 2017 at 10:30 AM and 5.5 seconds.\n Type: Long   recordKey The record's sequence number (see Stream Record Consumption).\n Type: Long   payload Decoded record data, formatted as the value type of the current class instance (V).\n Type: V\n    Type Parameters  V \u0026mdash;  the data type into which to convert the record's data payload (\u0026quot;value\u0026quot;).  ","keywords":["PayloadWithMetadata,","spark","streaming,","stream","records,","record","metadata,","record","payload,","decoding,","stream","shards,","shards,","stream","partitions,","record","ingestion,","stream","consumption,","record","sequence","number,","createTimeMs,","createTimeNano,","partitionId,","payload,","recordKey,","topicName"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/payloadwithmetadata/","title":"PayloadWithMetadata Class"},{"content":" In addition to its core data services, the platform comes pre-deployed with essential and useful proprietary and third-party open-source tools and libraries that facilitate the implementation of a full data science workflow, from data collection to production (see Introducing the Platform). Both built-in and integrated tools are exposed to the user as application services that are managed by the platform using Kubernetes. Each application is packaged as a logical unit within a Docker container and is fully orchestrated by Kubernetes, which automates the deployment, scaling, and management of each containerized application. This provides users with the ability and flexibility to run any application anywhere, as part of their operational pipeline.\nThe application services can be viewed and managed from the dashboard Services using a self-service model. This approach enables users to quickly get started with their development and focus on the business logic without having to spend precious time on deploying, configuring, and managing multiple tools and services. In addition, users can independently install additional software—such as real-time data analytics and visualization tools—and run them on top of the platform services.\n","keywords":["platform","services,","services,","application","services"],"path":"https://github.com/jasonnIguazio/services/","title":"Platform Services"},{"content":" Overview To use the platform, you must be logged in as a user with relevant permissions. A user can optionally be a member of one or more user groups. When a user is a member of multiple groups, one of the groups is defined as the user's primary group.\nThe platform has several predefined users. A security administrator — which is any user with the Security Admin management policy, including the predefined security_admin user — can manage platform users and user groups. A security administrator can create new local users and groups, import users and groups from a supported identity provider (IdP), and delete local or imported users and groups. NoteAll users can view information for their own user profile and edit relevant properties, such as the password, email address, or first and last names. But only a security administrator can view full user information for all users and edit secure properties such as the username, management policies, or groups.  The user management is done from the Identity page of the dashboard, as demonstrated in the following image:\n    Every user has a username and a password, which can be used to authenticate the user's identity. In addition, every user and user group must be assigned one or more management policies that determine user permissions to access resources and perform different operations. Users with the Application Admin management policy — including the predefined security_admin user — can define fine-grained data-access policies to restrict or allow user access to specific data resources. For more information, see the Security documentation, and specifically the Authentication and Authorization sections.\nNoteFor username restrictions, refer to the Software Specifications and Restrictions.  Predefined Users All Tenants The following users are predefined for the default tenant and for any new tenant that you create:\n pipelines The predefined pipelines user has the default Data management policy and is used by the platform's pipelines service to access ML pipeline data. Note that editing this user's profile might cause the monitoring service to stop working. monitoring The predefined monitoring user has the default Application Admin and Data management policies and is used by the platform's monitoring service to access performance logs. Note that editing this user's profile might cause the monitoring service to stop working. tenancy_admin The predefined tenancy-administrator user has the IT Admin and Tenant Admin management policies, which enable performing cluster administration — including shutting down the cluster, monitoring events and alerts, triggering log gathering, and managing tenants.  The default password of the tenancy_admin user is \u0026quot;IGZTEN\u0026quot;. You must change this password after the first login.  Default-Tenant Only The following users are predefined only for the default tenant:\n security_admin The predefined security-administrator user has the Security Admin management policy, which enables managing users and user groups — including creating and deleting users and user groups, integrating the platform with a supported IdP, and assigning management policies.  The default password of the security_admin user is \u0026quot;IGZSEC\u0026quot;. You must change this password after the first login. sys The predefined system user — known as \u0026quot;the backup user\u0026quot; — has the Application Admin and Data management policies and is used for performing backups.\nBackup Notes Data backups aren't activated automatically on all systems. Contact Iguazio's support team to check the backup status for your cluster. To allow backups when using data-access policy rules, ensure that as part of these rules, preferably at the start, you also grant the \u0026quot;sys\u0026quot; backup user access to the data. For more information, see Data-Access Policy Rules in the Security documentation.      Using an External Identity Provider (IdP) A user with a Security Admin management policy, such as the predefined security_admin user, can select to import users and user groups from an external identity provider (IdP) into the platform. When an IdP is configured, it is used to authenticate the identity of all its imported users in the platform. This doesn't prevent you from also defining local users and using the platform to authenticate them. For more information, see Authentication in the security documentation.\nIdP configuration is done from the IdP tab on the dashboard's Identity page. Start by selecting an IdP from the drop-down list next to the Remote host settings label. (In v3.2.1, only Microsoft Active Directory is supported.)    NoteWhen you complete the IdP configuration (as detailed in the following sections), remember to select Apply Changes to save your configuration.   Configuring the Remote IdP Host Configuring IdP Synchronization Configuring Default Management Policies  Configuring the Remote IdP Host In the Remote host settings configuration section, enter the required information for working with your selected IdP — the username and password of an IdP user with the necessary permissions, the address of the remote IdP host, and the root IdP user directory.\n   [Tech Preview] You can optionally use the Person filter field to add a Microsoft AD LDAP syntax filter for synchronizing only with specific user groups from the external IdP —\n(\u0026amp;(objectClass=Person)(memberOf=\u0026lt;full LDAP group path\u0026gt;) For example, to synchronize with a user group named AppA whose full group path is \u0026quot;IguazioDevUsers,OU=ApplicationAccess,OU=Groups,OU=GlobalProd,DC=GLOBAL,DC=ECOLAB,DC=CORP\u0026quot;, use this filter criteria:\n(\u0026amp;(objectClass=Person)(memberOf=CN=AppA-IguazioDevUsers,OU=ApplicationAccess,OU=Groups,OU=GlobalProd,DC=GLOBAL,DC=ECOLAB,DC=CORP)) You can add multiple group search criteria to the filter. Configuring IdP Synchronization In the Sync mode configuration section, select the mode for synchronizing the imported IdP users in the platform with the IdP after the initial import. You can also optionally set an interval for performing periodic synchronizations in the Periodic sync section.    You can select between two alternative modes of synchronization — Partial or Full:\nNoteIn either mode, the synchronization is always done in one direction: changes done in the IdP are applied locally in the platform, but the IdP is never modified to apply local platform changes.   Partial synchronization Synchronize addition and removal of users in the IdP after the initial import, but do not synchronize field changes for previously imported users and user groups. During partial synchronization, the currently configured IdP default management policies are applied to all new imported users and user groups, but the management policies of local previously imported IdP users and groups remain unaffected. For example:\n The following local changes to imported users or user groups in the platform are not overwritten during partial synchronization:  A user record field (such as an email address or job title) was added or removed, or a value of an existing field has changed. For example, you can disable an imported IdP user locally in the platform by changing the value of relevant user field without affecting the user's status in the external IdP. A user was added to or removed from an imported user group. A user's or user group's management policies were modified.   The following IdP changes since the previous synchronization are not applied locally in the platform during partial synchronization:  A user record field was added or removed, or the value of an existing field has changed. A user was added to or removed from an existing group.   The following IdP changes since the previous synchronization are also applied locally in the platform during partial synchronization:  A new user or user group was added. (The newly imported IdP users and groups will be assigned the default IdP management policies that are configured in the platform at the time of the synchronization.) An existing user or user group was deleted or renamed.     Full synchronization Synchronize all IdP user and user group additions, removals, and record updates by overwriting the current imported IdP user and user-group information with the updated IdP information.\nNote  Empty IdP user groups are not imported to the platform. When users are added to a group, the group is imported as part of the next full or partial IdP synchronization and the related user information is updated accordingly.\n  As part of the full-sync import, the currently configured IdP default management policies will be applied to all imported users and user groups.\n      Modifying the IdP configuration (including an initial configuration) triggers an automatic synchronization cycle. Periodic synchronizations are triggered according to the configured periodic-sync interval (if configured), and you can also always trigger a manual synchronization by selecting the Sync option in the IdP tab. All synchronizations are done according to the configured IdP synchronization mode. However, note that modifying the IdP's remote host address or root user directory essentially changes the configured IdP, so except for any common users or groups that might exist in both IdPs, any previous changes to the imported IdP users or groups will be overwritten as part of the synchronization even in the case of a partial synchronization. Configuring Default Management Policies In the Default management policies configuration section, select one or more management policies that will be applied to every imported IdP user and user group. For more information about management policies, see Management Policies.\n   NoteYou must select at least one default management policy. You can always change the management policies of an imported user or group after the import.  Deleting Users Before deleting a platform user, check the need to reallocate their resources and responsibilities. If the user is the running user of managed application services (such as Spark or Presto), a service administrator should either delete these services or reassign them to a different running user.\nSee Also  Security  ","keywords":["user","management,","users,","user","groups,","predefined","users,","security_admin,","tenancy_admin,","create","users,","remove","users,","impor","users,","synchronize","users,","idp,","identity","provider,","security,","authentication,","authentication,","authorization,","configuration,","dashboard"],"path":"https://github.com/jasonnIguazio/users-and-security/users/","title":"Platform Users"},{"content":" Learn about the MLOps Platform's user-management and security concepts and tools.\n","keywords":["users","and","security,","user","management","and","security,","platofrm","users,","users,","security"],"path":"https://github.com/jasonnIguazio/users-and-security/","title":"Platform Users and Security"},{"content":"This section contains how-to guides to help you set up and configure the MLOps Platform (\u0026quot;the platform\u0026quot;) after the initial installation (deployment).\n","keywords":["post-deployment","setup","and","configuration","how-tos,","setup","and","configuration","how-tos,","post-deployment","setup,","post-deployment","configuration,","setup","how-tos,","configurations","how-tos,","installation","how-tos,","setup,","configuration,","installation,","post","deployment"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/post-deployment-howtos/","title":"Post-Deployment How-Tos"},{"content":"Before you install an instance of the platform to an Azure cloud, perform the pre-installation steps outlined in this guide.\nInstall the Azure CLI Install the Azure CLI (az) by following the instructions in the Azure documentation. When you're done, run the following command from a command-line shell to verify that the CLI is functioning properly:\naz login This command should open an Azure portal login screen. Log into your Azure account and run the following command to list all the existing resource groups under your account:\naz group list Get Your Subscription ID The platform installation requires using your Azure subscriptions ID. Your Azure Tenant can contain multiple subscriptions. Run the following command to see the IDs of all the tenant subscriptions.\naz account list --output table Locate your subscription in the output, and save the subscription ID.\nSee Also  Azure cloud installation guide  ","keywords":["azure","pre-installation,","azure","cli,","azure","subscritpion","ids,","subscription","ids"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/howto/pre-install-steps/","title":"Pre-Installation Steps Using the Azure CLI"},{"content":" Introduction You can use the Presto open-source distributed SQL query engine to run interactive SQL queries and perform high-performance low-latency interactive analytics on data that is stored in the platform. Running Presto over the platform's data services enables you to filter data as close as possible to the source. The platform's Iguazio Presto connector defines a custom data source that enables you to use Presto to query data in the platform's NoSQL store — including support for table partitioning, predicate pushdown, column pruning, and performing optimized item-specific and range-scan queries. You can also use Presto's built-in Hive connector to query data of the supported file types, such as Parquet or ORC, that is stored in platform data containers; see Using the Hive Connector. In addition, it's possible to add an Oracle connector to Presto [Tech Preview]; for more information, contact Iguazio's support team.\nThe default v3.2.1 platform installation includes the following Presto version 359 components:\n The Presto command-line interface (CLI) for running queries. The web-based shell service and the terminals of the Jupyter Notebook platform service are automatically connected to the predefined Presto service and include both the native Presto CLI (presto-cli) and a presto wrapper to simplify working with the Iguazio Presto connector. For more information, see The Presto CLI. The Presto server. The server address is the API URL of the Presto service (presto), which you can copy from the dashboard Services page. The Presto web UI for monitoring and managing queries. This interface can be accessed by using the HTTP or HTTPS UI URLs of the Presto service, which are available from the dashboard Services page.  Table Paths For information about setting table paths when using the Iguazio Presto connector, see Table Paths in the Presto CLI reference. For information about setting table paths when using the Hive connector, see Using the Hive Connector.\nUsing the Hive Connector You can use Presto's built-in Hive connector to query data of the supported file types, such as Parquet or ORC, that is stored in platform data containers, or to save table-query views to the default Hive schema (hive.default). Running the Hive CLIYou can start the Hive CLI in the platform by running the hive command from a web shell or Jupyter terminal.  Enabling Hive To use the Presto Hive connector, you first need to create a Hive Metastore by enabling Hive for the platform's Presto service:   On the Services dashboard page, select to edit the Presto service and navigate to the Custom Parameters tab.\n  Check the Enable Hive check box and provide the required configuration parameters:\n Username — the name of a platform user for creating and accessing the Hive Metastore. Container — The name of the data container that contains the Hive Metastore. Hive Metastore path — The relative path to the Hive Metastore within the configured container. If the path doesn't exist, it will be created by the platform.  Select Save Service to save your changes.\n  Select Apply Changes from the top action toolbar of the Services page to deploy your changes.\n  Note  If you later select to disable Hive or change the Hive Metastore path, the previously configured Hive Metastore won't be deleted automatically. You can delete it like any other directory in the platform's distributed file system, by running a file-system command (such as rm -rf) from a command-line interface (a web shell or a Jupyter notebook or terminal).\n  You cannot change the Hive user of the Presto service for an existing Hive Metastore. To change the user, you need to either also change the metastore path so as not to point to an existing metastore; or first delete the existing metastore — disable Hive for Presto, apply your changes, delete the current Hive Metastore directory (using a file-system command), and then re-enable Hive for Presto and configure the same metastore path with a new user.\n    Creating External Tables To use the Hive connector to query data in a platform data container, you first need to use the Hive CLI to run a CREATE EXTERNAL TABLE statement that creates an external table. The statement should map the relevant data path to a unique table name, and define the names and data types of the table's columns (attributes); the data path in the statement should be specified as a fully qualified v3io path of the format v3io://\u0026lt;container name\u0026gt;/\u0026lt;relative data path\u0026gt;:\nCREATE EXTERNAL TABLE \u0026lt;table name\u0026gt; (\u0026lt;column name\u0026gt; \u0026lt;column type\u0026gt;[, \u0026lt;column name\u0026gt; \u0026lt;column type\u0026gt;, ...]) stored as \u0026lt;file type\u0026gt; LOCATION \u0026#39;\u0026lt;data path\u0026gt;\u0026#39;; For example, the following command creates an external Hive table that links to a \u0026quot;prqt1\u0026quot; Parquet file in a \u0026quot;mycontainer\u0026quot; container with a string col1 column and a big-integer col2 column:\nCREATE EXTERNAL TABLE prqt1 (col1 string, col2 bigint) stored as parquet LOCATION \u0026#39;v3io://mycontainer/prqt1\u0026#39;; You can then reference this table by its name from Presto queries that use the hive catalog. For example:\nSELECT * FROM hive.mycontainer.prqt1; Defining Table Partitions The Hive connector can also be used to query partitioned tables (see Partitioned Tables in the Presto CLI reference), but it doesn't automatically identify table partitions. Therefore, you first need to use the Hive CLI to define the table partitions after creating an external table. You can do this by using either of the following methods\n  Use the MSCK REPAIR TABLE statement to automatically identify the table partitions and update the table metadata in the Hive Metastore:\nMSCK REPAIR TABLE \u0026lt;table name\u0026gt;; For example, the following command updates the partition metadata for an external \u0026quot;prqt1\u0026quot; table:\nMSCK REPAIR TABLE prqt1; This is the simplest method, but it only identifies partition directories whose names are of the format \u0026lt;column name\u0026gt;=\u0026lt;column value\u0026gt;.\n  Use the ALTER TABLE ADD PARTITION statement to manually define partitions — where \u0026lt;partition spec\u0026gt; is of the format \u0026lt;partition column\u0026gt; = \u0026lt;partition value\u0026gt;[, \u0026lt;partition column\u0026gt; = \u0026lt;partition value\u0026gt;, ...], and \u0026lt;partition path\u0026gt; is a fully qualified v3io path of the format v3io://\u0026lt;container name\u0026gt;/\u0026lt;relative table-partition path\u0026gt;:\nALTER TABLE \u0026lt;table name\u0026gt; ADD [IF NOT EXISTS] PARTITION (\u0026lt;partition spec\u0026gt;) LOCATION \u0026#39;\u0026lt;partition path\u0026gt;\u0026#39;[, PARTITION \u0026lt;partition spec\u0026gt; LOCATION \u0026#39;\u0026lt;partition path\u0026gt;\u0026#39;]; For example, the following command defines a partition named \u0026quot;year\u0026quot; whose value is \u0026quot;2019\u0026quot;, which maps to a partition directory named year=2019 in a \u0026quot;prqt1\u0026quot; table in a \u0026quot;mycontainer\u0026quot; container:\nALTER TABLE prqt1 ADD PARTITION (year=2019) LOCATION \u0026#39;v3io://mycontainer/prqt1/year=2019\u0026#39;;    See Also  The Presto CLI Presto and Hive software specifications and restrictions  ","keywords":["presto","overview,","presto,","reference,","presto","cli,","presto","web","ui,","presto","web","interface,","Iguazio","Presto","connector,","sql","queries,","sql","hive","metastore,","hive,","parquet","tables,","parquet","files,","parquet,","orc","files,","orc,","data","paths,","table","paths,","range","scan,","nosql"],"path":"https://github.com/jasonnIguazio/data-layer/presto/overview/","title":"Introduciton to Using Presto in the Platform"},{"content":" This section contains guides for installing (deploying) the MLOps Platform (\u0026quot;the platform\u0026quot;) on-prem on virtual machines (VMs) using the Proxmox Virtual Environment (Proxmox VE, a.k.a. PVE) virtualization platform.\n","keywords":["proxmox","ve","vm","installation,","proxmox","ve","instalaltion,","pve","instalaltion,","proxmox","instalaltion,","proxmox","virtual","environment","installation,","proxmox","ve","setup,","pve","setup,","proxmox","setup,","proxmox","virtual","environment","setup,","proxmox","on-prem","installation,","proxmox","ve,","pve,","proxmox","virtual","environment,","proxmox"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/","title":"Proxmox VE VM Deployment"},{"content":"Description Creates an item with the provided attributes. If an item with the same name (primary key) already exists in the specified table, the existing item is completely overwritten (replaced with a new item). If the item or table do not exist, the operation creates them.\nRequest Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: PutItem \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;PutItem\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the item to add. The path includes the table path and the item's name (primary key). You can optionally set the item name in the request's Key JSON parameter instead of in the URL; you can also optionally set the relative table path within the container, or part of the path, in the request's TableName JSON parameter.\nRequest Data Syntax  { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Item\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;BOOL\u0026#34;: Boolean, \u0026#34;B\u0026#34;: \u0026#34;blob\u0026#34; } } }  payload = { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Item\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;BOOL\u0026#34;: Boolean, \u0026#34;B\u0026#34;: \u0026#34;blob\u0026#34; } } }    Parameters  TableName The table (collection) to which the new item should be added — a relative table path within the configured data container or the end of such a path, depending on the resource configuration in the request URL. See Data-Service Web-API General Structure.\n Type: String   Requirement: Required if not set in the request URL   Key A primary-key attribute whose value is the item's primary-key value, which uniquely identifies the item within the table. The primary-key value is also the item's name and is stored by the platform in the __name system attribute. When defining the key for the PutItem request by setting the Key JSON request parameter (instead of setting the key in the URL), the platform also automatically defines a primary-key user attribute of the specified name, type, and value (the primary-key value). See also Item Name and Primary Key.\nNote  To support range scans, use a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary-key value. For more information, see Working with NoSQL Data. See the primary-key guidelines in the Best Practices for Defining Primary Keys and Distributing Data Workloads guide.     Type: Attribute object   Requirement: Required if the item's name (primary key) is not set in the URL   ConditionExpression A Boolean condition expression that defines a conditional logic for executing the put-item operation. See Condition Expression for syntax details and examples. The condition expression is evaluated against the table item to be updated, if it exists. Note:\n  If the condition expression evaluates to true — including in the case of an empty condition-expression string — the item is created or overwritten (if it already exists).\n  If the condition expression evaluates to false — including when the expression references a non-existing item attribute — the operation completes successfully without creating or overwriting the item.\n  NoteIn v3.2.1, when ConditionExpression evaluates to false, PutItem returns a 400 error even though the operation is considered successful.   Type: String   Requirement: Optional   Item The item to add — an object containing zero or more attributes.\nNote As explained in the description of the Key parameter, when this parameter is set in the JSON request body, the platform defines a primary-key user attribute whose value is the item's name and primary-key value, which is also always stored in the item's __name system attribute. If you select to include in your request's Item object an attribute with the same name as specified in the Key parameter (even though this is redundant), make sure to set the value of this attribute to the same primary-key value as set in the Key parameter. To access the table with Spark DataFrames or Presto, the item must have a sharding-key user attribute, and in the case of a compound primary-key also a sorting-key user attribute; (for more efficient range scans, use a sorting-key attribute of type string). To access the table with V3IO Frames, the item must have a primary-key user attribute. The NoSQL Web API doesn't attach any special significance to such key user attributes, but to work with a structured-data API you must define the required attributes, set their values according to the value of the item's primary key (name) — which is composed of a sharding key and optionally also a sorting key — and refrain from modifying the values of these attributes. See also Sharding and Sorting Keys.     Type: An item JSON object that contains zero or more Attribute objects   Requirement: Required    Response Response DataNone\nExamples Request Add to the People table a person item with a primary-key ID attribute, and Name, Age, and Country attributes. If the item already exists, it will be entirely overwritten:\n POST /mycontainer/MyDirectory/People/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: PutItem X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;Key\u0026#34;: {\u0026#34;ID\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1234\u0026#34;}}, \u0026#34;Item\u0026#34;: { \u0026#34;Age\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;42\u0026#34;}, \u0026#34;Country\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UK\u0026#34;}, \u0026#34;Name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;John\u0026#34;} } }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/People/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;PutItem\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;Key\u0026#34;: {\u0026#34;ID\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1234\u0026#34;}}, \u0026#34;Item\u0026#34;: { \u0026#34;Age\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;42\u0026#34;}, \u0026#34;Country\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UK\u0026#34;}, \u0026#34;Name\u0026#34;: {\u0026#34;S\u0026#34;, \u0026#34;John\u0026#34;} } } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ","keywords":["PutItem,","put","item,","put","items,","nosql,","tables,","table","items,","attributes,","condition","expressions,","item","names,","object","names,","attribute","names,","primary","key,","sharding","key,","sorting","key,","range","scan,","ConditionExpression,","Key,","Item,","TableName"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/putitem/","title":"PutItem"},{"content":"Description Adds records to a stream.\nYou can optionally assign a record to specific stream shard by specifying a related shard ID (see the Records.ShardId request parameter), or associate the record with a specific partition key to ensure that similar records are assigned to the same shard (see the Records.PartitionKey parameter). By default, the platform assigns records to shards using a Round Robin algorithm. For more information, see Stream Sharding and Partitioning.\nNoteThe maximum JSON body size for PutRecords requests is 10 MB.  Request Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: PutRecords \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;PutRecords\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the stream to which to add the records. You can optionally set the stream name in the request's StreamName JSON parameter instead of in the URL.\nRequest Data Syntax  { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Records\u0026#34;: [ { \u0026#34;ClientInfo\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;PartitionKey\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardId\u0026#34;: number } ] }  payload = { \u0026#34;Records\u0026#34;: [ { \u0026#34;ClientInfo\u0026#34;: \u0026#34;MTIzNA0K\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;ZGF0YQ0K\u0026#34; } ] }    Parameters  StreamName The name of the stream that contains the shard resource.\n Type: String   Requirement: Required if not set in the request URL   Records An array of one or more records to add to the stream, up to a maximum of 1,000 records. If the records limit is exceeded, the request fails.\n Type: Array of record JSON objects   Requirement: Required  The record JSON object contains these elements:\n ClientInfo Custom opaque information that can optionally be provided by the producer. This metadata can be used, for example, to save the data format of a record, or the time at which a sensor or application event was triggered. See Record Metadata.\n Type: Blob — a Base64 encoded string   Maximum Size: 128 bits   Requirement: Optional   Data Record data.\n Type: Blob — a Base64 encoded string   Maximum Size: 1 MB   Requirement: Required   PartitionKey A partition key with which to associate the record (see Record Metadata). Records with the same partition key are assigned to the same shard, subject to the following exceptions: if a shard ID is also provided for the record (see the Records ShardId request parameter), the record is assigned according to the shard ID, and PartitionKey is ignored. In addition, if you increase a stream's shard count after its creation (see UpdateStream), new records with a previously used partition key will be assigned either to the same shard that was previously used for this partition key or to a new shard. All records with the same partition key that are added to the stream after the shard-count change will be assigned to the same shard (be it the previously used shard or a new shard). When neither a Shard ID or a partition key is provided in the request, the platform's default shard-assignment algorithm is used. See also Stream Sharding and Partitioning.\n Type: String   Maximum Size: 256 bytes   Requirement: Optional   ShardId The ID of the shard to which to assign the record, as an integer between 0 and one less than the stream's shard count.  When both ShardId and PartitionKey are set, the record is assigned according to the shard ID, and PartitionKey is ignored. When neither a Shard ID or a partition key is provided in the request, the platform's default shard-assignment algorithm is used. See also Stream Sharding and Partitioning.\n Type: Number   Requirement: Optional      Response Response Data Syntax { \u0026#34;FailedRecordCount\u0026#34;: number, \u0026#34;Records\u0026#34;: [ { \u0026#34;ErrorCode\u0026#34;: number, \u0026#34;ErrorMessage\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;SequenceNumber\u0026#34;: number, \u0026#34;ShardId\u0026#34;: number } ] } Elements  FailedRecordCount The number of records whose submissions failed.\n Type: Number   Records An array of result objects for each submitted record, in the same order as the records appeared in the submission request.\n Type: Array of record JSON objects  For records whose submission succeeded, the record JSON object contains these elements:\n SequenceNumber The record's sequence number, which uniquely identifies the record within the shard. This ID number can be used in a sequence-based Seek operation to get the location of a specific record within a given stream shard.\n Type: Number   ShardId The ID of the shard to which the record was assigned.\n Type: Number    For records whose submission failed, the record JSON object contains these elements:\n ErrorCode A unique numeric error code.\n Type: Number   ErrorMessage An error message in the form of a unique error-code string — see Errors.\n Type: String      Errors In the event of an error, the response includes a JSON object with an ErrorCode element that contains a unique numeric error code, and an ErrorMessage element that contains one of the following API error messages: Error Message Description   InvalidArgumentException A provided request parameter is not valid for this request.    Permission denied The sender of the request does not have the required permissions to perform the operation.    ResourceIsnotStream The specified stream path does not point to a stream.   ResourceNotFoundException The specified resource does not exist.   ShardIDOutOfRangeException The specified shard does not exist in this stream.    Examples Add a new record with custom client-information metadata to a MyStream stream. Because the request does not specify a shard ID or a partition key for the record, the record's shard assignment will be determined by the platform (default):\nRequest  POST /mycontainer/MyDirectory/MyStream/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: PutRecords X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;Records\u0026#34;: [ { \u0026#34;ClientInfo\u0026#34;: \u0026#34;MTIzNA0K\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;ZGF0YQ0K\u0026#34; } ] }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/MyStream/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;PutRecords\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;Records\u0026#34;: [ { \u0026#34;ClientInfo\u0026#34;: \u0026#34;MTIzNA0K\u0026#34;, \u0026#34;Data\u0026#34;: \u0026#34;ZGF0YQ0K\u0026#34; } ] } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... { \u0026#34;FailedRecordCount\u0026#34;: 0, \u0026#34;Records\u0026#34;: [ { \u0026#34;SequenceNumber\u0026#34;: 13495, \u0026#34;ShardId\u0026#34;: 199 } ] } ","keywords":["PutRecords,","put","records,","streaming,","stream","records,","stream","producer,","stream","shards,","sharding,","shard","count,","stream","partitions,","partitioning,","partition","key,","UpdateStream,","stream","update,","record","metadata,","record","sequence","number,","ClientInfo,","FailedRecordCount,","PartitionKey,","SequenceNumber,","ShardId"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/putrecords/","title":"PutRecords"},{"content":" This section contains how-to guides for on-prem deployments of the MLOps Platform (\u0026quot;the platform\u0026quot;) on virtual machines (VMs) using the Proxmox Virtual Environment (Proxmox VE) virtualization platform.\n","keywords":["proxmox","ve","setup","how-to,","pve","setup","how-to,","proxmox","setup","how-to,","proxmox","ve","installation","how-to,","pve","installation","how-to,","proxmox","ve","how-to,","pve","how-to,","proxmox","how-to,","proxmox","ve","setup,","pve","setup","proxmox","setup,","proxmox","ve","installation,","pve","installation,","proxmox","installation,","proxmox","ve","configuration,","pve","configuration,","proxmox","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/","title":"Proxmox VE Deployment How-Tos"},{"content":" Overview This guide outlines the required steps for installing (deploying) an instance of the MLOps Platform (\u0026quot;the platform\u0026quot;) to virtual machines (VMs) using the Proxmox Virtual Environment virtualization platform (PVE). When you complete the procedure, you'll have a platform instance running on your PVE virtual environment. The installation is done by using the platform installer — Provazio.\n Provisioning (deployment) of the platform's node VMs is done by using a dedicated virtualization package, provided by Iguazio.Don't attempt to provision the servers yourself prior to the deployment. Platform shutdown should be coordinated with Iguazio's support team. Don't shut down the data-node VMs non gracefully, as this might erase the data.    Prerequisites Before you begin, ensure that you have the following:\n  A Provazio API key and a Provazio vault URL, received from Iguazio.\n  Administrative access to the platform's PVE cluster — an operational PVE environment with one or more servers running the Proxmox VE hypervisor version 6.x or 7.0 (\u0026quot;PVE hosts\u0026quot;).\n  A PVE platform virtualization package with VMA GZ files for each of the platform nodes, received from Iguazio.\n  The PVE hosts have data stores with a minimum of 400 GB available storage for each of the platform nodes (to be used for running the nodes' VM boot-disk images).\n  Sufficient dedicated physical resources on the PVE hosts to allow running the platform's node VMs without over-provisioning.\n  At least two 1 TB enterprise-grade SSDs for each of the platform's data-node VM.\n  The platform's PVE hosts have the following network interfaces:\n A single-port 1 Gb (minimum) NIC for the management network For hosting data-node VMs — a dual-port 10 Gb (minimum) NIC for the data-path (client) and interconnect networks For hosting application-node VMs only — a single-port 10 Gb (minimum) NIC for the data-path (client) network  Each network must be on a different subnet, and the management network's subnet must be able to accommodate allocation of IP addresses for each of the platform's node VMs.\n  User credentials for configuring management IP addresses, received from Iguazio.\n  A machine running Docker.\n  For installations without internet connectivity (\u0026quot;offline installation\u0026quot;) —\n A platform installation package (\u0026lt;build name\u0026gt;.tar.gz), received from Iguazio. A preloaded Provazio dashboard Docker image (gcr.io/iguazio/provazio-dashboard:stable), received from Iguazio as an image archive (provazio-latest.tar.gz).    Deployment Steps To deploy an instance of the platform to a PVE cluster, execute the following steps.\nStep 1: Configure virtual networking | Step 2: Deploy the platform nodes | Step 3: Attach data disks to the data nodes | Step 4: Configure management IP addresses | Step 5: Run the platform installer | Step 6: Access the installer dashboard | Step 7: Choose the VM scenario | Step 8: Configure general parameters | Step 9: Configure cluster parameters | Step 10: Configure node parameters | Step 11: Review the settings | Step 12: Wait for completion\nStep 1: Configure Virtual Networking Follow the Configuring Virtual Networking (PVE) guide to configure virtual networking on the platform's PVE cluster.\nStep 2: Deploy the Platform Nodes Follow the Deploying the Platform Nodes (PVE) guide to deploy a set of VMs that will serve as the platform's data and application nodes.\nStep 3: Attach Data Disks to the Data Nodes Follow the Attaching Data Disks to the Data-Node VMs (PVE) guide to attach data disks (storage devices) to the platform's data-node VMs.\nStep 4: Configure Management IP Addresses Follow the Configuring IP Addresses (PVE) guide to configure IP addresses for the management network (\u0026quot;management IP addresses\u0026quot;) on the platform nodes, to allow the installer to connect to the node VMs.\nStep 5: Run the Platform Installer Offline-Installation NoteTo deploy the platform in environments without an internet connection (offline deployment), follow the instructions in the offline-installation guide instead of the procedure in this step, and then proceed to the next installation step.  To deploy In platform in environments with an internet connection (online deployment), run the platform installer, Provazio, by running the following command from a command-line shell on a server or a machine that is running Docker and has connectivity to the platform's data and application nodes:\ndocker pull gcr.io/iguazio/provazio-dashboard:stable \u0026\u0026 docker run --rm --name provazio-dashboard \\ -p 8060:8060 \\ -e PROVAZIO_API_KEY=\u0026lt;Provazio API Key -e PROVAZIO_VAULT_URL=\u0026lt;Provazio Vault URL \\ gcr.io/iguazio/provazio-dashboard:stable     API Key A Provazio API key, received from Iguazio (see the installation prerequisites).  Vault URL A Provazio vault URL, received from Iguazio (see the installation prerequisites). NoteDon\u0026#39;t include a slash (`\u0026#39;/\u0026#39;`) at the end of the vault URL.    Step 6: Access the Installer Dashboard In a web browser, browse to localhost:8060 to view the Provazio dashboard.\n   Select the plus-sign button (+) to create a new system.\nStep 7: Choose the VM Scenario On the Installation Scenario page, check Bare metal / virtual machines, and then select Next.\n   Step 8: Configure General Parameters On the General page, fill in the configuration parameters, and then select Next.\n    System Name A system ID of your choice, which will be part of the URL of your platform instance.  Valid Values: A string of 1–12 characters; can contain lowercase letters (a–z) and hyphens (-); must begin with a lowercase letter   Default Value: A randomly generated lowercase string   Description  A free-text string that describes the platform instance.  System Version  The platform version.  For online installations \u0026mdash; insert the release build number that you received from Iguazio (for example, \"3.2.0-b19.20211107120205\"). The installer will implicitly download the appropriate installation package for the specified build version.  For offline installations (in environments without an internet connection) \u0026mdash; set the value of this parameter to \"file://igzconf:igzconf@\u0026lt;IP of the installation-package data node\u0026gt;:/home/iguazio/installer/\u0026lt;platform version, as received from Iguazio\u0026gt;\"; replace the \u0026lt;...\u0026gt; placeholders with your specific data. Note that you first need to create a /home/iguazio/installer data-node directory and extract to this directory the platform installation-package archive that you received from Iguazio, as outlined in the offline-installation guide, which should have been executed in Step 5.    Owner Full Name An owner-name string, containing the full name of the platform owner, for bookkeeping.  Owner Email An owner-email string, containing the email address of the platform owner, for bookkeeping.  Username Username for a platform user to be created by the installation. This username will be used to log into the platform dashboard. You can add additional users after the platform is provisioned.  User Password Platform password for the user generated by the installation \u0026mdash; to be used with the configured username to log into platform dashboard; see the password restrictions. You can change this password after the platform is provisioned.  System Domain Custom domain (for example, \"customer.com\"). The installer prepends the value of the System Name parameter to this value to create the full system domain.  Termination Protection The protection level for terminating the platform installation from the installer dashboard.   Step 9: Configure Cluster Parameters On the Clusters page, fill in the configuration parameters, and then select Next.\n   Common Parameters (Data and Application Clusters) The following parameters are set for both the data and application clusters. Node references in the parameter descriptions apply to the platform's data-node VMs for the data cluster and the application-node VMs for the application cluster.\n Hypervisor  The hypervisor running on the cluster's hypervisor host machine. For PVE, select KVM.  # of Cores  The number of CPU cores to allocate for each node.  Valid Values: 8 or 16   Memory (GB)  The amount of RAM to allocate for each node.  Valid Values: 122 or 244    Data-Cluster Parameters The following parameters are applicable only to the platform's data cluster:\n Dashboard Virtual IP Address  An IP address that will be used internally to load-balance access to the platform dashboard. Choose an available address from the subnet of the client network. NoteThis parameter isn\u0026#39;t required for platforms with a single data node.   Storage Devices  The names of the block-storage devices (data disks) that are attached to the data nodes, as configured in Step 3.   Application-Cluster Parameters The following parameters are applicable only to the platform's application cluster:\n Kubernetes Kind  Leave this set to New Vanilla Cluster (Iguazio Managed).  API Server Virtual IP Address  An IP address that will be used internally to load-balance access to the API server of the Kubernetes cluster. Choose an available address from the subnet of the client network. NoteThis parameter isn\u0026#39;t required for platforms with a single application node.   GPU Support  Check this option to configure GPU support for the application cluster. NoteThis option is applicable only when there are GPUs attached to the application nodes.    Step 10: Configure Node Parameters On the Nodes page, fill in the configuration parameters, and then select Next.\n   The configuration includes\n Adding nodes Configuring the nodes' IP addresses  Add Nodes Select Add Nodes to display the Add Nodes window. Configure the new-node parameters in this window, and then select Add.\n     # of Data Nodes The number of data nodes in the platform's data cluster; must be at least 3 to support high availability (HA).  Valid Values: 1 or 3   # of App Nodes The number of application nodes in the platform's application cluster; must be at least 3 to support high availability (HA).  Client Network Prefix The subnet of the data-path (client) network. Either use the default value or specify a custom private subnet.  Data Management Interface Name Leave this set to eth0.  App Management Interface Name Leave this set to eth0.  Interconnect Network Prefix The subnet of the interconnect network. Either use the default value or specify a custom private subnet.  Management Network MTU Leave this set to 1500.  Client Network MTU Leave this set to 1500.  Interconnect Network MTU Leave this set to 1500.   Configure the Nodes' IP Addresses On the Nodes page, for each node that you added, select the adjacent edit icon (), enter the node's management IP address, and select Save.\nStep 11: Review the Settings On the Review page, review and verify your configuration; go back and make edits, as needed; and then select Create to provision a new instance of the platform.\n   Step 12: Wait for Completion Provisioning a new platform instance typically takes around 30–40 minutes, regardless of the cluster sizes. You can download the provisioning logs, at any stage, by selecting Download logs from the instance's action menu.\n   You can also follow the installation progress by tracking the Provazio Docker container logs.\nWhen the installation completes, you should have a running instance of the platform on your PVE cluster. You can use the Provazio dashboard to view the installed nodes (VMs). Then, proceed to the post-deployment steps.\nPost-Deployment Steps When the deployment completes, follow the post-deployment steps.\nSee Also  PVE Deployment How-Tos On-Prem Deployment Specifications  ","keywords":["proxmox","ve","installation,","pve","installation,","proxmox","installation,","proxmox","virtual","environment","installation,","proxmox","ve,","pve,","proxmox,","proxmox","virtual","environment,","proxmos","ve","deployment,","pve","deployment,","proxmos","deployment,","proxmox","virtual","environment","deployment,","provazio,","platform","installer"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/proxmox-installation-guide/","title":"Installing the Platform on Proxmox VE VMs"},{"content":" Overview Installing (deploying) the platform on virtual machines (VMs) in environments without an internet connection (a.k.a. \u0026quot;dark sites\u0026quot;) requires a variation on the online procedure that's described in the PVE Installation Guide, as outlined in this guide.\nPrerequisites Before you begin, ensure that you have the following:\n A platform installation package (\u0026lt;build name\u0026gt;.tar.gz), received from Iguazio. A preloaded Provazio dashboard Docker image (gcr.io/iguazio/provazio-dashboard:stable), received from Iguazio as an image archive (provazio-latest.tar.gz). A Provazio dashboard API key, received from Iguazio. Administrative access to a platform PVE cluster with the required networks configuration (see Configuring Virtual Networking (PVE) and Configuring IP Addresses (PVE)) and deployed VMs for each of the platform nodes (see Deploying the Platform Nodes (PVE)).  Run The Platform Installer Offline Execute the following steps to run the platform installer (Provazio) without internet connectivity (offline installation):\n  Copy and extract the installation package:\n Establish an SSH connection to one of the platform's data-node VMs. Create an installer directory under /home/iguazio ($HOME): Copy the \u0026lt;build name\u0026gt;.tar.gz installation-package archive (see the prerequisites) to the new /home/iguazio/installer directory. Extract the package archive to the installer directory.    Copy and extract the Provazio dashboard Docker image:\n  Copy the provazio-latest.tar.gz preloaded Provazio dashboard Docker-image archive (see the prerequisites) to the installation machine that is running Docker.\n  Extract the provazio-latest.tar.gz Docker-image archive by running the following command on the machine that is running Docker:\ndocker load -i The output for a successful execution should look like this:\nLoaded image: gcr.io/iguazio/provazio-dashboard:stable     Run the platform installer, Provazio, by running the following command from a command-line shell; replace the \u0026lt;Provazio API Key\u0026gt; placeholder with the Provazio API key that you received from Iguazio:\ndocker run --rm --name provazio-dashboard \\ -p 8060:8060 \\ -e PROVAZIO_API_KEY=\u0026lt;Provazio API Key \\ \u0026#8201;gcr.io/iguazio/provazio-dashboard:stable    What's Next? After successfully running the platform installer, proceed to the installer dashboard-access step in the PVE installation guide to configure installation parameters from the installer dashboard.\n","keywords":["offline","installation","on","proxmox","ve","vms,","proxmox","ve","offline","installation,","pve","offline","installation,","proxmox","offline","installation,","vm","offline","installation,","pve","offline","deployment,","proxmox","offline","deployment,","vm","offline","deployment,","dark-site","installation,","dark","site,","no","internet,","provazio"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/offline-install/","title":"Installing the Platform without an Internet Connection (Offline Installation) (PVE)"},{"content":"Overview After following the PVE Installation Guide, you should have a running instance of the platform on your PVE cluster. In some cases, additional post-deployment steps are required before you can properly use the platform, as outlined in this guide.\nRegistering a Custom Platform Domain If you chose to install the platform under a custom domain, you must register a few DNS records. If you need assistance, contact Iguazio's support team. Creating an HTTPS Certificate In some cases, you might need to create an HTTPS certificate for your platform installation. For more information, contact Iguazio's support team. Importing IdP Users and Groups from an Active Directory To import users and groups from an external Microsoft Active Directory, see the platform's IdP documentation. For additional assistance, contact Iguazio's support team. See Also  PVE Installation Guide  ","keywords":["on-prem","post","deployment,","vm","post","deployment,","vm","post","installation,","custom","domain","registration,","domain","registration,","ip","addresses,","network,","dns,","idp,","microsoft","active","directory,","active","directory,","microsoft","ad,","idp","users,","http","certificates"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/proxmox/howto/post-install-steps/","title":"Post-Installation Steps (PVE)"},{"content":" The platform's Jupyter Notebook service pre-deploys the pandas open-source Python library for high-performance data processing using structured DataFrames (\u0026quot;pandas DataFrames\u0026quot;). The platform also pre-deploys other Python packages that utilize pandas DataFrames, such the Dask parallel-computation library or Iguazio's V3IO Python SDK and V3IO Frames libraries. You can easily install additional Python machine-learning (ML) and scientific-computation packages — such as TensorFlow, Keras, scikit-learn, PyTorch, Pyplot, and NumPy. The platform's architecture was designed to deploy computation to one or more CPU or GPU with a single Python API.\nFor example, you can install the TensorFlow open-source library for numerical computation using data-flow graphs. You can use TensorFlow to train a logistic regression model for prediction or a deep-learning model, and then deploy the same model in production over the same platform instance as part of your operational pipeline. The data science and training portion can be developed using recent field data, while the development-to-production workflow is automated and time to insights is significantly reduced. All the required functionality is available on a single platform with enterprise-grade security and a fine-grained access policy, providing you with visibility into the data based on the organizational needs of each team. The following Python code sample demonstrates the simplicity of using the platform to train a TensorFlow model and evaluate the quality of the model's predictions:\nmodel.train( input_fn=lambda: input_fn(train_data, num_epochs, True, batch_size)) results = model.evaluate(input_fn=lambda: input_fn( test_data, 1, False, batch_size)) for key in sorted(results): print(\u0026#39;%s: %s\u0026#39; % (key, results[key])) The image-classification-with-distributed-training demo demonstrates how to build an image recognition and classification ML model and perform distributed model training by using Horovod, Keras, TensorFlow, and Nuclio.\nSee Also  Working with Services The Jupyter Notebook Service Dask The MPI-Operator Horovod Service Running Applications over GPUs Data Science Automation (MLOps) Services  ","keywords":["machine-learning","and","scientific","computation","packages,","ML","and","scientific-computation","packages,","machine-learning","packages,","ML","packages,","scientific-programming","packages,","machine","learning,","ML,","deep","learning,","model","training,","scientific","computation,","scientific","computation,","computation,","keras,","pyplot,","pytorch,","numpy,","scikit-learn,","sklearn,","tensorflow,","python","libraries,","python","apis,","python,","cpu,","gpu,","jupyter,","jupyter","notebook,","installation,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/python-ds-pkgs/","title":"Python Machine-Learning and Scientific-Computation Packages"},{"content":" Description Reads (consumes) item attributes from a NoSQL table into pandas DataFrames.\nNoteBy default, the method returns all user attributes (columns) of all the items (rows) in the specified NoSQL table. You can optionally restrict the read query by setting the filter parameter to a filter expression that restricts which items to read and/or setting the columns parameter to a list of item attributes to return for the read items (as restricted by the filter expression, if set).\n  Syntax read(backend, table=\u0026#39;\u0026#39;[, columns=None, filter=\u0026#39;\u0026#39;, max_rows_in_msg=0, iterator=False, **kw]) The following syntax statement replaces the kw parameter with the additional keyword arguments that can be passed for the NoSQL backend via this parameter:\nread(backend, table=\u0026#39;\u0026#39;[, columns=None, filter=\u0026#39;\u0026#39;, max_rows_in_msg=0, iterator=False, reset_index, sharding_keys, sort_key_range_start, sort_key_range_end])  NoteThe method has additional parameters that aren't currently supported for the NoSQL backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after table.\n  Parameters backend | columns | filter | kw | max_rows_in_msg | reset_index (kw argument) | sharding_keys (kw argument) | sort_key_range_start (kw argument) | sort_key_range_end (kw argument) | table\n backend The backend type — \u0026quot;nosql\u0026quot; or \u0026quot;kv\u0026quot; for the NoSQL backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a NoSQL table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/nosql/my_table\u0026quot;.\n Type: str   Requirement: Required   iterator Determines whether to return a pandas DataFrames iterator or a single DataFrame: True — return a DataFrames iterator; False (default) — return a single DataFrame.\n Type: bool   Requirement: Optional   Valid Values: True | False   Default Value: False (return a single DataFrame)   filter A platform filter expression that restricts which items (rows) to read. For example, \u0026quot;vendor=='Honda\u0026quot; AND color=='red'\u0026quot;. Only attributes for items that match the filter criteria are returned. See Filter Expression for syntax details and examples.\nNoteYou can use the columns parameter to specify which attributes to return for the items that match the filter expression. For more information, see the notes in the method description.\n   Type: str   Requirement: Optional   columns A list of item attributes (columns) to return. For example, [\u0026quot;vendor\u0026quot;, \u0026quot;color\u0026quot;, \u0026quot;km\u0026quot;]. By default, the method returns all of the items' user attributes.\nNoteYou can optionally use the filter parameter to restrict the items for which to return attributes. For more information, see the notes in the method description.\n   Type: []str   Requirement: Optional   max_rows_in_msg The maximum number of table items (rows) to read in each message (i.e., the size of the read chunks).  When read returns a DataFrames iterator (when the iterator parameter is set to True), the size each of the returned DataFrames is the configured message size.\n Type: int   Requirement: Optional   Default Value: 256   kw This parameter is used for passing a variable-length list of additional keyword (named) arguments. See the following kw Arguments section for a list of additional arguments that are supported for the NoSQL backend via the kw parameter.\n Type: ** — variable-length keyword arguments list   Requirement: Optional    kw Arguments The NoSQL backend supports the following read arguments via the kw parameter for passing a variable-length list of additional keyword arguments:\n reset_index Determines whether to reset the index index column of the returned DataFrame: True — reset the index column by setting it to the auto-generated pandas range-index column; False (default) — set the index column to the table's primary-key attribute.\n Type: bool   Requirement: Optional   Valid Values: True | False   Default Value: False   sharding_keys [Tech Preview] A list of item sharding-key values to query to get by using a range scan. The sharding-key value is the part to the left of the leftmost period in a compound primary-key value (item name). For example, [\u0026quot;january\u0026quot;, \u0026quot;february\u0026quot;, \u0026quot;march\u0026quot;]. You can optionally use the sort_key_range_start and/or sort_key_range_end keyword arguments to restrict the search to a specific range of sorting keys (sort_key_range_start \u0026gt;= \u0026lt;sorting key\u0026gt; \u0026lt; sort_key_range_end).\n Type: []str   Requirement: Optional   sort_key_range_start [Tech Preview] The minimal sorting-key value of the items to get by using a range scan. The sorting-key value is the part to the right of the leftmost period in a compound primary-key value (item name). This argument is applicable only together with the sharding_keys keyword argument. The scan will return all items with the specified sharding-key values whose sorting-key values are greater than or equal to (\u0026gt;=) than the value of the sort_key_range_start argument and less than (\u0026lt;) the value of the sort_key_range_end argument (if set).\n Type: str   Requirement: Optional   sort_key_range_end [Tech Preview] The maximal sorting-key value of the items to get by using a range scan. The sorting-key value is the part to the right of the leftmost period in a compound primary-key value (item name). This argument is applicable only together with the sharding_keys keyword argument. The scan will return all items with the specified sharding-key values whose sorting-key values are greater than or equal to (\u0026gt;=) the value of the sort_key_range_start argument (if set) and less than (\u0026lt;) the value of the sort_key_range_end argument.\n Type: str   Requirement: Optional    Return Value  When the value of the iterator parameter is True — returns a pandas DataFrames iterator. When the value of the iterator parameter is False (default) — returns a single pandas DataFrame.   Examples Following are some usage examples for the read method of the Frames NoSQL backend:\n  Read all items (rows) of a mytable table in the client's data container (table) into a DataFrames iterator (iterator = True):\ntable = \u0026#34;mytable\u0026#34; dfs = client.read(backend=\u0026#34;nosql\u0026#34;, table=table, iterator=\u0026#34;True\u0026#34;) for df in dfs: display(df)   Read from a mytable into a single DataFrame (default iterator value = False). Return only the specified item attributes (columns) for items whose age attribute value is greater or equal to 11 and less than 15 (filter).\ntable = \u0026#34;mytable\u0026#34; df = client.read(\u0026#34;nosql\u0026#34;, table=table, filter=\u0026#34;age\u0026gt;=11 AND age\u0026lt;15\u0026#34;, columns=[\u0026#34;username\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;]) display(df)   Read from a my_tables/students table in the client's data container (table) into a single DataFrame (default iterator value = False) and reset the index of the returned DataFrame (reset_index = True). Return only items whose age attribute value is greater than 10 and whose country attribute value is either \u0026quot;US\u0026quot; or \u0026quot;UK\u0026quot; (filter).\ntable = \u0026#34;/my_tables/students_11-15\u0026#34; df = client.read(\u0026#34;nosql\u0026#34;, table=table, reset_index=True, filter=\u0026#34;age \u0026gt; 10 AND country IN (\u0026#39;US\u0026#39;, \u0026#39;UK\u0026#39;)\u0026#34;) display(df)   See Also  Frames NoSQL-Backend Overview Frames Client Constructor Object Names and Primary Keys  ","keywords":["read","method,","frames","nosql","read","method,","frames","kv","read","method,","frames","read,","frames","nosql","read,","frames","kv","read,","frames","client","read,","frames","client","nosql","read,","frames","client","kv","read,","frames","read","reference,","frames","nosql","read","reference,","frames","kv","read","reference,","nosql","ingestion,","data","ingestion,","sharding","keys,","pandas","DataFrames,","DataFrames","index","columns,","primary","key","attribute,","item","names,","backend,","columns,","filter,","kw,","max_rows_in_msg,","reset_index,","sharding_keys,","table"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/nosql/read/","title":"read Method"},{"content":" Description Reads (consumes) data from a TSDB table into pandas DataFrames.\nAggregation Queries A TSDB query can include aggregation functions (\u0026quot;aggregators\u0026quot;) to apply to the sample metrics; for a list of the supported aggregation functions, see the description of the aggregators parameter. To create an aggregation query, set the read method's aggregators parameter to a list of aggregation functions. Each function is applied to all metrics configured for the query (see the columns parameter).\nThe Frames TSDB backend currently supports \u0026quot;over-time aggregation\u0026quot;, which aggregates the data for unique metric label sets over time, and returns a separate aggregation time series for each label set.\nThe aggregation is done at each aggregation step (a.k.a., aggregation interval) — the time interval for executing the aggregation functions over the query's time range; the step determines the aggregation data points, starting at the query's start time. The default step is the query's time range (which can be configured via the start and end parameters). You can override the default aggregation step by setting the step parameter.\nThe aggregation is applied to all sample data within the query's aggregation window, which currently always equals the query's aggregation step. For example, for an aggregation step of 1 hour, the aggregation at step 10:00 is done for an aggregation window of 10:00–11:00.\nPre-Aggregation NoteWhen creating a TSDB table, you can optionally configure pre-aggregates that will be calculated for all metric samples as part of their ingestion into the TSDB table. For each aggregation request in an over-time aggregation query, if the TSDB table has matching pre-aggregated data (same aggregation function and the query's aggregation window is a sufficient multiplier of the table's aggregation granularity), the pre-aggregated data is used instead of performing a new aggregation calculation, which speeds up the query processing. For more information about pre-aggregation and how to configure it, see the description of the create method's aggregates argument.\n  Syntax read(backend[, table=\u0026#39;\u0026#39;, columns=None, filter=\u0026#39;\u0026#39;, max_rows_in_msg=0, iterator=False, **kw]) The following syntax statement replaces the kw parameter with the additional keyword arguments that can be passed for the TSDB backend via this parameter:\nread(backend[, table=\u0026#39;\u0026#39;, columns=None, filter=\u0026#39;\u0026#39;, max_rows_in_msg=0, iterator=False, start, end, aggregators, step, multi_index])  NoteThe method has additional parameters that aren't currently supported for the TSDB backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after table.\n  Parameters aggregators (kw argument) | backend | columns | end (kw argument) | filter | kw | multi_index (kw argument) | start (kw argument) | step (kw argument) | table\n backend The backend type — \u0026quot;tsdb\u0026quot; for the TSDB backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a TSDB table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/tsdb/my_metrics\u0026quot;.\n Type: str   Requirement: Required   iterator Determines whether to return a pandas DataFrames iterator or a single DataFrame: True — return a DataFrames iterator; False (default) — return a single DataFrame.\n Type: bool   Requirement: Optional   Valid Values: True | False   Default Value: False (return a single DataFrame)   columns A list of metric names to which to apply the query. For example, [\u0026quot;cpu\u0026quot;, \u0026quot;temperature\u0026quot;, \u0026quot;disk\u0026quot;]. By default, the query is applied to all metrics in the TSDB table.\nNote Queries with multiple metric names is currently supported only as Tech Preview. You can restrict the metrics list for the query within the query filter, as explained for filter parameter.     Type: []str   Requirement: Optional   filter A platform filter expression that restricts the information that will be returned. See Filter Expression for syntax details and examples.  The filter is typically applied to metric labels; for example, \u0026quot;os=='linux' AND arch=='amd64'\u0026quot;.  You can also apply the filter to the _name attribute, which stores the metric name. This is less efficient than specifying the metric names in the columns parameter, but it might be useful in some cases. For example, if you have many \u0026quot;cpu\u0026lt;n\u0026gt;\u0026quot; metrics, you can use \u0026quot;starts(_name,'cpu')\u0026quot; in your filter expression to apply the query to all metrics (or all metrics specified in the columns parameter, if set) whose names begin with the string \u0026quot;cpu\u0026quot;.\nNoteCurrently, only labels of type string are supported; see the Software Specifications and Restrictions. Therefore, ensure that you embed label attribute values in your filter expression within quotation marks even when the values represent a number (for example, \u0026quot;node == '1'\u0026quot;), and don't apply arithmetic operators to such attributes (unless you want to perform a lexicographic string comparison).   Type: str   Requirement: Optional   kw This parameter is used for passing a variable-length list of additional keyword (named) arguments. See the following kw Arguments section for a list of additional arguments that are supported for the TSDB backend via the kw parameter.\n Type: ** — variable-length keyword arguments list   Requirement: Optional    kw Arguments The TSDB backend supports the following read arguments via the kw parameter for passing a variable-length list of additional keyword arguments:\n start The query's start time — the earliest sample time to query: read only items whose data sample time is at or after (\u0026gt;=) the specified start time.\n Type: str   Requirement: Optional   Valid Values: A string containing an RFC 3339 time, a Unix timestamp in milliseconds, a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where m = minutes, h = hours, and 'd' = days), or 0 for the earliest time. For example: \u0026quot;2016-01-02T15:34:26Z\u0026quot;; \u0026quot;1451748866\u0026quot;; \u0026quot;now-90m\u0026quot;; \u0026quot;0\u0026quot;.   Default Value: \u0026lt;end time\u0026gt; - 1h   end The query's end time — the latest sample time to query: read only items whose data sample time is before or at (\u0026lt;=) the specified end time.\n Type: str   Requirement: Optional   Valid Values: A string containing an RFC 3339 time, a Unix timestamp in milliseconds, a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where m = minutes, h = hours, and 'd' = days), or 0 for the earliest time. For example: \u0026quot;2018-09-26T14:10:20Z\u0026quot;; \u0026quot;1537971006000\u0026quot;; \u0026quot;now-3h\u0026quot;; \u0026quot;now-7d\u0026quot;.   Default Value: now   aggregators A list of aggregation functions (\u0026quot;aggregators\u0026quot;) to apply to the raw sample data of the configured query metrics (see the columns parameter) in order to perform an aggregation query. You can configure the aggregation step, which serves also as the aggregation window, in the step parameter.\n Type: str   Requirement: Optional   Valid Values: A string containing a comma-separated list of supported aggregation functions (\u0026quot;aggregators\u0026quot;); for example, \u0026quot;count,avg,min,max\u0026quot;. The following aggregation functions are supported: \navg — the average of the sample values. count — the number of ingested samples. last — the value of the last sample (i.e., the sample with the latest time). max — the maximal sample value. min — the minimal sample value. rate — the change rate of the sample values, which is calculated as \u0026lt;last sample value of the previous interval\u0026gt; - \u0026lt;last sample value of the current interval\u0026gt;) / \u0026lt;aggregation granularity\u0026gt;. stddev — the standard deviance of the sample values. stdvar — the standard variance of the sample values. sum — the sum of the sample values.     step The query step (interval), which determines the points over the query's time range at which to perform aggregations (for an aggregation query) or downsample the data (for a query without aggregators). The default step is the query's time range, which can be configured via the start and end parameters. In the current release, the aggregation step is also the aggregation window to which the aggregators are applies. For more information, see Aggregation Queries.\n Type: str   Requirement: Optional   Valid Values: A string of the format \u0026quot;[0-9]+[mhd]\u0026quot; — where 'm' = minutes, 'h' = hours, and 'd' = days. For example, \u0026quot;30m\u0026quot; (30 minutes), \u0026quot;2h\u0026quot; (2 hours), or \u0026quot;1d\u0026quot; (1 day).   multi_index Determines the indexing of the returned DataFrames: True — return a multi-index DataFrame in which all metric-label attributes are defined as index columns in addition to the metric sample-time attribute (the primary-key attribute); False (default) — return a single-index DataFrame in which only the metric sample-time attribute is defined as an index column.\n Type: bool   Requirement: Optional   Default Value: False (return a single-index DataFrame)    Return Value  When the value of the iterator parameter is True — returns a pandas DataFrames iterator. When the value of the iterator parameter is False (default) — returns a single pandas DataFrame.   Examples Following are some usage examples for the read method of the Frames TSDB backend. All of the examples set the read method's multi_index parameter to True to display metric-label attributes as index columns (in addition to the sample-time attribute, which is always displayed as an index column). Except where otherwise specified, the examples return a single DataFrame (default iterator value = False).\n  Read all items (rows) of a mytsdb table in the client's data container (table) — start = \u0026quot;0\u0026quot; and default end (\u0026quot;now\u0026quot;) and columns (all metrics):\ntsdb_table = \u0026#34;mytsdb\u0026#34; df = client.read(backend=\u0026#34;tsdb\u0026#34;, table=tsdb_table, start=\u0026#34;0\u0026#34;, multi_index=True) display(df.head()) display(df.tail())   Issue an aggregation query (aggregators) to a mytsdb table in the client's data container (table) for the \u0026quot;cpu\u0026quot; metric (columns); use the default aggregation step (step not set), which is the query's time range — 09:00–17:00 on 1 Jan 2019 (see start and end):\ntsdb_table = \u0026#34;mytsdb\u0026#34; df = client.read(\u0026#34;tsdb\u0026#34;, table=tsdb_table, start=\u0026#34;2019-01-01T09:00:00Z\u0026#34;, end=\u0026#34;2019-01-01T17:00:00Z\u0026#34;, columns=[\u0026#34;cpu\u0026#34;], aggregators=\u0026#34;avg,min,max\u0026#34;, multi_index=True) display(df)    Issue an aggregation query to a tsdb/my_metrics table in the client's data container (table) for the previous two days (start = \u0026quot;now-2d\u0026quot; and end = \u0026quot;now-1d\u0026quot;); apply the sum and avg aggregators (aggregators) to the \u0026quot;disk\u0026quot; and \u0026quot;cpu\u0026quot; metrics (columns) with a 12-hours aggregation step (step), and only apply the query to samples with a \u0026quot;linux\u0026quot; os label (filter = os=='linux).\ntsdb_table = \u0026#34;/tsdb/my_metrics\u0026#34; df = client.read(\u0026#34;tsdb\u0026#34;, table=tsdb_table, columns=[\u0026#34;disk\u0026#34;, \u0026#34;memory\u0026#34;], filter=\u0026#34;os==\u0026#39;linux\u0026#39;\u0026#34;, aggregators=\u0026#34;sum,avg\u0026#34;, step=\u0026#34;12h\u0026#34;, start=\u0026#34;now-2d\u0026#34;, end=\u0026#34;now-1d\u0026#34;, multi_index=True) display(df)    Issue a 1-hour raw-data downsampling query (step = \u0026quot;1h\u0026quot; and aggregators not set) to a mytsdb table in the client's data container (table); apply the query to all metric samples (default columns) from 1 Jan 2019 (start = \u0026quot;2019-01-01T00:00:00Z\u0026quot; and end = \u0026quot;2019-02-01T00:00:00Z\u0026quot;):\ntsdb_table = \u0026#34;mytsdb\u0026#34; df = client.read(\u0026#34;tsdb\u0026#34;, table=tsdb_table, start=\u0026#34;2019-01-01T00:00:00Z\u0026#34;, end=\u0026#34;2019-02-01T00:00:00Z\u0026#34;, step=\u0026#34;1h\u0026#34;, multi_index=True) display(df)   See Also  Querying a TSDB (The TSDB CLI) Frames TSDB-Backend Overview Frames Client Constructor  ","keywords":["read","method,","frames","tsdb","read","method,","frames","read,","frames","tsdb","read,","frames","client","read,","frames","client","tsdb","read,","frames","read","reference,","frames","tsdb","read","reference,","tsdb","ingestion,","data","ingestion,","TSDB","metrics,","TSDB","labels,","pandas","DataFrames,","DataFrames","index","columns,","aggregation,","TSDB","aggregation,","aggregation","functions,","aggregation","step,","aggregation","interval,","aggregation","window,","pre-aggregation,","label","aggregation,","interpolation,","interpolation","functions,","interpolation","tolerance,","aggregators,","aggregation_window,","backend,","columns,","end,","filter,","kw,","multi_index,","query,","start,","step,","table,","next,","prev,","linear,","none"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/tsdb/read/","title":"read Method"},{"content":"Fully Qualified Name org.apache.spark.streaming.v3io.RecordAndMetadata\nDescription A case class that represents a platform stream record. The class exposes methods for extracting decoded record data and related metadata.\nSummary Instance Constructors\ncase class RecordAndMetadata[V]( topic: String, partitionId: Short, private val rec: ConsumerRecord, valueDecoder: Decoder[V]) extends Encoding with Logging Methods   payload\ndef payload(): V   payloadWithMetadata\ndef payloadWithMetadata(): PayloadWithMetadata[V]   Instance Constructors Syntax RecordAndMetadata[V]( topic: String, partitionId: Short, private val rec: ConsumerRecord, valueDecoder: Decoder[V]) Parameters and Data Members  topic The name of the stream that contains the record.\n Type: String   Requirement: Required   partitionId The ID of the stream shard that contains the record (see Stream Sharding and Partitioning).\n Type: Short   Requirement: Required   valueDecoder A decoder instance for converting the record's data into the desired type — the value type of the current class instance (see the V type parameter).\n Type: Decoder[V]   Requirement: Required    Type Parameters  V \u0026mdash;  the data type into which to convert the record data (\u0026quot;value\u0026quot;).  payload Method Returns decoded record data (\u0026quot;payload\u0026quot;), in the specified format.\nSyntax payload(): V ParametersNone\nReturn Value Returns the decoded record data, formatted as the value type of the current class instance (see the constructor's V type parameter).\npayloadWithMetadata Method Returns decoded record data (\u0026quot;payload\u0026quot;), in the specified format, and related metadata.\nSyntax payloadWithMetadata(): PayloadWithMetadata[V] ParametersNone\nReturn Value Returns a PayloadWithMetadata object that contains the decoded record data and related metadata.\n","keywords":["RecordAndMetadata,","payload,","payloadWithMetadata,","spark","streaming,","stream","records,","record","metadata,","record","payload,","decoding,","decoder,","stream","shards,","shards,","stream","partitions,","partitionId,","topic,","valueDecoder"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/recordandmetadata/","title":"RecordAndMetadata Class"},{"content":"Review the product release notes for the MLOps Platform (\u0026quot;the platform\u0026quot;).\nConfidentialityThe documentation in this section is confidential under the Iguazio End User License Agreement. For more information, contact support@iguazio.com.  ","keywords":["release","notes,","version","history,","known","issues,","restrictions,","limitations,","key","features,","new","features,","enhancements,","deprecated"],"path":"https://github.com/jasonnIguazio/release-notes/","title":"Release Notes"},{"content":" Overview This reference lists reserved names in the platform's data=layer APIs. Don't use these names for your custom components such as for attribute names. For additional restrictions, see the Software Specifications and Restrictions. NoteThe reserved names are case insensitive. For example, for the reserved keyword set, also avoid using SET or Set.  Expressions All expression keywords, including the following, are reserved names in the platform:\n Logical operators Update-expression keywords  Logical Operators  and in or out  NoteFor more information, see the Logical Operators expressions reference.  Update-Expression Keywords  remove set  NoteFor more information, see the Update Expression reference.  Frames The V3IO Frames API uses these reserved keywords:\n idx idx-0 raw_data seq_number stream_time  In addition, when using the Frames TSDB backend, time is reserved for the name of the sample ingestion-time attribute and cannot be used as a label name. System Attributes For a list of reserved predefined platform system attributes, see the System-Attributes Reference.\nSee Also  Software Specifications and Restrictions Expressions Reference Frames API Reference System-Attributes Reference  ","keywords":["reference,","reserved","names,","reserved","keywords,","keywords,","reserved","attribute","names,","attribute","names,","attributes,","expressions,","v3io","frames,","frames,","system","attributes"],"path":"https://github.com/jasonnIguazio/data-layer/reference/reserved-names/","title":"Reserved Names"},{"content":" Overview The platform supports accelerated code execution over NVIDIA graphics processing units (GPUs):\n  You can run Nuclio serverless functions on GPUs.\n  You can run GPU applications that use one of the following supported GPU libraries from a platform Jupyter Notebook service with the GPU flavor:\n Horovod RAPIDS    Horovod The platform has a default (pre-deployed) shared single-instance tenant-wide Kubeflow MPI Operator service (mpi-operator), which facilitates Uber's Horovod distributed deep-learning framework. Horovod, which is already preinstalled as part of the platform's Jupyter Notebook service, is widely used for creating machine-learning models that are trained simultaneously over multiple GPUs or CPUs.\nYou can use Horovod to convert a single-GPU TensorFlow, Keras, or PyTorch model-training program to a distributed multi-GPU program. The objective is to speed up your model training with minimal changes to your existing single-GPU code and without complicating the execution. Note that you can also run Horovod code over CPUs with just minor modification. For an example of using Horovod on the platform, see the image-classification-with-distributed-training demo.\n Note To run Horovod code, ensure that the mpi-operator platform service is enabled. (This service is enabled by default.) Horovod applications allocate GPUs dynamically from among the available GPUs in the system; they don't use the GPU resources of the parent Jupyter Notebook service. See also the Jupyter GPU resources note.    RAPIDS You can use NVIDIA's RAPIDS open-source libraries suite to execute end-to-end data science and analytics pipelines entirely on GPUs.\nTo use the cuDF and cuML RAPIDS libraries, you need to create a RAPIDS Conda environment. For example, you can run the following command from a Jupyter notebook or terminal to create a RAPIDS Conda environment named rapids:\nconda create -n rapids -c rapidsai -c nvidia -c anaconda -c conda-forge -c defaults ipykernel rapids=0.17 python=3.7) cudatoolkit=11.0 For more information about using Conda to create Python virtual environments, see the platform' virtual-env.ipynb tutorial Jupyter notebook.\nFor a comparison of performance benchmarks using the cuDF RAPIDS GPU DataFrame library and pandas DataFrames, see the gpu-cudf-vs-pd.ipynb tutorial notebook.\nNote  RAPIDS supports GPUs with the NVIDIA Pascal architecture or better and compute capability 6.0+.\n  RAPIDS applications use the GPU resource of the parent Jupyter Notebook service. Therefore, you must configure at least one GPU resource for this service: from the dashboard Services page, select to edit your Jupyter Notebook service, select the Common Parameters tab, and set the Resources | GPU | Limit field to a value greater than zero. See also the Jupyter GPU resources note.\n    For more information about using RAPIDS to run applications over GPUs, see Ingesting and Preparing Data.\nJupyter GPU Resources Note In environments with GPUs, you can use the common Resources | GPU | Limit parameter of the Jupyter Notebook service to guarantee the configured number of GPUs for use by each service replica. In addition, you can enable scale to zero for a Jupyter Notebook service to automatically free up resources, including GPUs, when the service becomes idle, by checking the Enabled check box for the common Scale to zero parameter. When configuring your Jupyter Notebook service, take the following into account: while the Jupyter Notebook service is enabled and not scaled to zero, it monopolizes the configured amount of GPUs even when the GPUs aren't in use. RAPIDS applications use the GPUs that were allocated for the Jupyter Notebook service from which the code is executed, while Horovod applications allocate GPUs dynamically and don't use the GPUs of the parent Jupyter Notebook service. For example, on systems with limited GPU resources you might need to reduce the amount of GPU resources allocated to the Jupyter Notebook service or set it to zero to successfully run the Horovod code over GPUs. See Also  Working with Services The MPI-Operator Horovod Service Running DataFrames on GPUs using NVIDIA cuDF The Jupyter Notebook Service Nuclio Serverless Functions  ","keywords":["gpu","support,","gpu,","graphical","processing","units,","horovod,","rapids,","nvidia,","cudf,","cuml,","keras,","tensorflow,","pytorch,","nuclio,","serverless","functions,","kubeflow","mpi","operator,","mpi","operator,","mpi","jobs,","mpi,","jupyter,","jupyter","notebook,","deep","learning,","machine","learning,","ml,","ml","models,","model","training,","performance"],"path":"https://github.com/jasonnIguazio/services/app-services/gpu/","title":"Running Applications over GPUs"},{"content":" Overview You must authenticate your web-API requests to confirm the identity of the sender. You can do this by using any of the supported HTTP user-authentication methods. To further secure your requests, it's recommended that you also use the HTTPS protocol.\nHTTPS Requests The web APIs support sending secure requests using the HTTP Secure (HTTPS) protocol (also known as HTTP over TLS), as defined in the RFC 2818 specification. To send an HTTPS request, simply use an https:// IP address in the request URL.\nHTTP User Authentication Only platform users with relevant permissions can use the web APIs. The web APIs support several alternative methods for authenticating the identity of the user who sent the request:\n Access-key authentication  X-v3io-session-key authentication S3-like authentications   Basic HTTP username/password authentication  NoteThe examples in the documentation use sample authentication credentials. To run the examples, you must replace the sample credentials with valid credentials for your environment.  Access-Key Authentication The web APIs support several alternative syntax variations for performing access-key authentication:\n X-v3io-session-key authentication S3-like authentications  All of these methods use a platform access key to authenticate the identity of the user. You can get the access key from the platform dashboard: select the user-profile picture or icon from the top right corner of any page, and select Access Keys from the menu. In the Access Keys window, either copy an existing access key or create a new key and copy it. Alternatively, you can get the access key by checking the value of the V3IO_ACCESS_KEY environment variable in a web-shell or Jupyter Notebook service.\nX-v3io-session-key Authentication The web APIs support a custom X-v3io-session-key HTTP request header for access-key requests authentication. The value of the header is a platform access key.\nX-v3io-session-key: \u0026lt;access key\u0026gt; For example, a request with the following header will be authenticated with the \u0026quot;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026quot; access key:\nX-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf S3-Like Authentications To simplify porting Amazon Simple Storage Service (S3) code to the platform, the web APIs support the following AWS signature authentication variations; just replace your S3 access key in the Authorization header with a platform access key. For both variations, the platform extracts the access key from the header value (\u0026lt;access key\u0026gt;) and uses it to authenticate the request; any other information in the header, such as an S3 signature, is ignored.\n  AWS signature version 4 (AWS4) authentication syntax —\nAuthorization: AWS4-\u0026lt;...\u0026gt;Credential=\u0026lt;access key\u0026gt;/[...] For example, a request with the following header will be authenticated with the \u0026quot;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026quot; access key:\nAuthorization: AWS4-HMAC-SHA256 Credential=e8bd4ca2-537b-4175-bf01-8c74963e90bf/20190422/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=4708b8682367fff1ba5d33662a6a7bdbefa743b52e4744aedbd919ca73ce70f5   AWS signature version 2 (AWS2) authentication syntax —\nAWS \u0026lt;access-key\u0026gt;:\u0026lt;signature\u0026gt; For example, a request with the following header will be authenticated with the \u0026quot;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026quot; access key:\nAuthorization: AWS e8bd4ca2-537b-4175-bf01-8c74963e90bf:frJIUN8DYpKDtOLCwo//yllqDzg=   Basic HTTP Username/Password Authentication To use the username/password \u0026quot;Basic\u0026quot; HTTP authentication scheme, as defined in the RFC 7617 and RFC 7235 specifications, do the following: add an Authorization request header with the Basic authentication-scheme token followed by a Base64 string that encodes the username and password login credentials:\nAuthorization: Basic \u0026lt;Base64-encoded credentials\u0026gt; For example:\nAuthorization: Basic iguazio:$apr1$YgrCYAYo$6v6iumigwirH4Jsdt4MWr0  Postman NoteWhen using Postman, select the Authorization tab, select the \u0026quot;Basic Auth\u0026quot; authorization type, and enter your username and password in the respective fields. Postman will encode the login credentials and create the required header.  See Also  Security Data-Service Web-API General Structure  ","keywords":["web","APIs,","REST,","RESTful,","security,","http,","https,","authentication,","http","authentication,","authorization"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/security/","title":"Securing Your Web-API Requests"},{"content":" Overview The platform implements multiple mechanisms to secure access to resources and keep your data safe. All user and security management is done under a single pane of glass: the configuration is done in one place, in the user-friendly graphical platform dashboard, and applied across all platform interfaces. The result is a significantly simplified, yet robust, data-security solution that helps organizations meet their compliance objectives.\nThe platform allows you to define local users and import users from an external identity provider (IdP), authenticate user identities, and control users' access to resources, including the ability to define fine-grained data-access policies. To ensure proper security, the platform uses time-limited sessions and supports the HTTP Secure (HTTPS) protocol. You can view logs for security events and user actions (such as a failed login or deletion of a data container) on the Events | Audit dashboard tab. HTTP Secure Data Transmission For enhanced security, the platform's RESTful web and cluster-management APIs support the HTTP Secure (HTTPS) protocol (also known as HTTP over TLS), as defined in the RFC 2818 specification. See HTTPS Requests in the Securing Your Web-API Requests reference. Authentication Authentication is the process of validating a user's identity before granting the user access to a specific resource. Before granting a user access to resources, the platform verifies (authenticates) the identity of the user and then ensures that the user has the required permissions to perform the requested operation (see Authorization). To support authentication, the platform uses time-limited sessions and access keys. The time-to-live (TTL) session period is 24 hours.\nAuthentication of data-access requests is done using data sessions, which are handled transparently by the platform.\nAuthentication of management requests is done using management sessions, which are created transparently when performing operations from the platform dashboard, or handled by the user using the platform's RESTful cluster-management APIs (\u0026quot;the management APIs\u0026quot;) [Beta]. These APIs use a session-based HTTP scheme to support user authentication and authorization: access to management API resources requires a time-limited session cookie that is used to authenticate the sender of the request with a username and password, and determine the sender's authorization to perform the requested operation. See the Overview [Beta]. In addition, the platform's web APIs support user authentication by using either the username/password Basic HTTP authentication scheme or custom access-key (session-key) authentication. With either method, the user provides an authentication header with user credentials that are verified by the platform as a condition for sending the request. See HTTP User Authentication in the Securing Your Web-API Requests reference.\nAccess keys\nAccess keys are not time-bound, and are used mainly for programmatic purposes. Access keys are created by authenticated users and can allow only the actions in the scope of the user creating the key: each specific resource access is defined by the management policies of the user.\nAccess keys are specific to either the data plane, the control plane, or to both. The data plane is relevant to operations on the data itself (put object, write to stream, read kv etc.). The control plane includes create, read, update, and delete, operations on all resources that aren't specifically data in the containers, for example: projects, users, groups, management policies, the containers themselves (not the data inside them), services, etc.\nAccess keys are not limited to a specific use or environment. They can be delegated and passed on to other entities. The clients running inside Jupyter use the access key from Jupyter and pass it to the resources they create. For example, when MLRun SDK is running inside Jupyter, it passes the access key to the MLRun jobs it creates.\nSince access keys be used by anyone who has access to them, they should be closely watched, and their usage should be well monitored.\nTo create access keys, click the user icon (), then click Access Keys.\nWhen integrated with MLRun 8.0 and higher, in certain wizards, you must add an access key that includes the control plane, for example the create and deploy functions, and the create job and scheduled job. The Auto-generate access key, in the Resources section of the wizard, is selected by default in these wizards. Alternately, you can clear the checkbox and add a custom access key.\nIdentity Provider (IdP) The authentication of the user credentials can be done locally, using the platform's built-in user management, or using an external Identity Provider (IdP)—currently Microsoft Active Directory (AD). When an IdP is configured, it is used to authenticate the identity of all its imported users in the platform. This doesn't prevent you from also defining local users and using the platform to authenticate them. For more information about using an IdP, see Using an External Identity Provider (IdP). NoteIn the event of a change in the management policies of an authenticated user, the authentication token is revoked and the session expires.  Authorization Authorization is the process of granting a user permission to perform a specific action or access a specific resource based on predefined authorization rules. To support authorization, the platform uses policies and permissions that govern the ability to access resources.\n Management policies are assigned to users and user groups to determine management-related permissions. For example, the permission to create a storage pool or restart a cluster is reserved to users who have the IT Admin management policy, and the permission to access the data is reserved to users who have the Data management policy. Project membership is assigned to users and user groups to control access to projects and the levels of access. Project membership does not include access to project data. Data-access policies define fine-grained rules for determining data-access permissions. These policies are used as part of a multi-layered data-access authorization scheme, which also involves the Data management policy and POSIX ACLs.  User-Group Permissions InheritanceA user inherits the management policies and POSIX permissions of all user groups to which the user belongs. However, user-group permissions in data-access policy rules are checked only against a user's primary group.  Management Policies Every user and user group (whether locally created or imported) must be assigned one or more of the predefined management policies. These policies define resource-access permissions with management aspects that are applicable globally throughout the platform. The management policies are assigned by a security administrator, which is any user with the Security Admin management policy, including the predefined security_admin user. For more information about user management in the platform, see Platform Users. Predefined Management Policies These are the predefined management policies that a security administrator can assign to users and user groups:\n   Application Admin—full rights to all container operations, such as creating data containers, and defining data-access policies; view and use application services for the current user; view the pipelines dashboard.  All locally created and imported users in the platform (but not the predefined users) are automatically assigned this policy.\n   Application Read Only—view all reports without editing; view and use application services for the current user; view the pipelines dashboard.\n   Data—access data and run application services. The specific access level is derived from the data-access policies and POSIX ACLs.  This policy allows the implicit creation of data sessions, which are used for securing access to data.  All locally created and imported users in the platform (but not the predefined users) are automatically assigned this policy.\n   Developer—the only policy whose users can create projects, and manage and develop Nuclio serverless functions. (Developers can only see the projects that they are members of.)\n   IT Admin—full rights for all IT operations, such as defining storage pools or stopping and starting a cluster. This policy includes permissions for viewing event logs and for managing cluster support logs; for more information, see Logging, Monitoring, and Debugging.  The predefined tenancy_admin user is assigned this policy together with the Tenant Admin policy.\n   Project Admin—view all projects, perform all admin tasks on the project, and change a project owner to any user. This policy does not give rights to modify the entities underneath such as features, jobs etc., and does not give rights to delete a project.\n   Project Read Only—view all projects. A user with the Project Read Only management policy that is not a member of the project cannot drill down to see the project objects such as function, jobs, and so on.\n   Security Admin—full rights for managing users and user groups. This includes creating and deleting users and user groups, assigning management policies, and integrating the platform with a supported identity provider (see Using an External Identity Provider (IdP)). (All users can view their own user profile and edit some of the properties, including the password. For more information, see Platform Users.) This policy also includes permissions for viewing audit event logs. For more information, see Logging, Monitoring, and Debugging.  The predefined security_admin user is assigned this policy.\n   Service Admin—full rights for managing application services, including creating, configuring, restarting, and deleting user-defined services, configuring and restarting relevant default services, and managing service logs. This policy also includes permissions for viewing the pipelines dashboard and for viewing application-service logs from the log-forwarder service. For more information, see Logging, Monitoring, and Debugging.\n   Tenant Admin—full rights for managing tenants, including creating and deleting tenants.  The predefined tenancy_admin user is assigned this policy together with the IT Admin policy.   Services Management Policies To view the Services dashboard page, a user must have the Service Admin, Application Admin, or Application Read Only management policy.  The application policies enable viewing services that are owned by or shared with the logged-in user—i.e., services for which the user is the running user, shared services, and tenant-wide services without a running user.  The Service Admin policy enables viewing all services of the parent tenant. To run services and be assigned as the running user of a service, a user must have the Data management policy. To manage (administer) services, a user must have the Service Admin management policy. A service administrator can create, delete, disable, enable, or restart services, change service configurations, and view service logs for all users.    Project members Project membership provide segregation between the various projects and their users. You can specify project members (individual users or user groups) for each project. Each project members has a role that controls access to projects management and the levels of access. It ensures that only users that are members of a given project can view and manage the project.\nTo access the Projects page to view and manage projects, users must have one of the following management policies: Developer, Project Admin, Project Read Only.\nAn individual user's permissions (or a user group's permissions) for any one project are the sum of the management policy and the project membership role permissions for that user or group, for that project. Project membership can be managed by users with the Project Admin policy, and by the project's owner.\nProject members have one of the following roles:\n Admin—View, and add and remove members, and change their member role, and all the privileges of the Editor role. Editor—Edit the project. The editor has management and configuration rights for the project, for example changing function code, and creating a job. It cannot modify members or change ownership. Viewer—View all project content, but cannot create, edit, or delete objects in the project.  These roles are control plane roles only. Project data is managed on a different layer. Data permissions for data that resides in Iguazio’s data layer are controlled by the data access policies and POSIX ACLs.\nThe user that creates the project is the owner and is also an admin member of the project. The owner has management and configuration rights for the project, for example: changing function code; creating a job; and adding and modifying admin, editor, and viewer members. The owner can change the ownership to another user. In this scenario the original owner remains an admin member.\nTo help understand the permissions, here are a few examples:\n User x has Project Admin management policy and is a Viewer member of project p. The viewer member gives read permissions and the Project Admin policy gives permission to update the owner of project p. User y has Developer management policy and is an Admin member of project p. The Developer policy gives permission to create projects, and the Admin member allows the user to add and remove members, and change their member role for project p. User z has Developer management policy and is a Viewer member on a project p. But user z is also part of a group that has an Editor role on that project. User z effectively has the Editor role on project p and can create projects with the Developer policy.  The project owner and the number of members are both displayed in the left pane of the individual project pages. Click Change or Manage to modify the owner, or members and their permissions.    Upgrading from pre-v3.2.0\nDuring the upgrade you must designate one user to own all of the migrated projects.\nAfter upgrade to v3.2.0, all pre-existing projects are associated with a (dynamically updated) group of all users, named all users, such that all users have access to all projects. (This is the pre-upgrade behavior.) After the upgrade you can assign members to each project.\nData-Access Authorization The platform uses a multi-layered data-authorization scheme: each data-service operation—read, write, update, delete, etc.—is processed and examined in three layers, to ensure that the environment is protected and secured. Each layer can add to the restrictions of the previous layer:\n The \"Data\" Management Policy As a preliminary step to accessing data in the platform, a user must have the Data management policy. This policy enables the implicit creation of data sessions, which are used for securing access to data. (The tenancy_admin and security_admin predefined users do not have this policy and therefore cannot access data or view the dashboard's Data page.) POSIX ACLs Use the portable operating-system interface access control lists (POSIX ACLs) to define file-system permissions. Data-Access Policies Use the Data-access policies to define fine-grained policies for restricting access to determine whether to grant or restrict access to a specific data resource and to what extent. For example, to create a subnetwork (subnet) whitelist, define interface data-access eligibility, restrict the right to read a payments table that contains sensitive data to members of your organization's finance group (restrict access to a table only to specific user groups), limit the write privileges for updating the online transactions stream to members of the operational team (give some users only read-only permissions for a specific file). See additional information in the Data-Access Policy Rules section.  The following diagram illustrates the platform's multi-layered data-authorization scheme:    In most solutions, too many policy rules that need to inspect every data operation will come at a cost and may cause a performance degradation and low throughput, as the inspection takes time. However, in the MLOps Platform, the data-access policy rules are compiled and stored in an optimized binary format on every policy change—rule addition, removal, or update. This allows the platform to process the rules in a fast and effective manner, resulting in high-performance processing for each data request, in line rate, while keeping the environment highly secured.    NoteFor symbolic links, the platform requires that both the data-access permissions of the source and destination locations are met.  POSIX ACLs When you create a project, a directory is created in the projects container for that project and it serves as the default path for project files. This directory is created by the user who is the owner of the project, and by default is accessible to users that belong to the same primary POSIX group as the owner. When other users attempt to access the project files, they need to belong to the same group as the project owner. To restrict access to specific users:\n Add specific users and/or groups to directories and files as relevant. Deny access to all users and groups.  Users must have the Data management policy, and they must have access to any other locations where related data is stored.\nData-Access Policy Rules Users with the Application Admin management policy (such as the predefined security_admin user) can define a set of fine-grained data-access policy rules. These rules are checked for each data operation and are used to determine whether to allow or deny the data request. Data-access policy rules are defined in the context of a specific data container and apply to all data objects in the container, regardless of their type.\n Defining Rules Rules Processing Predefined Rules and Layers Examples Data Categories  Defining Rules Data-access policy rules are managed from the Data | \u0026lt;container\u0026gt; | Data-Access Policy dashboard tab, which displays the predefined data-access policy rules and layers and options for adding and editing rules, groups, and layers, as demonstrated for the \u0026quot;users\u0026quot; data container in the following image:\n   A rule must belong to a data-access layer. You can either add rules to one of the predefined layers for the parent data container or create your own layer: select the New Layer option from the New Rule drop-down menu in the top action toolbar; in the new-layer dialog window, enter the layer name and select Create. The following image demonstrates creation of a new layer named \u0026quot;Default layer\u0026quot;:    You can add rules directly to a layer or group multiple rules into one or more rule groups within a layer. To add a new group, select the New Group option from the New Rule drop-down menu in the action toolbar; in the new-group dialog window, enter the group name, select a parent layer, and select Create.\nThe purpose of the layers and groups is to help you manage your rules and easily reorder rules to change the processing logic, as explained in the Rules Processing section. You can rename a layer or group by selecting and editing the name in the rules table, and you can delete it by selecting the delete icon () for the relevant table entry.\nTo add a new data-access policy rule, select the New Rule option from the action toolbar; in the new-rule dialog window, enter the rule name, select a parent layer and optionally a parent group, and select Create. The following image demonstrates creation of a new rule named \u0026quot;Rule 1\u0026quot; in a layer named \u0026quot;Default layer\u0026quot;:    NoteRemember to select Apply Changes from the pending-changes toolbar to save your changes.  After you create a rule, select it from the rules table to display the rule pane and define the permissions for accessing the data based on one or more of the following characteristics (match criteria).\nNoteAll match-criteria rule sections, whether defined in the same tab or in different tabs, are accumulative (\u0026quot;AND\u0026quot;), but the values in each section are alternative (\u0026quot;OR\u0026quot;), except where otherwise specified. For more information, see the Rules Processing explanation and the Examples.   Sources A rule can be restricted to specific sources.\n   Currently, the platform support an Interfaces source type, which is an interface for accessing the data:\n Web APIs—the platform's web-APIs, which are available via the web-APIs service (webapi)  V3io Daemon—the platform's core daemon service (v3io-daemon), which connects application services (such as Spark and Presto) to the platform's data layer. File system—Linux file-system operations.     Users A rule can be restricted to a specific list of predefined users or user groups. Note that user-group match criteria in data-access policy rules are applicable only to the primary group of the user who attempts to access the data.\n    Resources A rule can be restricted to specific data resources.\n   A resource can be defined as a path within the container, such as the path to a table or stream or to a subdirectory or file.\n A resource can also be defined as a logical category of data—such as audio, video, logs, or documents. For a list of all resource data categories and the file extensions that they represent, see Data Categories.\n   After defining the match criteria for the rule, you define the data-access permissions to be applied when there's a full match. You can select whether to allow or deny access to the data and to what extent. For example, you can grant only read permissions, deny only the create and delete permissions, or allow or deny full access. The following image demonstrates full data-access permissions:    NoteYou can disable or enable, duplicate, or delete a rule, at any time, from the rule action menu ().  Rules Processing The rules are processed for each data operation according to the order in which they appear in the dashboard. You can change the processing order, at any time, by changing the order of the data-access policy rules in the dashboard: you can change the order of the rules and rule groups within each container data-access layer; change the order of rules within each group; and change the order of the layers.\nWhen a full match between the operation and a policy rule is found, the processing stops and the data accessibility is set according to the permissions of the first-matched rule. A match is identified by checking all components of the rule. All match-criteria rule sections are accumulative (\u0026quot;AND\u0026quot;) but the values in each section are alternative (\u0026quot;OR\u0026quot;), except where otherwise specified. See the examples for a better understanding. NoteThe platform's default data-access policy is fully permissive: users with data-access permissions—i.e., users with a Data management policy for the parent container—aren't restricted in their access, subject to the optional definition of POSIX rules. It's therefore recommended that you safeguard your data by always defining a deny-all rule as the last data-access policy rule, as demonstrated in the examples.  Predefined Rules and Layers The platform predefines the following data-access policy layers and rules for each data container, except where otherwise specified:\n  System layer—A system-administration layer that has the following predefined rule:\n  Backup—This rules grants the predefined \u0026quot;sys\u0026quot; backup user full data access, to support data backups. It's recommended that you keep this rule as the first rule in your processing order.\n       Monitoring layer—A monitoring-service layer that has the following predefined rules:\n  Monitoring—This rule is defined only for the predefined \u0026quot;users\u0026quot; container and grants the predefined \u0026quot;monitoring\u0026quot; user full data access to the monitoring directory, which is automatically created in the root directory of this container for use by the monitoring service.\n     No access—This rule denies the predefined \u0026quot;monitoring\u0026quot; user all data access. Note that on the \u0026quot;users\u0026quot; container, this rule must not precede the \u0026quot;Monitoring\u0026quot; rule, as the first rule takes precedence (see the rules processing order).        Examples The predefined data-access policy rules provide examples of granting and restricting data access for a specific user and/or resource (data directory). Following is a step-by-step example of adding your own custom data-access policy rules from the dashboard Data-Access Policy tab.\n  Create a new \u0026quot;Default layer\u0026quot; layer: from the top action toolbar, select the drop-down arrow on the New Rule button and select the New Layer option from the menu. In the Create new layer dialog window, enter your selected layer name—\u0026quot;Default layer\u0026quot; for this example:\n   Keep the new layer after the predefined layers in the rules table (default).\n  Define a custom \u0026quot;IT Logs\u0026quot; rule that grants members of the \u0026quot;it-admins\u0026quot; user group full permissions to access any log or document file in either the system/logs or it directories in the parent container: NoteTo define and test this rule, you need to create an \u0026quot;it-admins\u0026quot; group from the Identity | Groups dashboard tab, assign users to this group, and create the directories that are specified in the match criteria. Alternatively, you can change the match criteria to accommodate your environment and needs.    From the top action toolbar, select the New Rule option. In the Create new rule dialog window, enter your selected rule name—\u0026quot;IT Logs\u0026quot; for this example.\n     Select the Users/Groups cell of the \u0026quot;IT Logs\u0026quot; rule in the rules table to display the Users tab in the rule pane on the right. In the Users/Groups input box, start typing \u0026quot;it-admins\u0026quot; and select this group from the list.\n     Select the Resources tab in the \u0026quot;IT Logs\u0026quot; rule pane. In the Paths section, select the plus sign (), enter /system/logs in the input box, and select Apply. Repeat this step but this time enter the path /it.\n     In the Permissions tab, keep the default allow-all permissions.\n    Define a custom \u0026quot;Deny All\u0026quot; rule that denies all data access, as recommended in the rule-processing section:\n  Create a new rule in the \u0026quot;Default layer\u0026quot; layer and name it \u0026quot;Deny All\u0026quot;.\n  Select the Permissions cell of the \u0026quot;Deny All\u0026quot; rule in the rules table. In the Permissions rule tab, select the Deny option from the permissions drop-down box and keep all permission check boxes checked to deny all data-access permissions.\n   Note The deny-all rule must be the last rule in the data-access policy rules table; any rules that appear after it will be ignored. You can move the rule to another layer, if you wish. You might want to disable this rule during the initial stages of your development and testing, as it blocks all data access that isn't explicitly permitted in other (preceding) data-access policy rules.        Select Apply Changes from the pending-changes toolbar to save your changes:      You can now see your new layer and rules in the data-access policy rules table:\n   Data Categories The following table lists the supported data categories, which can be used to define a resource for a data-access policy rule, and the file extensions that each category represents:\n Resource Category  File Extensions   Archives 7Z, ACE, AR, ARC, ARJ, B1, BAGIT, BZIP2, CABINET, CFS, COMPRESS, CPIO, CPT, DGCA, DMG, EGG, GZIP, ISO, KGB, LBR, LHA, LZIP, LZMA, LZOP, LZX, MPQ, PEA, RAR, RZIP, SHAR, SIT, SQ, SQX, TAR, TAR.GZ, UDA, WAD, XAR, XZ, Z, ZIP, ZIPX, ZOO, ZPAQ   Audio AIFF, AIFCDA, M4A, M4B, MID, MIDI, MP3, MPA, OGG, WAV, WMA, WPL   Data AVRO, CSV, DAT, DATA, JSON, MDB, ORC, PARQUET, RC, SAV, TSV, XML   Documents DOC, DOCX, KEY, ODT, ODP, PDF, PPS, PPT, PPTX, RTF, TEX, TXT, WKS, WPS, WPD, XLS, XLSX   Logs LOG   Pictures ANI, ANIM, APNG, ART, BMP, BPG, BSAVE, CAL, CIN, CPC, CPT, CUR, DDS, DPX, ECW, EXR, FITS, FLIC, FLIF, FPX, GIF, HDRI, HEVC, ICER, ICNS, ICO, ICS, ILBM, J2K, JBIG, JBIG2, JLS, JNG, JP2, JPEG, JPF, JPG, JPM, JPX, JXR, KRA, LOGLUV, MJ2, MNG, MIFF, NRRD, ORA, PAM, PBM, PCX, PGF, PGM, PICTOR, PPM, PNM, PNG, PSB, PSD, PSP, QTVR, RAS, RBE, SGI, TGA, TIF, TIFF, UFO, UFP, WBMP, WEBP, XBM, XCF, XPM, XR, XWD   Programs/Binaries BIN, CER, CFM, CGI, CLASS, COM, CPP, CSS, DLL, EXE, H, HTM, HTML, JAVA, JS, JSP, PART, PHP, PL, PY, RSS, SH, SWIFT, VB, XHTML   Software Packaging APK, DEB, EAR, JAR, JAVA, MSI, RAR, RPM, VCD, WAR   System Files BAK, CAB, CFG, CPL, CUR, DMP, DRV, ICN, INI, LNK, SYS, TMP   Video 3G2, 3GP, AVI, FLV, H264M4V, MKV, MOV, MP4, MPG, RM, SWF, VOB, WMV   Virtual-Machine (VM) Images NVRAM, VMDK, VMSD, VMSN, VMSS, VMTM, VMX, VMXF   See Also  Platform Users Securing Your Web-API Requests Sessions Management API [Beta]  Working with Services Security software specifications and restrictions  ","keywords":["security,","policies,","data-access","policies,","data","policies,","authentication,","http","authentication,","authorization,","access","control,","https,","tls,","users,","user","management,","management,","managemnt","policies,","data","categories,","web","apis,","cluster-management","apis,","management","apis,","project","permissions"],"path":"https://github.com/jasonnIguazio/users-and-security/security/","title":"Security"},{"content":"Description Returns the requested location within the specified stream shard, for use in a subsequent GetRecords operation. The operation supports different seek types, as outlined in the Stream Record Consumption overview and in the description of the Type request parameter below.\nRequest Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: Seek \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;Seek\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the target stream shard. The path includes the stream path and the shard ID. You can optionally set the stream name and shard ID, or only the shard ID, in the request's StreamName and ShardId JSON parameters instead of in the URL.\nRequest Data Syntax  { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardId\u0026#34;: number, \u0026#34;Type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;TimestampSec\u0026#34;: number, \u0026#34;TimestampNSec\u0026#34;: number, \u0026#34;StartingSequenceNumber\u0026#34;: number }  payload = { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardId\u0026#34;: number, \u0026#34;Type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;TimestampSec\u0026#34;: number, \u0026#34;TimestampNSec\u0026#34;: number, \u0026#34;StartingSequenceNumber\u0026#34;: number }    Parameters  StreamName The name of the stream that contains the shard resource.\n Type: String   Requirement: Required if not set in the request URL   ShardId The ID of the shard for which to obtain the requested location. The shard ID is an integer between 0 and one less than the stream's shard count.\n Type: Number   Requirement: Required if not set in the request URL   Type The seek type, which determines the location in the specified stream shard to retrieve.\n Type: String   Requirement: Required  The following seek types are supported:\n  \u0026quot;EARLIEST\u0026quot; — the location of the earliest ingested record in the shard.\n  \u0026quot;LATEST\u0026quot; — the location of the end of the shard.\n  \u0026quot;TIME\u0026quot; — the location of the earliest ingested record in the shard beginning at the base time set in the TimestampSec and TimestampNSec request parameters. If no matching record is found (i.e., if all records in the shard arrived before the specified base time) the operation returns the location of the end of the shard.\n  \u0026quot;SEQUENCE\u0026quot; — the location of the record whose sequence number matches the sequence number specified in the StartingSequenceNumber request parameter. If no match is found, the operation fails.\n   TimestampSec The base time for a time-based seek operation (Type=TIME), as a Unix timestamp in seconds. For example, 1511260205 sets the search base time to 21 Nov 2017 at 10:30:05 AM UTC. The TimestampNSec request parameter sets the nanoseconds unit of the seek base time.  When the TimestampSec and TimestampNSec parameters are set, the operation searches for the location of the earliest ingested record in the shard (the earliest record that arrived at the platform) beginning at the specified base time. If no matching record is found (i.e., if all records in the shard arrived before the specified base time), return the last location in the shard.\n Type: Number   Requirement: Required when the value of the Type request parameter is TIME\n   TimestampNSec The nanoseconds unit of the TimestampSec base-time timestamp for a time-based seek operation (Type=TIME). For example, if TimestampSec is 1511260205 and TimestampNSec is 500000000, seek should search for the earliest ingested record since 21 Nov 2017 at 10:30 AM and 5.5 seconds.\n Type: Number   Requirement: Required when the value of the Type request parameter is TIME\n   StartSequenceNumber Record sequence number for a sequence-number based seek operation — Type=SEQUENCE. When this parameter is set, the operation returns the location of the record whose sequence number matches the parameter.\n Type: Number   Requirement: Required when the value of the Type request parameter is SEQUENCE\n    Response Response Data Syntax { \u0026#34;Location\u0026#34;: \u0026#34;blob\u0026#34; } Elements  Location The requested location within the specified stream shard (see the Type request parameter).\n Type: Blob — a Base64 encoded string    Errors In the event of an error, the response includes a JSON object with an ErrorCode element that contains a unique numeric error code, and an ErrorMessage element that contains one of the following API error messages: Error Message Description   InvalidArgumentException A provided request parameter is not valid for this request.    Permission denied The sender of the request does not have the required permissions to perform the operation.    ResourceNotFoundException The specified resource does not exist.   ShardIDOutOfRangeException The specified shard does not exist in this stream.    Examples Obtain the location of the earliest ingested record in shard 199 of a MyStream stream:\nRequest  POST /mycontainer/MyDirectory/MyStream/199 HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: Seek X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;Type\u0026#34;: \u0026#34;EARLIEST\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyDirectory/MyStream/199\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;Seek\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = {\u0026#34;Type\u0026#34;: \u0026#34;EARLIEST\u0026#34;} response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ... {\u0026#34;Location\u0026#34;: \u0026#34;AQAAAAAAAAAAAAAAAAAAAA==\u0026#34;} ","keywords":["Seek,","stream","seek,","seek","records,","streaming,","seek","types,","stream","records,","stream","consumption,","GetRecords,","stream","shards,","record","arrival","time,","record","sequence","number,","record","location,","Location,","StartingSequenceNumber,","TimestampNSec,","TimestampSec"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/seek/","title":"Seek"},{"content":"API Endpoint /api/sessions\n","keywords":["sessions","management","api,","sessions","api,","api","reference,","management,","session","management,","sessions,","/api/sessions,","api","endpoints"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/reference/management-apis/sessions-api/","title":"Sessions API"},{"content":"Introduction The platform's Simple-Object Web API enables working with data containers and performing simple data-object operations via a RESTful interface that resembles Amazon's Simple Storage Service (S3) API. In addition to the S3-like capabilities, the Simple-Object Web API enables appending data to existing objects (see Appending Data).\nFor information on how to secure your Simple-Object Web API requests — including support for HTTPS and HTTP user-authentication methods — see Securing Your Web-API Requests.\nFor an overview of the Simple-Object Web API response syntax, see the response-syntax documentation in the Data-Service Web-API General Structure reference, which applies also to the Simple-Object Web API (with the exception of the error-information syntax in the response body). See Also  Data Objects Securing Your Web-API Requests Response Syntax (Data-Service Web-API General Structure) Working with Data Containers and Ingesting and Consuming Files tutorials — including Simple-Object Web API examples   ","keywords":["simple-object","web","api,","api","reference,","simple","objects,","S3,","REST,","RESTful,","Amazon,","AWS,","containers,","data","objects,","appending","data"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/simple-object-web-api/overview/","title":"Overview of the Simple-Objet Web API"},{"content":"","keywords":["simple-object","web","api,","api","reference,","simple","objects,","S3"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/simple-object-web-api/","title":"Simple-Object Web API Reference"},{"content":" Following is a list of software specifications and restrictions for version 3.2.1 of the MLOps Platform (\u0026quot;the platform\u0026quot;). For a list of reserved names in the platform, see the reserved-names reference. For a list of known issues for the current release, see the release notes.  (Note: For amount values, M = million; B = billion.)\nConfidentialityThe documentation in this section is confidential under the Iguazio End User License Agreement. For more information, contact support@iguazio.com.  General  Replication factor 2  Maximum number of concurrent sessions 100,000 Maximum number of rules per data-access policy 100  Maximum number of users 10,000  Maximum number of user groups 64 per user; 1,000 overall. Note: If a user is a member of more than 64 user groups, only 64 of the groups (which always include the primary user group) are used for checking data-access permissions.  Usernames Usernames are subject to the following restrictions:\n  Contain only the following characters:\n Alphanumeric characters (a–z, A–Z, 0–9) Hyphens (-) Underscores (_)    Begin with a letter (a–z, A–Z)\n  Length of 1–32 characters\n    User passwords Use passwords are subject to the following restrictions:\n Contain at least one uppercase letter (A–Z) Contain at least one lowercase letter (a–-z) Contain at least one special character (!, @, #, $, %, ^, \u0026amp;, *) Contain at least one digit (0–9) Length of at least 8 characters    Project names Project names must conform to the following restrictions (RFC 1123 DNS label-name requirements, a.k.a DNS-1123 label):\n  Contain only the following characters:\n Lowercase letters (a–z) and numeric digits (0–9) Hyphens (-)    Begin and end with a lowercase letter (a–z) or a numeric digit (0–9)\n  Length of 1–63 characters\n  (See the MLRun project-name restrictions for information regarding upgrading old MLRun projects that don't meet these requirements.)\n  DNS dependency Platform startup requires an accessible Domain Name System (DNS).   Data Elements  Maximum number of objects The maximum supported number of objects depends on the size of the data-node memory and on the number of data nodes. The Max Objects — Single Node column indicates the maximum number of objects on a single data node (which is also the total maximum number of objects on a Development Kit cluster); the Max Objects — Cluster column indicates the maximum number of objects on the entire cluster for a 3-node cluster (Operational Cluster).\nDeployment Type Memory Size Max Objects \u0026mdash; Single\u0026nbsp;Node Max Objects \u0026mdash; Cluster  Cloud — AWS i3.8xlarge 244 GiB 500M 1B  Cloud — AWS i3.4xlarge 122 GiB 180M 360M  Cloud — AWS i3.2xlarge 61 GiB 90M 90M  Cloud — Azure L16s_v2 128 GB 180M 360M  VM — large node 256 GB 500M 1B  VM — small node 128 GB 180M 360M  Bare-metal 512 GB 500M 1B     Maximum number of data containers  Cloud — 10 VM — 20 Bare-metal — 20    Maximum number of collections per cluster The maximum number of collections (tables/streams/directories) in a cluster depends on the size of the data-node memory.\nDeployment Type Memory Size Max Elements  Cloud — AWS i3.8xlarge 244 GiB 10M  Cloud — AWS i3.4xlarge 122 GiB 5M  Cloud — AWS i3.2xlarge 61 GiB 1M  Cloud — Azure L16s_v2 128 GB 5M  VM — large node 256 GB 10M  VM — small node 128 GB 5M  Bare-metal 512 GB 10M     Maximum elements in a collection (table/stream/directory) The maximum elements in a collection equals the maximum number of objects per data node and depends on the size of the data-node memory.\nDeployment Type Memory Size Max Elements   Cloud — AWS i3.8xlarge 244 GiB 500M   Cloud — AWS i3.4xlarge 122 GiB 180M   Cloud — Azure L16s_v2 128 GB 180M   VM — large node 256 GB 500M   VM — small node 128 GB 180M   Bare-metal 512 GB 500M     Maximum object path size The maximum size of the relative path to an object (file) within a given data container is 4,096 bytes.  Maximum object size The maximum size of an object (file) depends on the interface:\n  For the Simple-Object Web API (PUT Object) — 5 GB\n  For the file-system interface (file creation) — 1 TB.  The truncate -s command can be used to add a maximum of 240 MB to the current file size.\n  For other APIs — 500 GB.\n    Maximum NoSQL-table item size The maximum size of a NoSQL-table item (row) — i.e., the maximum total size of the item's attributes — is 1.5 MB.  Displayed object size Displayed object (file) sizes don't include the size of the object's attributes.  Maximum number of distinct attribute names per container 64,000  Maximum number of attributes per object Between 400 and 1600 attributes, depending on the attribute type  Maximum attribute size The maximum object (item) attribute size is 61,200 bytes for string attributes and 1 MB for other attribute types, subject to the following restrictions:\n  For blob attributes when using the NoSQL Web API, the maximum size applies to the decoded data for the PutItem operation and to the encoded data for the UpdateItem operation.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8 KB.\n    Maximum number of shards in a stream 4,096  Maximum stream retention period 1 year  Object names Names of data objects (such as items and files) are subject to the general file-system naming restrictions, including a maximum length of 255 characters. In addition —\n A period in an object name indicates a compound name of the format sharding key\u0026gt;.\u0026lt;sorting key\u0026gt;. See Object Names and Primary Keys.    Container names Container names are subject to the general file-system naming restrictions and the following additional restrictions:\n  Contain only the following characters:\n Lowercase letters (a–z) and numeric digits (0–9) Hyphens (-) Underscores (_)    Begin and end with a lowercase letter (a–z) or a numeric digit (0–9)\n  Contain at least one lowercase letter (a–z)\n  Not contain multiple successive hyphens (-) or underscores (_)\n  Length of 1–128 characters\n    Attribute names Attribute names are subject to the general file-system naming restrictions and the following additional restrictions:\n  Contain only the following characters:\n Alphanumeric characters (a–z, A–Z, 0–9) Underscores (_)    Begin either with a letter (a–z, A–Z) or with an underscore (_)\n  Not identical to a reserved name — see Reserved Names\n  Length of 1–255 characters\n    Sharding-key value The value of an object's sharding key cannot contain periods, because the leftmost period in an object's primary-key value (name) is assumed to be a separator between sharding and sorting keys.  \u0026quot;users\u0026quot; container  Don't attempt to delete the predefined \u0026quot;users\u0026quot; container. If you have a web-based shell, Jupyter Notebook, or Zeppelin service, don't delete the \u0026lt;username\u0026gt; running-user directory in the \u0026quot;users\u0026quot; container or the auto-generated service files in this directory.    Stream-shard file operations Don't perform any file operations other than delete on stream-shard directories by using the Simple-Object Web API or the file-system interface (including commands that trigger such operations, such as vi or cat). Such operations should be executed only via a dedicated streaming API, such as the Streaming Web API.   File System NoteSee also the files and directories information in the Data Elements section.   File and directory names All files and directories in the platform — including containers, tables, streams, and attributes — are subject to the file-naming restrictions of the Linux operating system.  The size of a file or directory name must be between 1 and 255 characters.\nNoteThe platform allows using the same name for a file and a directory that reside in the same path. However, you cannot access such files and directories from the file-system interface because of the file-system naming restrictions.    Moving and renaming files The maximum file size when moving or renaming files using a local file-system commands is 30 GB. To move or rename a larger file, use an Hadoop FS command.  Moving (renaming) directories  Moving (renaming) of directories is not supported. Renaming of streams is not supported  See also the specific Jupyter Notebook directory copy and move restrictions.  Sparse files Sparse files are not supported: the physical space consumed by such files in the platform is the same as the logical space, including all holes.   Security  Authorization-policies distribution In some cases, authorization-policies distribution can take up to 30 seconds.  Maximum number of imported IdP users 1,000  Symbolic links For symbolic links, the platform checks the data-access policy rules and POSIX ACLs of both the source and destination locations.   Expressions  Maximum length of an expression 8 KB  Maximum number of operators in an expression 700  Backslash in expressions The backslash character (\\) isn't supported within expressions in the current release.   MLRun NoteThe restrictions refer to MLRun version 0.8, which is used in version 3.2.1 of the platform. For full specifications and restrictions, see the MLRun documentation.   Project names See the general project-name restrictions. Note: Prior to platform version 3.0.0 / MLRun version 0.6.0, these restrictions weren't enforced for MLRun projects. Older MLRun projects whose names don't conform to the updated restrictions will be in a degraded mode and won't have Nuclio functionality (no serverless functions or API gateways). To fix this you need to replace the old projects (you cannot simply rename them). For assistance in upgrading old projects, contact Iguazio's support team.   Nuclio NoteThe restrictions refer to Nuclio version 1.6, which is used in version 3.2.1 of the platform. For full specifications and restrictions, see the Nuclio documentation.   Reserved names The following names are reserved for internal use and cannot be used as the names of Nuclio functions or API gateways:\n controller dashboard dlx scaler    Project names See the general project-name restrictions.  Offline function deployment As a general rule, you can deploy Nuclio functions also in air-gapped environments without an internet connection (a.k.a. \u0026quot;dark sites\u0026quot;). However, for code languages (function runtimes) that download files dynamically during the build, offline deployment might either require an additional effort or might not be possible. For example —\n  Offline deployment of Ruby, Node.js, and .NET Core Nuclio functions is currently not supported.\n  Offline deployment of Go Nuclio functions requires implementing a solution in which the necessary module files are available during the build. The reason for this is that platform version 3.2.1 uses Nuclio version 1.6, which uses Go modules, which download module files dynamically. You can bypass this problem, for example, by doing either of the following:\n Use Nuclio to build the function on an environment with an internet connection, and then push the function's Docker image to the Docker Registry service of your air-gapped platform environment. Once the Docker image is available in the system's Docker Registry, you can deploy a Nuclio function using this Docker image. Set up a Go proxy in your offline platform environment that contains the required modules for building the function, and use it to serve the modules for the build. Note that this implementation is outside the scope of the Iguazio support.      Scale to zero Functions that have been scaled to zero can only be awoken by using an HTTP trigger.\nNote that you can avoid scaling a function to zero by setting its minimum-replicas configuration parameter to a value higher than zero.\n   Web APIs NoSQL Web API  Maximum JSON body size 2 MB  Parallel range scan GetItems doesn't support combining parallel table scans and range scans in the same request.   Streaming Web API  Maximum PutRecords JSON body size 10 MB  Maximum record size 2 MB  Maximum number of records that can be added in a single PutRecords operation 1,000  Maximum overall size of records that can be retrieved in a single GetRecords operation 10 MB   Jupyter  Scala notebooks Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.  Copying and moving (renaming) directories  The JupyterLab UI handles directory renaming by using the shutil.move command. Because file-system moving (renaming) of directories isn't supported in the platform, the command tries to execute the move by recursively copying the directory to the new location, and then deleting the original directory. To prevent possible data loss as a result of such a recursive copy (which copies only file objects and not additional metadata, such as object attributes), the platform blocks renaming of directories with nested directories in the Jupyter Notebook service. You can still move such directories, when necessary, by running recursive file-system copy and delete commands from a command line. (Note that Jupyter automatically creates a hidden directory for each notebook, therefore every directory with a notebook has a nested directory and cannot be moved from the JupyterLab UI in the platform.) See also the FS directory copy and move restrictions. If a notebook or other file remains open while the parent directory is being moved (renamed), when the move completes you need to re-select the notebook's kernel (such as \u0026quot;Python 3\u0026quot;) from the JupyterLab UI before running code from this notebook.    Files and directories deletion from the Jupyter UI To allow deletion of directories (folders) from the Jupyter UI, the trash mechanism is disabled for both files and directories, so deleted items are not moved to the trash and cannot be restored.   V3IO TSDB NoteSee also the TSDB Nuclio functions specifications and restrictions.   TSDB metric samples TSDB metric-sample values can be of type integer or float and cannot contain periods ('.') or commas (','). Note that all values for a given metric must be of the same type.  TSDB labels TSDB label values must be of type string and cannot contain commas (',').  In addition, Frames doesn't support labels named \u0026quot;time\u0026quot; (as this is reserved for the name of the sample ingestion-time attribute).  TSDB queries   TSDB queries can be applied only to sample metrics and not to labels. However, you can use labels in query filters (for example, select cpu where host='A' and os='linux' — where \u0026quot;cpu\u0026quot; is a metric name and \u0026quot;host\u0026quot; and \u0026quot;os\u0026quot; are label names).\n   The SQL query syntax [Tech Preview] doesn't support uppercase letters in keyword names; for example, use select and not SELECT.\n     V3IO Frames  Maximum write DataFrame size 120 MB  Attribute data types See the Frames Attribute Data Types reference for the supported item-attribute data types.  Partitioned tables The NoSQL backend (\u0026quot;nosql\u0026quot;|\u0026quot;kv\u0026quot;) doesn't support writing partitioned tables (although you can read partitioned tables with Frames).   Spark NoteFor issues related to running Spark from a Jupyter Notebook, see the Jupyter section.  NoSQL Spark DataFrame  Write-buffer size The default size of write buffers for NoSQL Spark DataFrames is 24 KB. The buffer size is automatically adjusted according to the specific write request but this affects the performance.  Non-string sorting key Range-scan queries on a table with a non-string sorting-key attribute ignore the items' sorting-key values: the query's sharding-key value is still used to identify the relevant data slice more quickly, but the entire slice is scanned rather than scanning only the items within the query's sorting-keys range (as done for a string sorting key). The reason is that the lexicographic sorting-key sort order that's used for storing the items on the data slice might not match the logical sort order for a non-string sorting key. Therefore, for faster range scans, use string sorting keys.   Spark Streaming  Shard-count increase Consuming stream records from new shards after increasing a stream's shard count (using the UpdateStream web-API operation) requires first restarting the Spark Streaming consumer application.   Presto and Hive  Supported operations The Iguazio Presto connector supports the CREATE VIEW, DROP VIEW, SELECT, SHOW CATALOGS, SHOW CREATE VIEW, SHOW FUNCTIONS, SHOW SCHEMAS, and SHOW TABLES queries and the custom v3io.schema.infer command. Note:\n  SHOW TABLES returns only tables that reside in the container's root directory, provided the access key includes data-access permissions for this directory.\n  The view commands (CREATE VIEW, DROP VIEW, and SHOW CREATE VIEW) require that you first enable Hive for the platform's Presto service.\n    Non-string sorting key Range-scan queries on a table with a non-string sorting-key attribute ignore the items' sorting-key values: the query's sharding-key value is still used to identify the relevant data slice more quickly, but the entire slice is scanned rather than scanning only the items within the query's sorting-keys range (as done for a string sorting key). The reason is that the lexicographic sorting-key sort order that's used for storing the items on the data slice might not match the logical sort order for a non-string sorting key. Therefore, for faster range scans, use string sorting keys.   Grafana  Service restart Changes to the Grafana UI — including custom dashboards and data sources — might not be retained when restarting the Grafana service.   Web Shell  Restricted command shell The web-based shell service doesn't provide a fully functional Linux shell. It's designed mainly for running application services — such as Spark jobs or Presto queries — and for performing basic file-system operations, but it doesn't support all Linux commands and tools.   Dashboard (UI) Maximum number of concurrent dashboard users 30  Maximum upload file size 2 GB   Backup, Recovery, and High Availability (HA)  Stream-data backup Stream data is not backed up and restored as part of the platform's backup and upgrade operations.  Data duplication during system-failure recovery Automated system-failure recovery for ingested stream records, data appended to a simple object, or update expressions might result in duplicate data writes.  Restricted operations in degraded mode The following restrictions are applicable when the cluster in the degraded mode:\n  Create and delete container operations aren't permitted.\n  The file-system interface doesn't return information about the number of objects in container directories (as reflected in the dashboard's Number of objects directory-metadata field when browsing a data container).\n    System-attributes backup Objects' modification-time (__mtime_*) and creation-time (__ctime_*) system attributes aren't restored as part of the platform's backup and upgrade operations.\n  See Also  Reserved Names Support and Certification Matrix Deployment and Specifications Release Notes   ","keywords":["software","specifications,","software","specs,","software,","specifications,","specs,","restrictions,","limitations,","known","issues,","names,","object","names,","item","names,","attribute","names,","user","names,","tenant","name,","backup,","recovery,","security,","user","management,","web","apis,","spark,","spark","dataframes,","nosql","spark","dataframe,","nosql","dataframe,","presto,","range","scan,","primary","key,","sharding","key,","sorting","key"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/sw-specifications/","title":"Software Specifications and Restrictions"},{"content":" Browse reference documentation for the MLOps Platform's Python and Scala Apache Spark APIs for working with NoSQL (key-value) and stream data.\n","keywords":["spark","apis,","api","reference,","spark,","spark","datasets,","spark","dataframes,","spark","streaming,","scala,","python"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/","title":"Spark API References"},{"content":"The platform currently supports the following Spark DataFrame data types. For a description of the Spark types, see the Spark SQL data types documentation For a general reference of the attribute data types that are supported in the platform, see the Attribute Data Types Reference.\nNoteThe platform implicitly converts between Spark DataFrame column data types and platform table-schema attribute data types, and converts integer (IntegerType) and short (ShortType) values to long values (LongType / \u0026quot;long\u0026quot;) and floating-point values (FloatType) to double-precision values (DoubleType / \u0026quot;double\u0026quot;). The \u0026quot;Schema of Data Type\u0026quot; column in the following table indicates the matching platform attribute data type for each Spark data type.  Spark Data Type Schema Data Type  BooleanType \u0026quot;boolean\u0026quot;  DoubleType \u0026quot;double\u0026quot;  FloatType \u0026quot;double\u0026quot;  IntegerType \u0026quot;long\u0026quot;  LongType \u0026quot;long\u0026quot;  NullType \u0026quot;null\u0026quot;  ShortType \u0026quot;long\u0026quot;  StringType \u0026quot;string\u0026quot;  TimestampType \u0026quot;timestamp\u0026quot;   See Also  Objects Attributes Attribute Data Types Reference Spark Datasets Overview The NoSQL Spark DataFrame NoSQL Table Schema Reference  ","keywords":["spark","dataframe","data","types,","spark","data","types,","spark,","spark","datasets,","spark","dataframes,","data","types,","BooleanType,","DoubleType,","FloatType,","IntegerType,","LongType,","NullType,","ShortType,","StringType,","type","conersion,","double,","float,","integer,","long,","sql,","nosql,","nosql","table","schema,","table","schema,","schema","types,","attributes,","attribute","data","types,","TimestampType,","timestamps"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-datasets/spark-df-data-types/","title":"Spark DataFrame Data Types"},{"content":"\n","keywords":["spark","datasets,","spark","dataframes,","api","reference,","spark,","nosql"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-datasets/","title":"Spark Datasets Python and Scala API References"},{"content":"  Introduction The platform supports the following standard Apache Spark APIs and custom extensions and APIs for working with data over the Spark engine:\n  Spark Datasets — you can consume and update data in the platform by using Apache Spark SQL Datasets/DataFrames. You can also extend the standard Spark DataFrames functionality by using the platform's custom NoSQL Spark DataFrame data source. See Spark Datasets API Reference.\n  Spark Streaming API — you can use the platform's Spark-Streaming Integration Scala API to map platform streams to Spark input streams, and then use the Apache Spark Streaming API to consume data and metadata from these streams. See Spark-Streaming Integration API Reference.\n   Note that the platform's NoSQL Web API extends the functionality provided by the Spark APIs and related platform extensions. This API supports various item update modes, conditional-update logic and the use of update expressions, and the ability to define counter attributes. For more information, see NoSQL Web API Reference.\nYou can run Spark jobs in the platform using standard industry tools. For example, you can run spark-submit from a web-based shell or Jupyter Notebook service, or run Spark jobs from a web notebook such as Jupyter Notebook, provided the service is connected to a Spark service. All these platform interfaces have a predefined SPARK_HOME environment variable that maps to the Spark installation directory. The spark-installation binaries directory ($SPARK_HOME/bin) contains the required binaries and shell scripts for running Spark; this directory is included in the environment path ($PATH) to simplify execution from any directory. The installation also includes the required library files for using the platform's Spark APIs and the built-in Spark examples.\nNoteIt's good practice to create a Spark session at the start of the execution flow (for example, by calling SparkSession.builder and assigning the result to a spark variable) and stop the session at the end of the flow to release its resources (for example, by calling spark.stop()).  Running Spark Jobs with spark-submit You can run Spark jobs by executing spark-submit from the UI of a web-based shell service or from a terminal or notebook in the UI of a Jupyter Notebook service, provided the service is connected to a Spark service. For detailed information about spark-submit, see the Submitting Applications Spark documentation. spark-submit is mapped to the location of the script ($SPARK_HOME/bin/spark-submit), so you can run spark-submit without specifying the path.\nThe master URL of the Spark cluster is preconfigured in the environments of the platform web-based shell and Jupyter Notebook services. Do not use the --master option to override this configuration. The library files for the built-in Spark examples are found at $SPARK_HOME/examples/jars. You can run the following command, for example, to execute the SparkPi example, which calculates the value of pi:\nspark-submit --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples*.jar 10 When the command succeeds, the output should contain the following line:\nPi is roughly 3.1432911432911435 To refer spark-submit to your own Spark application or library (JAR) file, upload the file to one of your cluster's data containers, and then specify the path to the file by using the v3io cluster data mount — /v3io/\u0026lt;container name\u0026gt;/\u0026lt;path to file\u0026gt;. For example, the following command runs a myapp.py Python application that is found in a pyspark_apps directory in the \u0026quot;projects\u0026quot; container:\nspark-submit /v3io/projects/pyspark_apps/myapp.py Deploy Modes Client Deployment By default, spark-submit launches applications using the client deploy mode. In this mode, the driver is launched in the same worker process as the client that submits the application (such as Jupyter Notebook, or a web shell).\nCluster Deployment You can optionally submit Spark jobs using the cluster deploy mode by adding --deploy-mode=cluster to the spark-submit call. In this mode, the driver is launched from a worker process in the cluster. This mode is supported for Scala and Java; Spark doesn't currently support Python in standalone clusters.\nCluster deployment provides a variety of advantages such as the ability to automate jobs execution and run Spark jobs remotely on the cluster — which is useful, for example, for running ongoing Spark jobs, such as streaming.\nRunning Spark Jobs from a Web Notebook One way to run Spark jobs is from a web notebook for interactive analytics. The platform comes preinstalled with an open-source web-notebook application — Jupyter Notebook. (See Support and Certification Matrix and The Platform's Application Services). For more information about these tools and how to use them to run Spark jobs, see the respective third-party product documentation.\nSee Also  The Spark Service Spark software specifications and restrictions  ","keywords":["spark","apis,","spark,","spark","datasets,","spark","dataframes,","sql,","nosql,","nosql","dataframe,","spark","streaming,","spark","streaming","api,","spark","streaming","integration","api,","streaming,","scala,","python,","spark-submit"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/overview/","title":"Overview of the Spark APIs"},{"content":"Using Spark DataFrames A Spark Dataset is an abstraction of a distributed data collection that provides a common way to access a variety of data sources. A DataFrame is a Dataset that is organized into named columns (\u0026quot;attributes\u0026quot; in the platform's unified data model). See the Spark SQL, DataFrames and Datasets Guide. You can use the Spark SQL Datasets/DataFrames API to access data that is stored in the platform. In addition, the platform's Iguazio Spark connector defines a custom data source that enables reading and writing data in the platform's NoSQL store using Spark DataFrames — including support for table partitioning, data pruning and filtering (predicate pushdown), performing \u0026quot;replace\u0026quot; mode and conditional updates, defining and updating counter table attributes (columns), and performing optimized range scans. For more information, see The NoSQL Spark DataFrame. See Spark DataFrame Data Types for the data types that are currently supported in the platform.\nSpark DataFrames and Tables A DataFrame consumes and updates data in a table, which is a collection of data objects — items (rows) — and their attributes (columns). The attribute name is the column name, and its value is the data stored in the relevant item (row). See also Working with NoSQL Data. As with all data in the platform, the tables are stored within data containers.\nWhen writing (ingesting) data to a table with a Spark DataFrame, you need to set the key option to a column (attribute) that identifies the table's sharding key — the sharding-key attribute. With the NoSQL DataFrame, you can also optionally set the custom sorting-key write option to an attribute that identifies the table's sorting key — the sorting-key attribute. The combination of these keys makes up the table's primary key and is used to uniquely identify items in the table. When no sorting-key attribute is defined, the sharding-key attribute is also the table's primary-key attribute (identity column). See also Object Names and Primary Keys.\nData Paths When using Spark DataFrames to access data in the platform's data containers, provide the path to the data as a fully qualified v3io path of the following format — where \u0026lt;container name\u0026gt; is the name of the parent data container and \u0026lt;data path\u0026gt; is the relative path to the data within the specified container:\nv3io://\u0026lt;container name\u0026gt;/\u0026lt;data path\u0026gt; You pass the path as a string parameter to the relevant Spark method for the operation that you're performing — such as load or csv for read or save for write. For example, save(\u0026quot;v3io://mycontainer/mytable\u0026quot;) or csv(\u0026quot;v3io://mycontainer/mycoldata.csv\u0026quot;). For additional information and examples, see the NoSQL Spark DataFrame reference and the Getting Started with Data Ingestion Using Spark tutorial.\nSee Also  Ingesting and preparing data using Spark Getting Started with Data Ingestion Using Spark The NoSQL Spark DataFrame Spark DataFrame Data Types Data Objects  ","keywords":["spark","datasets,","spark","dataframes,","spark,","nosql","dataframe,","Iguazio","Spark","connector,","nosql,","nosql","tables,","counters,","counter","attributes,","data","paths,","table","paths,","v3io,","table","items,","rows,","attributes,","columns,","primary","key,","identity","column,","sharding","key,","sorting","key,","range","scan,","spark","dataframe","data","types,","spark","data","types,","containers,","scala,","python"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-datasets/overview/","title":"Overview of Spark Datasets"},{"content":"The platform's Spark-Streaming Integration API provides a way for consuming stream data using the Apache Spark Streaming API. The integration API exposes a V3IOUtil object that contains a createDirectStream method for mapping platform V3IO streams to a Spark input stream. You can use the Spark input streams that you create with this method to consume record data and metadata from platform streams via the Spark Streaming API.\nThis section documents the platform's Spark-Streaming Integration Scala API for Spark v3.1.2, which is provided in the org.apache.spark.streaming.v3io package.\n","keywords":["spark-streaming","integration","api,","spark,","spark","streaming,","spark","streaming","api,","streaming,","streams,","stream","records,","stream","consumption,","V3IOUtil,","createDirectStream,","spark","input","streams,","DStream,","InputDStream,","record","metadata,","scala,","ride-hailing","example,","taxi_streaming,","consume_drivers_stream_data.py"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/overview/","title":"Overview of the Spark-Streaming Integration API"},{"content":"\n","keywords":["spark-streaming","integration","api,","api","reference,","spark,","spark","streaming,","streaming,","scala"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/","title":"Spark-Streaming Integration Scala API Reference"},{"content":"This section describes platform Scala types for encoding and decoding stream data. The types are defined in the io.iguaz.v3io.spark.streaming package.\n","keywords":["stream-data","encoding","types,","stream-data","decoding","types,","spark","streaming,","streams,","encoding","types,","decoding","types,","encoding,","decoding,","encoder,","decoder,","data","types,","scala"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/encoding-decoding-types/","title":"Stream-Data Encoding and Decoding Types"},{"content":" ","keywords":["streaming","web","api,","api","reference,","streaming"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/","title":"Streaming Web API Reference"},{"content":"Fully Qualified Name io.iguaz.v3io.spark.streaming.Decoder.StringDecoder\nDescription A class for defining a Decoder object that converts a byte array into a string.\nSummary Instance Constructors\nclass StringDecoder(config: Option[Properties] = None) extends Decoder[String] Methods   fromBytes\ndef fromBytes(bytes: Array[Byte])( implicit encoding: Option[Properties] =\u0026gt; Charset) : String   Instance Constructors Syntax new StringDecoder(config: Option[Properties]) Parameters and Data Members  config Optional properties that are passed to the encoding function of the class's fromBytes method.\n Type: Option[Properties]   Requirement: Optional    fromBytes Method Converts a byte array into a string.\nSyntax fromBytes(bytes: Array[Byte])( implicit encoding: Option[Properties] =\u0026gt; Charset) : String Parameters  bytes The byte array to convert (decode).\n Type: Array[Bytes]   Requirement: Required   encoding An implicit function that returns the character encoding to use in the conversion.\n Type: encoding: Option[Properties] =\u0026gt; Charset)   Requirement: Implicit    Return Value Returns the converted (decoded) string.\n","keywords":["StringDecoder,","spark","streaming,","decoding"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/encoding-decoding-types/stringdecoder/","title":"StringDecoder Class"},{"content":" Overview The platform comes pre-deployed with proprietary and third-party open-source tools and libraries that are exposed as application services that are managed using Kubernetes. Relevant services can be viewed and managed by users from the platform dashboard using a self-service model. (Note that some services that don't require user intervention aren't visible in the dashboard.) Users can also enhance their development experience by independently installing additional software and run it on top of the platform services. For more information, see The Platform's Application Services.\nThe platform has two types of managed application services:\n Default services There are several service instances — such as Presto and the web APIs — that are spawn automatically when the platform starts and have a tenant-wide scope (i.e., they're accessible to all tenant users with service permissions). The default services can't be deleted by users, but service administrators can disable or restart these services and modify some service configurations. User-defined services Service administrators can create a wide variety of new service instances for certified services — such as Spark and Jupyter Notebook. Except where otherwise specified, user assigned services should be assigned to a specific running user but can optionally be shared also with all other tenant users with service permissions.  Tech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Pre-deployed Application Services and Tools The following software packages, services, and tools are pre-deployed as part of the default version 3.2.1 platform installation:\nConda | Dashboard | Docker Registry | Elasticsearch | Frames | Grafana | Hadoop | Hive | Horovod | Jupyter | Kubernetes | Log Forwarder | MLRun | Monitoring | MPI Operator | Nuclio Serverless Framework | Operating System | Pipelines | Presto | OAuth2 (OIDC) Authenticator | Prometheus | Spark | Spark Operator | TSDB CLI (V3IO) | V3IO Daemon | Web APIs | Web Shell Note Open-source tools and related services are subject to open-source restrictions. See Application Library Versions for programming application library versions that are used or certified for usage by relevant services, such as Nuclio and Jupyter Notebook.    Service  Type  Description  Version   Dashboard Default The platform's graphical user interface. 3.2.1  Kubernetes Default The Kubernetes (k8s) container orchestration system for automating deployment, scaling, and management of containerized applications. Application services in the platform run on top of Kubernetes. k8s 1.20\n Helm 2.14.1  Kubernetes on cloud Default The managed Kubernetes (k8s) (EKS, AKS, GKE). k8s 1.20 Nuclio Serverless Framework (nuclio) Default Iguazio's Nuclio Enterprise Edition serverless framework for development, deployment, and execution of serverless functions for real-time data processing and analysis. The Nuclio dashboard is available as part of the Projects area of the platform dashboard (for users with the Function Admin management policy). For more information, see Nuclio Serverless Functions. 1.6  MLRun (mlrun) Default Iguazio's https://github.com/mlrun/mlrun/tree/release/v0.8.x-latest/ open-source machine-learning operations (MLOps) orchestration framework for automating and tracking data science tasks and full workflows, including integration with Kubeflow Pipelines and the Nuclio serverless framework. For more information, see Data Science Automation (MLOps) Services. 0.8  Pipelines (pipelines) Default The Google Kubeflow Pipelines open-source framework for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers. For more information, see Data Science Automation (MLOps) Services. 1.0.1  Web APIs (webapi) Default The platform's web-APIs (web-gateway) service, which provides access to its web APIs. 3.2.1  Jupyter User-defined The JupyterLab UI, including the Jupyter Notebook web application and shell terminals and the Conda binary package and environment manager. For more information, see The Jupyter Notebook Service. See also the Jupyter application-libraries compatibility matrix. JupyterLab 2.2.0  V3IO TSDB CLI Default The Iguazio V3IO Time-Series Database (TSDB) command-line interface (CLI) tool (tsdbctl) for creating and managing time-series databases (TSDBs) using the Iguazio V3IO TSDB library. 0.11  V3IO Prometheus User-defined The Iguazio V3IO Prometheus distribution, which provides a version of the Prometheus systems monitoring and alerting toolkit that is packaged with the V3IO TSDB library and can be used to query time-series databases in the platform. 3.5\n Prometheus 3.5.8  V3IO Frames (framesd) User-defined The platform's V3IO Frames service, which provides access to the Frames API — an open-source unified high-performance Python DataFrame API for accessing NoSQL, stream, and time-series data in the platform's data store. 0.8 (server)\n 0.8 (supported client)  Spark User-defined The Apache Spark data-processing engine, including the following libraries:\n SQL and DataFrames Streaming MLlib for machine learning GraphX for graphs and graph-parallel computation   3.1.2  Spark Operator (spark-operator) Default The spark-on-k8s-operator Kubernetes Operator for Spark (\u0026quot;Spark Operator\u0026quot;), which enables simplifying submission and scheduling of Spark jobs. This service is designed to be used via the MLRun Spark Operator API. 3.1.2  Presto (presto) Default The Presto distributed SQL query engine for big data. 359  Hive Metastore Internal An internal Apache Hive Metastore service that can be enabled for the Presto service to allow saving views and using the Presto Hive connector. 3.1.2  Horovod / MPI Operator (mpi-operator) Default Distributed training using Kubeflow MPI Operator and Uber's Horovod distributed deep-learning framework for creating machine-learning models that are trained simultaneously over multiple GPUs or CPUs. For more information, see The MPI-Operator Horovod Service and Running Applications over GPUs. 0.2.3   Log Forwarder (log-forwarder) Default A platform service that uses Filebeat to forward application-service logs to be stored and indexed in an instance of the Elasticsearch search and analytics engine.  Note that this default service is disabled by default because you need to configure the URL of an Elasticsearch service for storing and indexing the logs. For more information, see Logging, Monitoring, and Debugging. 7.8.0\n Supports Elasticsearch 7.10  Monitoring (monitoring) Default A platform service for monitoring application services and gathering performance statistics and additional data. The gathered data is visualized on Grafana dashboards using the platform's Grafana services. For more information, see Monitoring Platform Services. 3.5  Grafana User-defined The Grafana analytics and monitoring platform.  In cloud platform environments, Grafana is currently available as a shared single-instance tenant-wide service.  The platform also has a shared single-instance tenant-wide application-cluster Grafana service with monitoring dashboards for the entire Kubernetes application cluster, which isn't visible on the Services dashboard page but is accessible from the Clusters page. For more information, see Monitoring Platform Services. 7.2  Docker Registry Default and user-defined A platform service for working with a Docker Registry, which is used by the Nuclio service to store the function images. You can create a Docker Registry service and configure it to work with a remote off-cluster Docker Registry. On the default tenant, the Nuclio service is configured by default to work with a pre-deployed default tenant-wide docker-registry service that uses a pre-deployed local on-cluster Docker Registry. 2.7.1  OAuth2 (OIDC) Authenticator (authenticator) Default A federated OpenID Connect (OIDC) provider over OAuth2, using OpenID Connect (OIDC). This service is used for OAuth2 authentication of user access to Nuclio API gateways and shared Grafana services, including access by external (non-platform) users. 2.23.0  Web Shell User-defined A platform service that provides a web-based command-line shell (\u0026quot;web shell\u0026quot;) for running application services — such as Spark jobs and Presto queries — and performing basic file-system operations.  Note that this isn't a fully functional Linux shell. For more information, see The Web-Shell Service. 3.2.1  Hadoop Default The Apache Hadoop distributed data-processing library. For more information, see The Hadoop Service. 3.2  V3IO Daemon (v3io-daemon) Default (internal) An internal service for integrating the platform with external applications by using the platform's V3IO library. 3.2.1   Operating System Internal The CentOS Linux operating-system. 7.6   Application Library Versions The following table provides information about the versions of application libraries (packages) that are used or certified for usage with different pre-deployed platform tools and services:\n Service  Pre-deployed and Certified Application Library Versions   Platform API Libraries Java 8\n Scala 2.11  Frames Python 3.7  Nuclio Python 3.6, 3.7, and 2.7  Jupyter Iguazio platform tutorials (v3io/tutorials) 3.0\n Iguazio V3IO Python SDK (v3io/v3io-py) 0.5 (Python 3.5–3.8)  Nuclio Jupyter package (nuclio/nuclio-jupyter) 0.8  Python 3.7  Conda 4.8.3  NVIDIA CUDA 11.0  NVIDIA RAPIDS 0.17  Frames client 0.8   Web Shell Python 3.7\n Scala 2.11   Integration with Additional Tools You can independently install additional software tools — such as TensorFlow, PyTorch, or scikit-learn — and use them on top of the platform services. You can also configure remotely installed tools — such as Tableau or Looker — to analyze and visualize data in the platform. In addition, you can use Conda (which is available as part of the platform's Jupyter Notebook service) and pip (which is available as part of the Jupyter Notebook, Zeppelin, and web-shell services) to install Python packages. For more information, see The Platform's Application Services.\nNoteServices that are installed locally on the platform need to run on top of Kubernetes.  See Also  Platform Services Introducing the Platform Software Specifications and Restrictions Deployment and Specifications  ","keywords":["support","and","certification","matrix,","support","matrix,","certification","matrix,","certified","software,","certified","packages,","services,","application","services,","specifications,","software","specifications,","software","specs,","software,","specifications,","specs,","software","compatibility,","compatibility,","installation,","default","installation,","urls,","ports,","endpoints,","dashboard,","ui,","opearting","system,","OS,","centos,","Linux,","Linux","distribution,","application","nodes,","apache,","authenticator,","conda,","cuda,","dex,","docker","registry,","elasticsearch,","github,","github","authentication,","grafana,","hadoop,","hive,","horovod,","jupyter,","jupyterlab,","jupyter","notebook,","kubernetes,","log","forwarder,","logging","services,","logging,","monitoring","service,","monitoring,","nuclio,","nuclio","jupyter,","oauth2,","oicd,","openid","connect,","openid,","presto,","prometheus,","serverless,","spark,","spark","operator,","spark-on-k8s-operator,","tensorflow,","tsdb,","tsdb","cli,","tsdbctl,","v3io","daemon,","v3io,","web","apis,","web","gateway,","web","server,","nginx,","web","shell,","libraries,","java,","python,","scala,","tableau,","tech","preview"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/support-matrix/","title":"Support and Certification Matrix"},{"content":" The platform defines the following object (item) system attributes. For more information, see the description of system attributes in the attribute-types introduction.\nNoteThe names of the predefined system attributes are reserved names in the platform. For more information, see Reserved Names.  Attribute Description  __name Object name, which uniquely identifies the object within a collection and serves as the value of the object's primary key. See Object Names and Primary Keys. __size Object size.  __mode Object access control list (ACL).  __atime_secs Access time in seconds, as a Unix timestamp.  __atime_nsecs Nano-seconds resolution for the __atime_secs access time.  __mtime_secs Last modification time in seconds, as a Unix timestamp.  __mtime_nsecs Nano-seconds resolution for the __mtime_secs last modification time.  __ctime_secs Creation time in seconds, as a Unix timestamp.  __ctime_nsecs Nano-seconds resolution for the __ctime_nsecs creation time.  __uid Unique identifier (UID) of the object owner.  __gid Group identifier (GID) of the object owner's group.  __obj_type Object type.  __collection_id Parent-collection ID. A collection can be a stream, a table, or a directory.  __last_sequence_num The sequence number of the last record in stream shard; applicable only to stream-shard objects.    See Also  Objects Attributes  System attributes.   Attribute Data Types Reference  ","keywords":["reference,","attributes,","system","attributes,","attribute","names,","objects,","object","names,","primary","key,","__name"],"path":"https://github.com/jasonnIguazio/data-layer/reference/system-attributes/","title":"System-Attributes Reference"},{"content":"","keywords":null,"path":"https://github.com/jasonnIguazio/tags/","title":"Tags"},{"content":" Get introduced to the platform's data-layer APIs for working with different data formats — including time-series databases, NoSQL (key-value), stream, an simple-object data.\nNoteYou can also browse, ingest, consume, and manipulate data from the dashboard.  ","keywords":["data","layer","apis,","data","fabric","apis,","data","store","apis,","database","apis,","data","layer,","data","fabric,","data","store,","database,","apis","overview,","apis"],"path":"https://github.com/jasonnIguazio/data-layer/apis/","title":"The Data-Layer APIs"},{"content":" The platform provides a self-service and open-source Apache Hadoop framework that makes it easy, fast, and cost-effective to process and analyze vast amounts of data for operational, analytics, and data-engineering needs. The platform's distributed file system (DFS) is an Hadoop Compatible File System (HCFS). You can run Hadoop commands from any platform command-line interface — such as a web-based shell, a Jupyter notebook or terminal, or a Zeppelin notebook — as demonstrated, for example, in the Working with Data Containers tutorial and in the getting-started ingestion examples in the platform's Jupyter tutorial notebook.\nSee Also  Working with Services File-system software specifications and restrictions  ","keywords":["hadoop,","hdfs,","hcfs,","file","system,","command","line,","jupyter,","jupyter","notebook,","jupyter","terminals,","web","shell,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/hadoop/","title":"The Hadoop Service"},{"content":" Overview Jupyter is a project for development of open-source software, standards, and services for interactive computation across multiple programming languages. The Platform comes preinstalled with the JupyterLab web-based user interface, including Jupyter Notebook and JupyterLab Terminals, which are available via a Jupyter Notebook user application service.\nJupyter Notebook is an open-source web application that allows users to create and share documents that contain live code, equations, visualizations, and narrative text; it's currently the leading industry tool for data exploration and training. Jupyter Notebook supports integration with all key analytics services, enabling users to perform all stages of the data science flow, from data collection to production, from a single interface using various APIs and tools to concurrently access the same data without having to move the data. Your Jupyter Notebook code can execute Spark jobs (for example, using Spark DataFrames); run SQL queries using Presto; define, deploy, and trigger Nuclio serverless functions; send web-API requests; use pandas and V3IO Frames DataFrames; use the Dask library to scale the use of pandas DataFrames; and more. You can use Conda and pip, which are available as part of the Jupyter Notebook service, to easily install Python packages such as Dask and machine-learning and computation packages. In addition, you can use Jupyter terminals to execute shell commands, such as file-system and installation commands. As part of the configuration of the platform's Jupyter Notebook service you select a specific Jupyter flavor and you can optionally define environment variables for the service.\nIguazio provides tutorial Jupyter notebooks with code examples ranging from getting-started examples to full end-to-end demo applications, including detailed documentation. Start out by reading the introductory welcome.ipynb notebook (available also as a Markdown README.md file), which is similar to the introduction on the documentation site. Then, proceed to the getting-started tutorial.\nJupyter Flavors In version 3.2.1 of the platform, you can set the custom Flavor parameter of the Jupyter Notebook service to one of the following flavors to install a matching Jupyter Docker image:\n Jupyter Full Stack A full version of Jupyter for execution over central processing units (CPUs). Jupyter Full Stack with GPU A full version of Jupyter for execution over graphics processing units (GPUs). This flavor is available only in environments with GPUs and is sometimes referred to in the documentation as the Jupyter \u0026quot;GPU flavor\u0026quot;. For more information about the platform's GPU support, see Running Applications over GPUs.  See Also  Working with Services The tutorial Jupyter notebooks The Web-Shell Service Python Machine-Learning and Scientific-Computation Packages Dask Data Science Automation (MLOps) Services Running Applications over GPUs Jupyter software specifications and restrictions `  ","keywords":["jupyter,","jupyter","notebook,","jupyterlab,","jupyter","terminals,","web","notebooks,","jupyter","tutorials,","v3io","tutorials,","v3io,","tutorials,","command-line","shell,","command","line,","shell,","spark,","presto,","presto","cli,","sql","queries,","sql,","python,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/jupyter/","title":"The Jupyter Notebook Service"},{"content":" The platform has a default (pre-deployed) shared single-instance tenant-wide Kubeflow MPI Operator service (mpi-operator), which facilitates Uber's Horovod distributed deep-learning framework. Horovod, which is already preinstalled as part of the platform's Jupyter Notebook service, is widely used for creating machine-learning models that are trained simultaneously over multiple GPUs or CPUs. For more information about using the Horovod to run applications over GPUs, see Running Applications over GPUs.\nSee Also  Running Applications over GPUs Working with Services The Jupyter Notebook Service  ","keywords":["horovod,","kubeflow","mpi","operator,","mpi","operator,","mpi","jobs,","gpu","spport,","gpu,","cpu,","uber,","deep","learning,","machine","learning,","ml,","ml","models,","model","training,","performance,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/horovod-mpi-operator/","title":"The MPI-Operator Horovod Service"},{"content":"Introduction The platform includes the Iguazio Spark connector, which defines a custom Spark data source for reading and writing NoSQL data in the platform's NoSQL store using Spark DataFrames. A Spark DataFrame of this data-source format is referred to in the documentation as a NoSQL DataFrame. This data source supports data pruning and filtering (predicate pushdown), which allows Spark queries to operate on a smaller amount of data; only the data that is required by the active job is loaded. The data source also allows you to work with partitioned tables; perform \u0026quot;replace\u0026quot; mode and conditional item updates; define specific item attributes as counter attributes and easily increment or decrement their values; and perform optimized range scans. Data Source To use the Iguazio Spark connector to read or write NoSQL data in the platform, use the format method to set the DataFrame's data-source format to the platform's custom NoSQL data source — \u0026quot;io.iguaz.v3io.spark.sql.kv\u0026quot;. See the following read and write examples:  import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() val df = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://mycontainer/src_table\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;id\u0026#34;).save(\u0026#34;v3io://mycontainer/dest_table\u0026#34;)  from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() df = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://mycontainer/src_table\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;id\u0026#34;).save(\u0026#34;v3io://mycontainer/dest_table\u0026#34;)    Table Paths Specify the path to the NoSQL table that is associated with the DataFrame as a fully qualified v3io path of the following format — where \u0026lt;container name\u0026gt; is the name of the table's parent data container and \u0026lt;data path\u0026gt; is the relative path to the table within the specified container (see Data Paths in the Spark Datasets overview):\nv3io://\u0026lt;container name\u0026gt;/\u0026lt;table path\u0026gt; Examples The following example uses a Spark DataFrame to create a NoSQL table named \u0026quot;cars\u0026quot; in a mydata directory in the \u0026quot;projects\u0026quot; container, and then reads the contents of the table into a DataFrame:\n val nosql_source = \u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34; var table_path = \u0026#34;v3io://projects/mydata/cars/\u0026#34; val writeDF = Seq( (\u0026#34;7843321\u0026#34;, \u0026#34;Honda\u0026#34;, \u0026#34;Accord\u0026#34;, \u0026#34;silver\u0026#34;, 123000), (\u0026#34;2899941\u0026#34;, \u0026#34;Ford\u0026#34;, \u0026#34;Mustang\u0026#34;, \u0026#34;white\u0026#34;, 72531), (\u0026#34;6689123\u0026#34;, \u0026#34;Kia\u0026#34;, \u0026#34;Picanto\u0026#34;, \u0026#34;red\u0026#34;, 29320) ) writeDF.toDF(\u0026#34;reg_license\u0026#34;, \u0026#34;vendor\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;color\u0026#34;, \u0026#34;odometer\u0026#34;) .write.format(nosql_source) .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .save(table_path) val readDF = spark.read.format(nosql_source).load(table_path) readDF.show()  import sys from pyspark.sql import * from pyspark.sql.functions import * nosql_source = \u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34; table_path = \u0026#34;v3io://projects/mydata/cars/\u0026#34; writeDF = spark.createDataFrame([ (\u0026#34;7843321\u0026#34;, \u0026#34;Honda\u0026#34;, \u0026#34;Accord\u0026#34;, \u0026#34;silver\u0026#34;, 123000), (\u0026#34;2899941\u0026#34;, \u0026#34;Ford\u0026#34;, \u0026#34;Mustang\u0026#34;, \u0026#34;white\u0026#34;, 72531), (\u0026#34;6689123\u0026#34;, \u0026#34;Kia\u0026#34;, \u0026#34;Picanto\u0026#34;, \u0026#34;red\u0026#34;, 29320), ], [\u0026#34;reg_license\u0026#34;, \u0026#34;vendor\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;color\u0026#34;, \u0026#34;odometer\u0026#34;]) writeDF.write.format(nosql_source) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .save(table_path) readDF = spark.read.format(nosql_source).load(table_path) readDF.show()    The following code shows several equivalent alternatives for changing the table path from the previous example to a \u0026quot;cars\u0026quot; table in the running-user directory of the \u0026quot;users\u0026quot; container; (note the Python code requires that you add import os). All variations except for the first one use environment variables instead of explicitly specifying the name of the running user (\u0026quot;iguazio\u0026quot; in the example):\n table_path = \u0026#34;v3io://users/iguazio/cars\u0026#34; table_path = \u0026#34;v3io://users/\u0026#34; + System.getenv(\u0026#34;V3IO_USERNAME\u0026#34;) + \u0026#34;/cars\u0026#34; table_path = \u0026#34;v3io://\u0026#34; + System.getenv(\u0026#34;V3IO_HOME\u0026#34;) + \u0026#34;/cars\u0026#34; table_path = System.getenv(\u0026#34;V3IO_HOME_URL\u0026#34;) + \u0026#34;/cars\u0026#34;  table_path = \u0026#34;v3io://users/iguazio/cars\u0026#34; table_path = \u0026#34;v3io://users/\u0026#34; + os.getenv(\u0026#34;V3IO_USERNAME\u0026#34;) + \u0026#34;/cars\u0026#34; table_path = \u0026#34;v3io://\u0026#34; + os.getenv(\u0026#34;V3IO_HOME\u0026#34;) + \u0026#34;/cars\u0026#34; table_path = os.getenv(\u0026#34;V3IO_HOME_URL\u0026#34;) + \u0026#34;/cars\u0026#34;    For additional examples, see the Examples section on this page.\nSave Modes The Iguazio Spark connector supports the standard Spark DataFrame save modes, which can be set using the Spark DataFrame mode method when writing data from a NoSQL DataFrame to a table. For more information and examples, see the Spark SQL, DataFrames and Datasets Guide and the NoSQL DataFrame write examples.\nNoteIn some situations — such as when using the columnUpdate or counter write options — the append save mode behaves differently when used with the Iguazio Spark connector, as outlined in the platform documentation.  Options Use the Spark DataFrame option method to configure relevant options. See the Apache Spark Datasets documentation for the built-in options. In addition, the Iguazio Spark connector supports the following custom NoSQL DataFrame options:\nRead Options  inferSchema Set this option to true (option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;)) to instruct the platform to infer the schema of the NoSQL data that is being read. See Inferring the Table Schema.\n Type: Boolean   Requirement: Optional    Write Options  key The name of the table's sharding-key attribute (for example, username). This option is used together with the optional sorting-key option to define the table's primary key, which uniquely identifies items within the table (see Spark DataFrames and Tables).\nFor example, for a DataFrame item (row) with a username attribute (column) whose value is \u0026quot;johnd\u0026quot;, calling option(\u0026quot;key\u0026quot;, \u0026quot;username\u0026quot;) without setting the sorting-key option defines a simple username primary key and sets the item's primary-key value (name) to johnd.\nNote  The written DataFrame must contain a compatible attribute (column) whose name matches the value of the key option. Do not modify the value of this attribute after the item's ingestion, as this will result in a mismatch with the item's name and primary-key value (which remains unchanged).\n  The value of the sharding-key attribute cannot contain periods, because the leftmost period in an item's primary-key value is assumed to be a separator between sharding and sorting keys.\n  See the primary-key guidelines in the Best Practices for Defining Primary Keys and Distributing Data Workloads guide.\n     Type: String   Requirement: Required   sorting-key The name of the table's sorting-key attribute (for example, login-date). This option can optionally be used together with the key option to define a compound primary key, which uniquely identifies items within the table (see Spark DataFrames and Tables).\nFor example, for a DataFrame item (row) with a username attribute whose value is \u0026quot;johnd\u0026quot; and a login-date attribute whose value is \u0026quot;20180125\u0026quot;, calling both option(\u0026quot;key\u0026quot;, \u0026quot;username\u0026quot;) and option(\u0026quot;sorting-key\u0026quot;, \u0026quot;login-date\u0026quot;) defines a compound username.login-date primary key and sets the item's primary-key value (name) to johnd.20180125. When using the even-distribution write option, the item's primary-key value will be johnd_\u0026lt;n\u0026gt;.20180125 (for example, johnd_2.20180125) — see Even Workload Distribution.\nNote  The written DataFrame must contain a compatible attribute (column) whose name matches the value of the sorting-key option. Do not modify the value of this attribute after the item's ingestion, as this will result in a mismatch with the item's name and primary-key value.\n  You must set this option if you wish to define a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; table primary key. Note that support for range scans requires a compound primary key and that range scans for tables with a string sorting-key attribute are more efficient. For more information and best-practice guidelines, see Best Practices for Defining Primary Keys and Distributing Data Workloads.\n     Type: String   Requirement: Optional   allow-overwrite-schema Set this option to true (option(\u0026quot;allow-overwrite-schema\u0026quot;, \u0026quot;true\u0026quot;)) to instruct the platform to overwrite the current schema of the target table (if exists) with the schema that is automatically inferred from the contents of the DataFrame. By default, if the inferred schema differs from an existing schema for the same table, the existing schema isn't overwritten and the write fails — see Overwriting an Existing Table Schema.\n Type: Boolean   Requirement: Optional   columnUpdate Set this option to true (option(\u0026quot;columnUpdate\u0026quot;, \u0026quot;true\u0026quot;)) together with the append save mode for a custom replace mode — append new items and overwrite existing items (similar to the update logic of the counter option). See the replace-mode write example.\n Type: Boolean   Requirement: Optional   condition A Boolean condition expression that defines a conditional logic for executing the write operation. See Condition Expression for syntax details and examples. As explained in the expression reference documentation, attributes in the target table item are referenced in the expression by using the attribute name. To reference a column (attribute) in the write DataFrame from within the expression, use the syntax ${\u0026lt;column name\u0026gt;}. For example, option(\u0026quot;condition\u0026quot;, \u0026quot;${version} \u0026gt; version)\u0026quot; will write to the table only if the table has a matching item (identified by its name — see the key option) whose current version attribute value is lower than the value of the version column (attribute) in the Spark DataFrame. For more information, see Conditional Updates and the conditional-update write example.\n Type: String   Requirement: Optional   counter A comma-separated list of one or more attribute (column) names that identify counter attributes. For example, option(\u0026quot;counter\u0026quot;, \u0026quot;odometer, loops\u0026quot;) identifies odometer and loops as counter attributes. For more information, see Counter Attributes and the counter-attributes write example.\nNote Counter attributes must have numeric values. The counter option is supported only with the NoSQL DataFrame append save mode, which for counter attributes functions as a custom replace mode (similar to the update logic of the columnUpdate option). The DataFrame should contain a value for each of the specified counter attributes. This value will be added to or subtracted from the attribute's current value, or used as the initial attribute value if the attribute doesn't already exist in the table.     Type: String   Requirement: Optional   partition [Tech Preview] A comma-separated list of one or more attribute (column) names that identify partition attributes. The written items are saved to \u0026lt;table path\u0026gt;/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;[/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;/...] partition directories according to the values of the items' partition attributes. Note that the order of the partition attribute names in the option string determines the partitioning hierarchy. For example, option(\u0026quot;partition\u0026quot;, \u0026quot;year, month, day, hour\u0026quot;) identifies year, month, day, and hour as partition attributes and saves items in year=\u0026lt;value\u0026gt;/month=\u0026lt;value\u0026gt;/day=\u0026lt;value\u0026gt;/hour=\u0026lt;value\u0026gt; partitions (such as mytable/year=2018/month=2/day=12/hour=21) within the root table directory. For more information and examples, see Partitioned Tables.\n Type: String   Requirement: Optional   range-scan-even-distribution Set this option to true (option(\u0026quot;range-scan-even-distribution\u0026quot;, \u0026quot;true\u0026quot;)) to instruct the platform to distribute items with the same sharding-key attribute value among multiple data slices, to achieve a more even distribution of non-uniform data. This option is applicable only for tables with a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key, which can be created by using both the key and sorting-key write options. For more information, see Even Workload Distribution.\n Type: Boolean   Requirement: Optional    Defining the Table Schema Spark DataFrames handle structured data. Therefore, Spark needs to be aware of the schema of the data structure. When writing NoSQL data by using the platform's Spark DataFrame or Frames APIs, the schema of the data table is automatically identified and saved and then retrieved when reading the data with a Spark DataFrame, Frames, or Presto (unless you select to explicitly define the schema for the read operation). However, to use a Spark DataFrame, Frames, or Presto to read NoSQL data that was written to a table in another way, you first need to define the table schema. You can use either of the following alternative methods to define or update the schema of a NoSQL table as part of a NoSQL DataFrame read operation:\n Use the custom inferSchema option to infer the schema (recommended). Define the schema programmatically as part of the Spark DataFrame read operation. (You can also do this for data that was written using a Spark DataFrame in the platform, although it's not required.)  For more information, see the NoSQL Table Schema Reference.\nOverwriting an Existing Table Schema By default, if the schema inferred from the DataFrame's contents during a write operation differs from the table's current schema — as defined in its schema file (if such a file exists) — the write fails. This is designed to protect against inadvertent schema changes. However, you can override this default behavior by using the custom allow-overwrite-schema write option, which forces an overwrite of the current table schema with the inferred schema.\nTable Schema-Overwrite Examples The following example creates a \u0026quot;mytable\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container with AttrA and AttrB attributes of type string and an AttrC attribute of type long, and then overwrites the table schema to change the type of AttrC to double:\n import org.apache.spark.sql.SparkSession var df = Seq((\u0026#34;a\u0026#34;, \u0026#34;z\u0026#34;, 123), (\u0026#34;b\u0026#34;, \u0026#34;y\u0026#34;, 456)) .toDF(\u0026#34;AttrA\u0026#34;, \u0026#34;AttrB\u0026#34;, \u0026#34;AttrC\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;AttrA\u0026#34;) .save(\u0026#34;v3io://mycontainer/mytable/\u0026#34;) df = Seq((\u0026#34;c\u0026#34;, \u0026#34;x\u0026#34;, 32.12), (\u0026#34;d\u0026#34;, \u0026#34;v\u0026#34;, 45.2)) .toDF(\u0026#34;AttrA\u0026#34;, \u0026#34;AttrB\u0026#34;, \u0026#34;AttrC\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;AttrA\u0026#34;) .option(\u0026#34;allow-overwrite-schema\u0026#34;, \u0026#34;true\u0026#34;) .save(\u0026#34;v3io://mycontainer/mytable/\u0026#34;)  from pyspark.sql import SparkSession df = spark.createDataFrame([ (\u0026#34;a\u0026#34;, \u0026#34;z\u0026#34;, 123), (\u0026#34;b\u0026#34;, \u0026#34;y\u0026#34;, 456) ], [\u0026#34;AttrA\u0026#34;, \u0026#34;AttrB\u0026#34;, \u0026#34;AttrC\u0026#34;]) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;AttrA\u0026#34;) \\ .save(\u0026#34;v3io://mycontainer/mytable/\u0026#34;) df = spark.createDataFrame([ (\u0026#34;c\u0026#34;, \u0026#34;x\u0026#34;, 32.12), (\u0026#34;d\u0026#34;, \u0026#34;v\u0026#34;, 45.2) ], [\u0026#34;AttrA\u0026#34;, \u0026#34;AttrB\u0026#34;, \u0026#34;AttrC\u0026#34;]) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;AttrA\u0026#34;) \\ .option(\u0026#34;allow-overwrite-schema\u0026#34;, \u0026#34;true\u0026#34;) \\ .save(\u0026#34;v3io://mycontainer/mytable/\u0026#34;)     NoteIf you remove or comment out the option(\u0026quot;allow-overwrite-schema\u0026quot;, \u0026quot;true\u0026quot;) call in the second write command, the write will fail with the following schema-mismatch error:\njava.lang.RuntimeException: Note you are about the rewrite existing schema file. old schema = Schema(List(Field(AttrA,string,false,None), Field(AttrB,string,true,None), Field(AttrC,long,false,None)),AttrA,None,0) new schema = Schema(ArraySeq(Field(AttrA,string,false,None), Field(AttrB,string,true,None), Field(AttrC,double,false,None)),AttrA,None,0).   Inferring the Table Schema You can use the custom NoSQL DataFrame inferSchema read option to automatically infer the schema of the read table from its contents.\nInfer-Schema Examples The following example uses a Spark DataFrame to read data from a NoSQL \u0026quot;employees\u0026quot; table in a \u0026quot;department_store\u0026quot; data container.\n import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() val myDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;v3io://department_store/employees\u0026#34;)  from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() myDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) \\ .load(\u0026#34;v3io://department_store/employees\u0026#34;)    To generate a schema file from the inferred schema, you can write back the content of the read DataFrame to the same table using the append save mode; the write operation automatically creates a schema file if it doesn't exist. For efficiency, use limit(0) to write only the schema file:  myDF.limit(0).write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;employee_id\u0026#34;) .save(\u0026#34;v3io://department_store/employees\u0026#34;)  myDF.limit(0).write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;employee_id\u0026#34;) \\ .save(\u0026#34;v3io://department_store/employees\u0026#34;)    Defining the Table Schema Programmatically You can define a NoSQL DataFrame's table schema programmatically by using the Spark DataFrame schema method as part of a read operation: in your code, you can define a schema variable of the relevant list type, and populate it with structures that provide the required information about the table's attributes (columns). Then, pass the variable as a parameter to the DataFrame schema read method — for example, for a schema variable, you can call schema(schema). See The Item-Attributes Schema Object ('fields') reference and the following examples. NoteProgrammatically created table schemas don't support range-scan or even-distribution table queries.  Programmatic Table-Schema Definition Examples The following example uses a Spark DataFrame to read data from a NoSQL \u0026quot;employees\u0026quot; table in a \u0026quot;department_store\u0026quot; data container. The table has five attributes (columns), which are depicted using the schema variable:\n \u0026quot;id\u0026quot; — a numeric employee ID, which serves as the table's primary key and isn't nullable. \u0026quot;firstname\u0026quot; — the employee's first name, depicted as a string. \u0026quot;lastname\u0026quot; — the employee's last name, depicted as a string. \u0026quot;department\u0026quot; — the department to which the employee belongs, depicted as a string . \u0026quot;managerid\u0026quot; — the numeric ID of the employee's manager.   import org.apache.spark.sql.SparkSession import org.apache.spark.sql.types._ val spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() val schema = StructType(List( StructField(\u0026#34;id\u0026#34;, LongType, nullable = false), StructField(\u0026#34;firstname\u0026#34;, StringType, nullable = true), StructField(\u0026#34;lastname\u0026#34;, StringType, nullable = true), StructField(\u0026#34;department\u0026#34;, StringType, nullable = true), StructField(\u0026#34;managerid\u0026#34;, LongType, nullable = true))) val myDF = spark.read.schema(schema) .format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://department_store/employees\u0026#34;)  import sys from pyspark.sql import SparkSession from pyspark.sql import * from pyspark.sql.types import * from pyspark.sql.functions import * spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() schema = StructType([ StructField(\u0026#34;id\u0026#34;, LongType(), False), StructField(\u0026#34;firstname\u0026#34;, StringType(), True), StructField(\u0026#34;lastname\u0026#34;, StringType(), True), StructField(\u0026#34;department\u0026#34;, StringType(), True), StructField(\u0026#34;managerid\u0026#34;, LongType(), True)]) myDF = spark.read.schema(schema) \\ .format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://department_store/employees\u0026#34;)    For additional examples, see the read examples in the Getting Started with Data Ingestion Using Spark tutorial.\nConditional Updates You can use the custom condition write option of the NoSQL Spark DataFrame to perform conditional item updates, whereby the write operation will be executed only if the specified condition expression evaluates to true.\nThe condition expression is evaluated against the table item to be updated, if it exists. If the condition evaluates to true, the write operation is executed and the item is updated or created; otherwise, the operation completes successfully without an update. Note If the expression references a non-existent item attribute, the condition processing stops and the operation completes successfully without updating or creating the item. If the item doesn't exist and the condition expression doesn't reference any attributes (for example, a \u0026quot;1==1\u0026quot; or \u0026quot;2==3\u0026quot; expression, which could potentially be auto generated in some programming scenarios), the operation completes successfully and the item is updated or created only if the condition evaluates to true.    See the NoSQL DataFrame conditional-update write example.\nCounter Attributes The Iguazio Spark connector enhances the standard Spark DataFrame by allowing you to define numeric attributes (columns) in a table as counter attributes and easily increment or decrement the attributes' values. This is done by using the custom counter write option of the NoSQL DataFrame to name one or more attributes as counter attributes.\nThe counter option is supported only with the NoSQL DataFrame append save mode. However, the significance of this mode when using the counter option is different than the standard Spark DataFrame behavior (for both counter and non-counter attributes):\n If a DataFrame attribute isn't already found in the table, the attribute is added to the table and initialized to the value set for it in the DataFrame. If the table or item don't exist, they're created and updated with the DataFrame's contents. This is the standard append save-mode behavior ant it's the same for both counter and non-counter attributes. If a DataFrame counter attribute is already found in the table, its value is incremented or decremented according to the value that was set for this attribute in the DataFrame — i.e., the attribute value indicates the increment or decrement step (a negative value = decrement). If a DataFrame non-counter attribute is already found in the table, its value is overwritten with the value that was set for it in the DataFrame but other attributes in the table remain unaffected (i.e., replace mode, similar to an append write with the columnUpdate option.)  See the NoSQL DataFrame counter-attributes write example.\nPartitioned Tables [Tech Preview] Table partitioning is a common technique for optimizing physical data layout and related queries. In a partitioned table, some item attributes (columns) are used to create partition directories within the root table directory using the format \u0026lt;table path\u0026gt;/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;[/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;/...], and each item is then saved to the respective partition directory based on its attribute values. For example, for a \u0026quot;mytable\u0026quot; table with year and month attribute partitions, an item with attributes year = 2018 and month = 1 will be saved to a mytable/year=2018/month=1/ directory. This allows for more efficient data queries that search for the data only in the relevant partition directories instead of scanning the entire table. This technique is used, for example, by Hive, and is supported for all the built-in Spark Dataset file-based data sources (such as Parquet, CSV, and JSON) via the partitionBy write method. See also the Partitioned Tables documentation on the Working with NoSQL Data page, including best practices.\nThe Iguazio Spark connector supports table partitioning for the NoSQL DataFrame [Tech Preview]:\n Creating a partitioned table — the custom NoSQL DataFrame partition option allows you to select specific item attributes (columns) in a write DataFrame to be used as partitions. When using this option, the platform creates the necessary partition directory path for each written item. (Note that after you define partitions for a table, you need to specify the same partitions whenever your write to this table unless you decide to overwrite it.) Querying a partitioned table — a partitioned table is queried like any other table, with the table path set to the root table directory and not to a specific partition directory. Version 3.2.1 of the platform doesn't support using using wild cards in the table path, such as \u0026quot;mytable/year=*/month=5\u0026quot; to search the month=5 directories in all mytable/year=\u0026quot;value\u0026quot; directories. However, you can easily restrict the query to specific partition directories by using the Spark DataFrame filter method with a filter that references one of the partition attributes. In such cases, the platform searches the root table directory that is specified in the read command for nested directories of the format \u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;. If it finds such directories, it searches only the partition directories that match the query. For example, for a table partitioned by year and month attributes, a month == 12 filter will return only the items from the month=12 partition directories in all year=* directories.   Table-Partitioning Examples  import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;Table-Partitioning Example\u0026#34;).getOrCreate() val table_path = \u0026#34;v3io://mycontainer/weather/\u0026#34; val df = Seq( (2016, 3, 25, 6, 16, 0.00, 55), (2016, 3, 25, 17, 19, 0.10, 62), (2016, 7, 24, 7, 20, 0.00, 52), (2016, 12, 24, 9, 10, 0.05, 47), (2016, 12, 24, 19, 8, 0.20, 47), (2017, 5, 7, 14, 21, 0.00, 70), (2017, 11, 1, 10, 16, 0.00, 34), (2017, 11, 1, 22, 13, 0.01, 41), (2017, 12, 12, 16, 12, 0.00, 47), (2017, 12, 24, 17, 11, 1.00, 50), (2018, 1, 18, 5, 8, 0.00, 37), (2018, 1, 18, 17, 10, 2.00, 45), (2018, 5, 20, 15, 24, 0.00, 62), (2018, 5, 20, 21, 20, 0.00, 59), (2018, 11, 1, 11, 11, 0.12, 65) ).toDF(\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;day\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;degrees_cel\u0026#34;, \u0026#34;rain_ml\u0026#34;, \u0026#34;humidity_per\u0026#34;) val df_with_key = df.withColumn(\u0026#34;time\u0026#34;, concat($\u0026#34;year\u0026#34;, $\u0026#34;month\u0026#34;, $\u0026#34;day\u0026#34;, $\u0026#34;hour\u0026#34;)) df_with_key.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;time\u0026#34;) .option(\u0026#34;partition\u0026#34;, \u0026#34;year, month, day\u0026#34;) .save(table_path) var readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;month\u0026#34; \u0026lt; 7) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;month\u0026#34; === 12 \u0026amp;\u0026amp; $\u0026#34;day\u0026#34; === 24) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;month\u0026#34; \u0026gt; 6 \u0026amp;\u0026amp; $\u0026#34;hour\u0026#34; \u0026gt;= 8 \u0026amp;\u0026amp; $\u0026#34;hour\u0026#34; \u0026lt;= 20) readDF.show()  import sys from pyspark.sql import SparkSession from pyspark.sql import * from pyspark.sql.functions import * spark = SparkSession.builder.appName(\u0026#34;Table-Partitioning Example\u0026#34;).getOrCreate() table_path = \u0026#34;v3io://mycontainer/weather/\u0026#34; df = spark.createDataFrame([ (2016, 3, 25, 6, 16, 0.00, 55), (2016, 3, 25, 17, 19, 0.10, 62), (2016, 7, 24, 7, 20, 0.00, 52), (2016, 12, 24, 9, 10, 0.05, 47), (2016, 12, 24, 19, 8, 0.20, 47), (2017, 5, 7, 14, 21, 0.00, 70), (2017, 11, 1, 10, 16, 0.00, 34), (2017, 11, 1, 22, 13, 0.01, 41), (2017, 12, 12, 16, 12, 0.00, 47), (2017, 12, 24, 17, 11, 1.00, 50), (2018, 1, 18, 5, 8, 0.00, 37), (2018, 1, 18, 17, 10, 2.00, 45), (2018, 5, 20, 15, 24, 0.00, 62), (2018, 5, 20, 21, 20, 0.00, 59), (2018, 11, 1, 11, 11, 0.12, 65) ], [\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;day\u0026#34;, \u0026#34;hour\u0026#34;, \u0026#34;degrees_cel\u0026#34;, \u0026#34;rain_ml\u0026#34;, \u0026#34;humidity_per\u0026#34;]) df_with_key = df.withColumn( \u0026#34;time\u0026#34;, concat(df[\u0026#34;year\u0026#34;], df[\u0026#34;month\u0026#34;], df[\u0026#34;day\u0026#34;], df[\u0026#34;hour\u0026#34;])) df_with_key.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;time\u0026#34;) \\ .option(\u0026#34;partition\u0026#34;, \u0026#34;year, month, day, hour\u0026#34;) \\ .save(table_path) readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;month \u0026lt; 7\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;month == 12 AND day == 24\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;month \u0026gt; 6 AND hour \u0026gt;= 8 AND hour \u0026lt;= 20\u0026#34;) readDF.show()    This examples creates a partitioned \u0026quot;weather\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container. The option(\u0026quot;partition\u0026quot;, \u0026quot;year, month, day\u0026quot;) write option partitions the table by the year, month, and day item attributes. As demonstrated in the following image, if you browse the container in the dashboard after running the example, you'll see that the weather directory has year=\u0026lt;value\u0026gt;/month=\u0026lt;value\u0026gt;/day=\u0026lt;value\u0026gt; partition directories that match the written items. If you select any of the nested day partition directories, you can see the written items and their attributes. For example, the first item (with attribute values 2016, 3, 25, 6, 16, 0.00, 55) is saved to a 20163256 file in a weather/year=2016/month=3/day=25 partition directory.    Following is the output of the example's show commands for each read. The filtered results are gathered by scanning only the partition directories that match the filter criteria.\nFull table read\n+----+-----+---+----+-----------+-------+------------+----------+ |year|month|day|hour|degrees_cel|rain_ml|humidity_per| time| +----+-----+---+----+-----------+-------+------------+----------+ |2016| 12| 24| 9| 10| 0.05| 47| 201612249| |2016| 12| 24| 19| 8| 0.2| 47|2016122419| |2016| 3| 25| 6| 16| 0.0| 55| 20163256| |2016| 3| 25| 17| 19| 0.1| 62| 201632517| |2016| 7| 24| 7| 20| 0.0| 52| 20167247| |2017| 11| 1| 22| 13| 0.01| 41| 201711122| |2017| 11| 1| 10| 16| 0.0| 34| 201711110| |2017| 12| 12| 16| 12| 0.0| 47|2017121216| |2017| 12| 24| 17| 11| 1.0| 50|2017122417| |2017| 5| 7| 14| 21| 0.0| 70| 20175714| |2018| 1| 18| 5| 8| 0.0| 37| 20181185| |2018| 1| 18| 17| 10| 2.0| 45| 201811817| |2018| 11| 1| 11| 11| 0.12| 65| 201811111| |2018| 5| 20| 15| 24| 0.0| 62| 201852015| |2018| 5| 20| 21| 20| 0.0| 59| 201852021| +----+-----+---+----+-----------+-------+------------+----------+ month \u0026lt; 7 filter — retrieve all data for the first six months of each year:\n+----+-----+---+----+-----------+-------+------------+---------+ |year|month|day|hour|degrees_cel|rain_ml|humidity_per| time| +----+-----+---+----+-----------+-------+------------+---------+ |2016| 3| 25| 6| 16| 0.0| 55| 20163256| |2016| 3| 25| 17| 19| 0.1| 62|201632517| |2017| 5| 7| 14| 21| 0.0| 70| 20175714| |2018| 1| 18| 5| 8| 0.0| 37| 20181185| |2018| 1| 18| 17| 10| 2.0| 45|201811817| |2018| 5| 20| 15| 24| 0.0| 62|201852015| |2018| 5| 20| 21| 20| 0.0| 59|201852021| +----+-----+---+----+-----------+-------+------------+---------+ month == 12 AND day == 24 filter — retrieve all hours on Dec 24 each year:\n+----+-----+---+----+-----------+-------+------------+----------+ |year|month|day|hour|degrees_cel|rain_ml|humidity_per| time| +----+-----+---+----+-----------+-------+------------+----------+ |2016| 12| 24| 9| 10| 0.05| 47| 201612249| |2016| 12| 24| 19| 8| 0.2| 47|2016122419| |2017| 12| 24| 17| 11| 1.0| 50|2017122417| +----+-----+---+----+-----------+-------+------------+----------+ month \u0026gt; 6 AND hour \u0026gt;= 8 AND hour \u0026lt;= 20 filter — retrieve 08:00–20:00 data for every day in the last six months of each year:\n+----+-----+---+----+-----------+-------+------------+----------+ |year|month|day|hour|degrees_cel|rain_ml|humidity_per| time| +----+-----+---+----+-----------+-------+------------+----------+ |2016| 12| 24| 9| 10| 0.05| 47| 201612249| |2016| 12| 24| 19| 8| 0.2| 47|2016122419| |2017| 11| 1| 10| 16| 0.0| 34| 201711110| |2017| 12| 12| 16| 12| 0.0| 47|2017121216| |2017| 12| 24| 17| 11| 1.0| 50|2017122417| |2018| 11| 1| 11| 11| 0.12| 65| 201811111| +----+-----+---+----+-----------+-------+------------+----------+ Range Scans A NoSQL Spark DataFrame table query that uses supported sharding-key and optional sorting-key filters to retrieve items with the same sharding-key value, is processed by performing a range scan, which is more efficient than the standard full table scan. See NoSQL Range Scans. Note that the support for Spark DataFrame range scans requires a table schema that was inferred with a NoSQL Spark DataFrame, Frames, or the Iguazio Presto connector. Range-Scan OperatorsThe NoSQL Spark DataFrame uses range scan for compound primary-key table queries that apply the equal-to (=) or IN (IN/isin) operator to the sharding-key attribute, and optionally also apply a comparison operator (=/\u0026gt;/\u0026gt;=/\u0026lt;/\u0026lt;=) to the sorting-key attribute.  Range-Scan Examples Example 1 — Basic Range Scan  import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;Range-Scan Example\u0026#34;).getOrCreate() val table_path = \u0026#34;v3io://mycontainer/mytaxis/rides/\u0026#34; var writeDF = Seq( (24, \u0026#34;20180601\u0026#34;, 8, 332.0, 18), (24, \u0026#34;20180602\u0026#34;, 5, 260.0, 11), (24, \u0026#34;20180701\u0026#34;, 7, 352.1, 21), (1, \u0026#34;20180601\u0026#34;, 25, 125.0, 40), (1, \u0026#34;20180602\u0026#34;, 20, 106.0, 46), (1, \u0026#34;20180701\u0026#34;, 28, 106.4, 42), (16, \u0026#34;20180601\u0026#34;, 1, 224.2, 8), (16, \u0026#34;20180602\u0026#34;, 10, 244.0, 45), (16, \u0026#34;20180701\u0026#34;, 6, 193.2, 24) ).toDF(\u0026#34;driver_id\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;num_rides\u0026#34;, \u0026#34;total_km\u0026#34;, \u0026#34;total_passengers\u0026#34;) writeDF = writeDF .withColumn(\u0026#34;avg_ride_km\u0026#34;, $\u0026#34;total_km\u0026#34; / $\u0026#34;num_rides\u0026#34;) .withColumn(\u0026#34;avg_ride_passengers\u0026#34;, $\u0026#34;total_passengers\u0026#34; / $\u0026#34;num_rides\u0026#34;) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;driver_id\u0026#34;) .option(\u0026#34;sorting-key\u0026#34;, \u0026#34;date\u0026#34;) .save(table_path) // Range-scan queries var readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;driver_id\u0026#34; === 1) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;driver_id\u0026#34; === 24 \u0026amp;\u0026amp; $\u0026#34;date\u0026#34; \u0026gt;= \u0026#34;20180101\u0026#34; \u0026amp;\u0026amp; $\u0026#34;date\u0026#34; \u0026lt; \u0026#34;20180701\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;driver_id\u0026#34;.isin(1, 16, 24) \u0026amp;\u0026amp; $\u0026#34;avg_ride_passengers\u0026#34; \u0026gt;= 3) readDF.show() }  from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;Range-Scan Example\u0026#34;).getOrCreate() table_path = \u0026#34;v3io://mycontainer/mytaxis/rides/\u0026#34; writeDF = spark.createDataFrame([ (24, \u0026#34;20180601\u0026#34;, 8, 332.0, 18), (24, \u0026#34;20180602\u0026#34;, 5, 260.0, 11), (24, \u0026#34;20180701\u0026#34;, 7, 352.1, 21), (1, \u0026#34;20180601\u0026#34;, 25, 125.0, 40), (1, \u0026#34;20180602\u0026#34;, 20, 106.0, 46), (1, \u0026#34;20180701\u0026#34;, 28, 106.4, 42), (16, \u0026#34;20180601\u0026#34;, 1, 224.2, 8), (16, \u0026#34;20180602\u0026#34;, 10, 244.0, 45), (16, \u0026#34;20180701\u0026#34;, 6, 193.2, 24) ], [\u0026#34;driver_id\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;num_rides\u0026#34;, \u0026#34;total_km\u0026#34;, \u0026#34;total_passengers\u0026#34;]) writeDF = writeDF.withColumn( \u0026#34;avg_ride_km\u0026#34;, writeDF[\u0026#34;total_km\u0026#34;] / writeDF[\u0026#34;num_rides\u0026#34;]) \\ .withColumn( \u0026#34;avg_ride_passengers\u0026#34;, writeDF[\u0026#34;total_passengers\u0026#34;] / writeDF[\u0026#34;num_rides\u0026#34;]) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;driver_id\u0026#34;) \\ .option(\u0026#34;sorting-key\u0026#34;, \u0026#34;date\u0026#34;) \\ .save(table_path) # Range-scan queries readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;driver_id == 1\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;driver_id == 24 AND date \u0026gt;= \u0026#39;20180101\u0026#39; AND date \u0026lt; \u0026#39;20180701\u0026#39;\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;driver_id IN (1, 16, 24) AND avg_ride_passengers \u0026gt;= 3\u0026#34;) readDF.show()    This example creates a \u0026quot;rides\u0026quot; table in a mytaxis directory in a \u0026quot;mycontainer\u0026quot; data container. The option(\u0026quot;key\u0026quot;, \u0026quot;driver_id\u0026quot;) and option(\u0026quot;sorting-key\u0026quot;, \u0026quot;date\u0026quot;) write options define the driver_id attribute as the table's sharding key and the date attribute as the table's sorting key; all items in the DataFrame define these attributes. If you browse the container in the dashboard after running the example, you'll see that the names of the files in the mytaxis/rides directory are of the format \u0026lt;sharding-key value\u0026gt;.\u0026lt;sorting-key value\u0026gt;, as demonstrated in the following image (for example, 16.20180602):    All of the read commands will result in faster range scans, compared to standard full-table scans, because they include range-scan sharding-key and optionally also sorting-key filters. Following is the output of the example's show commands for each read:\n\u0026quot;driver_id == 1\u0026quot; filter — retrieve all items with a driver_id sharding-key attribute value of 1 (regardless of the sorting-key value):\n+---------+--------+---------+--------+----------------+------------------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers| avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+------------------+-------------------+ | 1|20180601| 25| 125.0| 40| 5.0| 1.6| | 1|20180602| 20| 106.0| 46| 5.3| 2.3| | 1|20180701| 28| 106.4| 42|3.8000000000000003| 1.5| +---------+--------+---------+--------+----------------+------------------+-------------------+ \u0026quot;driver_id == 24 AND date \u0026gt;= '20180101' AND date \u0026lt; '20180701'\u0026quot; filter — retrieve all items with a driver_id sharding-key attribute value of 24 and a date sorting-key attribute value within the first six months of 2018:\n+---------+--------+---------+--------+----------------+-----------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers|avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+-----------+-------------------+ | 24|20180601| 8| 332.0| 18| 41.5| 2.25| | 24|20180602| 5| 260.0| 11| 52.0| 2.2| +---------+--------+---------+--------+----------------+-----------+-------------------+ \u0026quot;driver_id IN (1, 16, 24) AND avg_ride_passengers \u0026gt;= 3\u0026quot; filter — retrieve all items with a driver_id sharding-key attribute value of 1, 16, or 24 (regardless of the sorting-key value) and an avg_ride_passengers attribute value that is greater or equal to 3:\n+---------+--------+---------+--------+----------------+------------------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers| avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+------------------+-------------------+ | 16|20180601| 1| 224.2| 8| 224.2| 8.0| | 16|20180602| 10| 244.0| 45| 24.4| 4.5| | 16|20180701| 6| 193.2| 24|32.199999999999996| 4.0| | 24|20180701| 7| 352.1| 21|50.300000000000004| 3.0| +---------+--------+---------+--------+----------------+------------------+-------------------+ Example 2 — Even-Distribution Range Scan  import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;Even-Distribution Range-Scan Example\u0026#34;).getOrCreate() val table_path = \u0026#34;v3io://mycontainer/mytaxis/even_distribution_range_scan/rides/\u0026#34; var writeDF = Seq( (24, \u0026#34;20180601\u0026#34;, 8, 332.0, 18), (24, \u0026#34;20180602\u0026#34;, 5, 260.0, 11), (24, \u0026#34;20180701\u0026#34;, 7, 352.1, 21), (1, \u0026#34;20180101\u0026#34;, 4, 90.0, 14), (1, \u0026#34;20180102\u0026#34;, 14, 141.4, 28), (1, \u0026#34;20180202\u0026#34;, 8, 220.8, 22), (1, \u0026#34;20180601\u0026#34;, 25, 125.0, 40), (1, \u0026#34;20180602\u0026#34;, 20, 106.0, 46), (1, \u0026#34;20180701\u0026#34;, 28, 106.4, 42), (16, \u0026#34;20180601\u0026#34;, 1, 224.2, 8), (16, \u0026#34;20180602\u0026#34;, 10, 244.0, 45), (16, \u0026#34;20180701\u0026#34;, 6, 193.2, 24) ).toDF(\u0026#34;driver_id\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;num_rides\u0026#34;, \u0026#34;total_km\u0026#34;, \u0026#34;total_passengers\u0026#34;) .withColumn(\u0026#34;avg_ride_km\u0026#34;, $\u0026#34;total_km\u0026#34; / $\u0026#34;num_rides\u0026#34;) .withColumn(\u0026#34;avg_ride_passengers\u0026#34;, $\u0026#34;total_passengers\u0026#34; / $\u0026#34;num_rides\u0026#34;) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;driver_id\u0026#34;) .option(\u0026#34;sorting-key\u0026#34;, \u0026#34;date\u0026#34;) .option(\u0026#34;range-scan.even-distribution\u0026#34;, \u0026#34;true\u0026#34;) .save(table_path) // Range-scan queries var readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;driver_id\u0026#34; === 1) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;driver_id\u0026#34; === 24 \u0026amp;\u0026amp; $\u0026#34;date\u0026#34; \u0026gt;= \u0026#34;20180101\u0026#34; \u0026amp;\u0026amp; $\u0026#34;date\u0026#34; \u0026lt; \u0026#34;20180701\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) .filter($\u0026#34;driver_id\u0026#34;.isin(1, 16, 24) \u0026amp;\u0026amp; $\u0026#34;avg_ride_passengers\u0026#34; \u0026gt;= 3) readDF.show()  from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\u0026#34;Even-Distribution Range-Scan Example\u0026#34;).getOrCreate() table_path = \u0026#34;v3io://mycontainer/mytaxis/even_distribution_range_scan/rides/\u0026#34; writeDF = spark.createDataFrame([ (24, \u0026#34;20180601\u0026#34;, 8, 332.0, 18), (24, \u0026#34;20180602\u0026#34;, 5, 260.0, 11), (24, \u0026#34;20180701\u0026#34;, 7, 352.1, 21), (1, \u0026#34;20180101\u0026#34;, 4, 90.0, 14), (1, \u0026#34;20180102\u0026#34;, 14, 141.4, 28), (1, \u0026#34;20180202\u0026#34;, 8, 220.8, 22), (1, \u0026#34;20180601\u0026#34;, 25, 125.0, 40), (1, \u0026#34;20180602\u0026#34;, 20, 106.0, 46), (1, \u0026#34;20180701\u0026#34;, 28, 106.4, 42), (16, \u0026#34;20180601\u0026#34;, 1, 224.2, 8), (16, \u0026#34;20180602\u0026#34;, 10, 244.0, 45), (16, \u0026#34;20180701\u0026#34;, 6, 193.2, 24) ], [\u0026#34;driver_id\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;num_rides\u0026#34;, \u0026#34;total_km\u0026#34;, \u0026#34;total_passengers\u0026#34;]) writeDF = writeDF.withColumn( \u0026#34;avg_ride_km\u0026#34;, writeDF[\u0026#34;total_km\u0026#34;] / writeDF[\u0026#34;num_rides\u0026#34;]) \\ .withColumn( \u0026#34;avg_ride_passengers\u0026#34;, writeDF[\u0026#34;total_passengers\u0026#34;] / writeDF[\u0026#34;num_rides\u0026#34;]) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;driver_id\u0026#34;) \\ .option(\u0026#34;sorting-key\u0026#34;, \u0026#34;date\u0026#34;) \\ .option(\u0026#34;range-scan.even-distribution\u0026#34;, \u0026#34;true\u0026#34;) \\ .save(table_path) # Range-scan queries readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;driver_id == 1\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;driver_id == 24 AND date \u0026gt;= \u0026#39;20180101\u0026#39; AND date \u0026lt; \u0026#39;20180701\u0026#39;\u0026#34;) readDF.show() readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;).load(table_path) \\ .filter(\u0026#34;driver_id IN (1, 16, 24) AND avg_ride_passengers \u0026gt;= 3\u0026#34;) readDF.show()    This example creates a \u0026quot;rides\u0026quot; table in a mytaxis/even_distribution_range_scan directory in a \u0026quot;mycontainer\u0026quot; data container. The content of the table is similar to that of the \u0026quot;rides\u0026quot; table in the basic range-scan example but with additional items with a sharding-key value of 1. For demonstration purposes, the assumption is that the data in the table is expected to become less uniform as more items are added, with most items having the same sharding-key value (for example, 1). Therefore, the range-scan-even-distribution write option is added to the writeDF write command — option(\u0026quot;range-scan.even-distribution\u0026quot;, \u0026quot;true\u0026quot;) — to recalculate the items' sharding-key values and distribute the items more evenly across multiple data slices. See Even Workload Distribution.\nThe read queries remain the same as in the basic range-scan example. However, if you browse the container in the dashboard after running the example, you'll see that the names of the files in the mytaxis/even_distribution_range_scan/rides directory are of the format \u0026lt;original sharding-key value\u0026gt;_\u0026lt;n\u0026gt;.\u0026lt;sorting-key value\u0026gt;, as demonstrated in the following image, and not \u0026lt;original sharding-key value\u0026gt;.\u0026lt;sorting-key value\u0026gt; as in the basic example (for example, 16_36.20180602 instead of 16.20180602):    Following is the output of the example's show commands for each read. If you compare the output to that of the basic range-scan example, you'll see that it's similar except that the even-distribution range-scan query results have some additional results for sharding-key 1 items that aren't found in the basic-example table and the sort order is different.\n\u0026quot;driver_id == 1\u0026quot; filter — retrieve all items with a driver_id sharding-key attribute value of 1 (regardless of the sorting-key value):\n+---------+--------+---------+--------+----------------+------------------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers| avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+------------------+-------------------+ | 1|20180202| 8| 220.8| 22| 27.6| 2.75| | 1|20180102| 14| 141.4| 28| 10.1| 2.0| | 1|20180101| 4| 90.0| 14| 22.5| 3.5| | 1|20180602| 20| 106.0| 46| 5.3| 2.3| | 1|20180701| 28| 106.4| 42|3.8000000000000003| 1.5| | 1|20180601| 25| 125.0| 40| 5.0| 1.6| +---------+--------+---------+--------+----------------+------------------+-------------------+ \u0026quot;driver_id == 24 AND date \u0026gt;= '20180101' AND date \u0026lt; '20180701'\u0026quot; filter — retrieve all items with a driver_id sharding-key attribute value of 24 and a date sorting-key attribute value within the first six months of 2018:\n+---------+--------+---------+--------+----------------+-----------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers|avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+-----------+-------------------+ | 24|20180601| 8| 332.0| 18| 41.5| 2.25| | 24|20180602| 5| 260.0| 11| 52.0| 2.2| +---------+--------+---------+--------+----------------+-----------+-------------------+ \u0026quot;driver_id IN (1, 16, 24) AND avg_ride_passengers \u0026gt;= 3\u0026quot; filter — retrieve all items with a driver_id sharding-key attribute value of 1, 16, or 24 (regardless of the sorting-key value) and an avg_ride_passengers attribute value that is greater or equal to 3:\n+---------+--------+---------+--------+----------------+------------------+-------------------+ |driver_id| date|num_rides|total_km|total_passengers| avg_ride_km|avg_ride_passengers| +---------+--------+---------+--------+----------------+------------------+-------------------+ | 1|20180101| 4| 90.0| 14| 22.5| 3.5| | 16|20180602| 10| 244.0| 45| 24.4| 4.5| | 16|20180701| 6| 193.2| 24|32.199999999999996| 4.0| | 16|20180601| 1| 224.2| 8| 224.2| 8.0| | 24|20180701| 7| 352.1| 21|50.300000000000004| 3.0| +---------+--------+---------+--------+----------------+------------------+-------------------+ Even Workload Distribution The NoSQL Spark DataFrame offers custom support for even distribution of ingested items across the available data slices for the parent container. The objective is to improve the system's performance when working with a non-uniform data set — see the Recalculating Sharding-Key Values for Even Workload Distribution best-practice guidelines.\nWhen writing (ingesting) data to a table with a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key (see the key and sorting-key write options), you can optionally also set the custom range-scan-even-distribution option. This option instructs the platform to recalculate the primary-key value for each of the ingested items, by splitting the item's original sharding-key value into multiple values, according to the number configured in the platform's v3io.kv.range-scan.hashing-bucket-num configuration property (default = 64). As a result, items with the same original sharding-key value (which remains stored in the items' sharding-key attribute) are distributed across multiple data slices, based on the value of the items' sorting key, instead of being stored on the same slice. This is done implicitly, although if you browse the table directory you can see the new primary-key values (item names) — of the format \u0026lt;original sharding-key value\u0026gt;_\u0026lt;n\u0026gt;.\u0026lt;sorting-key value\u0026gt; (for example, johnd_1.20180602) — as demonstrated in the even-distribution range-scan example.\nWhen submitting a NoSQL Spark DataFrame or Presto sharding-key query for a table that was created with the even-distribution Spark DataFrame option or by using similar calculations, use the original sharding-key value. Behind the scenes, the platform searches for all the primary-key values that were derived from the original sharding-key value. Note that this custom support requires a table schema that was inferred with a NoSQL Spark DataFrame or with the Iguazio Presto connector. For more information on the behind-the-scenes implementation to support this feature, see the Using a NoSQL Spark DataFrame for Even Workload Distribution best-practices documentation.\nExamples Following are some examples of using the NoSQL Spark DataFrame. For schema-definition, table-partitioning, and range-scan examples, see the Defining the Table Schema, Partitioned Tables, and Range Scans sections, respectively; the range-scan examples also demonstrate the support for even workload distribution. For additional examples, see Getting Started with Data Ingestion Using Spark.\nRead Examples Note For a NoSQL DataFrame read example with an explicit table-schema definition, see the examples in the Defining the Table Schema Programmatically section. The conditional-update write example also demonstrates reading from a NoSQL table to validate the execution of the write commands.    Example 1 — Basic Read and Related Queries  import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() val df = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;v3io://mycontainer/WebData\u0026#34;) df.select($\u0026#34;url\u0026#34;, $\u0026#34;pages\u0026#34; + $\u0026#34;ads\u0026#34; as \u0026#34;views\u0026#34;) .where($\u0026#34;browser\u0026#34; != lit(\u0026#34;Chrome\u0026#34;))  from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() val df = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) \\ .load(\u0026#34;v3io://mycontainer/WebData\u0026#34;) df.select(df[\u0026#34;url\u0026#34;], df[\u0026#34;pages\u0026#34;] + df[\u0026#34;ads\u0026#34;] as \u0026#34;views\u0026#34;) \\ .where(df[\u0026#34;browser\u0026#34;] != lit(\u0026#34;Chrome\u0026#34;))    This example reads page-click data from a \u0026quot;WebData\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container. The inferSchema option is used to infer the schema of the table (in case the table doesn't have a schema file). NoteBy using Spark's predicate pushdown, the select and where operations are handed over to the platform's NoSQL store, and pruned/filtered data is returned to Spark.  Write Examples Example 1 — Simple Append-Mode Write  val df = Seq((\u0026#34;ians\u0026#34;, \u0026#34;Ian Smith\u0026#34;, 25), (\u0026#34;dorisb\u0026#34;, \u0026#34;Doris Brown\u0026#34;, 31)) .toDF(\u0026#34;username\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;username\u0026#34;) .save(\u0026#34;v3io://mycontainer/IT/Users/\u0026#34;)  df = spark.createDataFrame([ (\u0026#34;ians\u0026#34;, \u0026#34;Ian Smith\u0026#34;, 25), (\u0026#34;dorisb\u0026#34;, \u0026#34;Doris Brown\u0026#34;, 31) ], [\u0026#34;username\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;username\u0026#34;) \\ .save(\u0026#34;v3io://mycontainer/IT/Users/\u0026#34;)    This example writes two items (rows) to an \u0026quot;IT/Users\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container whose primary-key attribute is username. The save mode is set to append. Therefore, DataFrame items that don't already exist in the table will be added in full, and existing items (based on the specified primary-key attribute values) will be updated only to add missing attributes, but values of existing item attributes won't be modified.\nExample 2 — Replace-Mode Write  val df = Seq((\u0026#34;ians\u0026#34;, \u0026#34;Ian Smith\u0026#34;, 26), (\u0026#34;janed\u0026#34;, \u0026#34;Jane Doe\u0026#34;, 42)) .toDF(\u0026#34;username\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;columnUpdate\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;username\u0026#34;) .save(\u0026#34;v3io://mycontainer/IT/Users/\u0026#34;)  df = spark.createDataFrame([ (\u0026#34;ians\u0026#34;, \u0026#34;Ian Smith\u0026#34;, 26), (\u0026#34;janed\u0026#34;, \u0026#34;Jane Doe\u0026#34;, 42) ], [\u0026#34;username\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;]) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .option(\u0026#34;columnUpdate\u0026#34;, \u0026#34;true\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;username\u0026#34;) \\ .save(\u0026#34;v3io://mycontainer/IT/Users/\u0026#34;)    This example writes two items (rows) to the same table as in the previous simple append-mode write example — \u0026quot;IT/Users\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container whose primary-key attribute is username. The save mode is set to append and the columnUpdate option is set to \u0026quot;true\u0026quot;. Therefore, assuming the code is run after the simple append-mode write example, the new \u0026quot;janed\u0026quot; item (which doesn't exist in the table) will be appended to the table; the existing \u0026quot;ians\u0026quot; item, which was included in the previous write example, will be overwritten with the item from the new write DataFrame (and the value of the age attribute will change from 25 to 26); and the existing \u0026quot;dorisb\u0026quot; item, which was written only in the previous example, will remain unchanged.\nExample 3 — Counter Attributes  val df = Seq((532, 5, \u0026#34;IP-953481-35\u0026#34;, \u0026#34;Jenny Penny\u0026#34;, 7866689)) .toDF(\u0026#34;kms\u0026#34;, \u0026#34;num_drives\u0026#34;, \u0026#34;policy\u0026#34;, \u0026#34;primary_driver\u0026#34;, \u0026#34;reg_num\u0026#34;) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .mode(\u0026#34;append\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;reg_num\u0026#34;) .option(\u0026#34;counter\u0026#34;, \u0026#34;kms, num_drives\u0026#34;) .save(\u0026#34;v3io://mycontainer/Cars/\u0026#34;)  df = spark.createDataFrame([ (532, 5, \u0026#34;IP-953481-35\u0026#34;, \u0026#34;Jenny Penny\u0026#34;, 7866689) ], [\u0026#34;kms\u0026#34;, \u0026#34;num_drives\u0026#34;, \u0026#34;policy\u0026#34;, \u0026#34;primary_driver\u0026#34;, \u0026#34;reg_num\u0026#34;]) df.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;reg_num\u0026#34;) \\ .option(\u0026#34;counter\u0026#34;, \u0026#34;kms, num_drives\u0026#34;) \\ .save(\u0026#34;v3io://mycontainer/Cars/\u0026#34;)    This example writes an item (row) to a \u0026quot;Cars\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container whose primary-key attribute is reg_num. The save mode is set to append, which is the required mode when defining Counter Attributes. The example demonstrates the special significance of this mode when used together with the counter option.\nThe DataFrame contains a primary-key reg_num attribute (7866689); num_drives (5) and kms (532) attributes, which are defined as counter attributes using the counter option; and regular (non-counter) policy (\u0026quot;IP-953481\u0026quot;) and primary_driver (\u0026quot;Jenny Penny\u0026quot;) attributes.  Assume a matching item (reg_num=7866689) already exists in the table and that its has a num_drives attribute with the value 95 and a primary_driver attribute with the value \u0026quot;Harry Carey\u0026quot;, but no kms or policy attributes.\nBecause the table item already has the num_drives counter attribute, its current value (95) will be incremented by the specified attribute value (5), updating the attribute's value to 100. Because the kms counter attribute is new, it will be added to the item and initialized to its DataFrame value — 532.  Both non-counter attributes in the DataFrame will be added to the table item with the respective DataFrame values, overwriting any existing values: the value of the primary_driver attribute will change from \u0026quot;Harry Carey\u0026quot; to \u0026quot;Jenny Penny\u0026quot;, and a policy attribute with the value \u0026quot;IP-953481\u0026quot; will be added to the item. (This behavior is different when using the append or overwrite save modes without the counter option for the same non-counter attributes.)\nExample 4 — Conditional Update  import org.apache.spark.sql.SparkSession val spark = SparkSession.builder.appName(\u0026#34;My Spark Application\u0026#34;).getOrCreate() var writeDF = Seq((\u0026#34;7843321\u0026#34;, \u0026#34;Honda\u0026#34;, 29321)) .toDF(\u0026#34;reg_license\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;odometer\u0026#34;) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;v3io://mycontainer/cars/\u0026#34;) var readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF.show() writeDF = Seq((\u0026#34;7843321\u0026#34;, \u0026#34;Honda\u0026#34;, 31718)) .toDF(\u0026#34;reg_license\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;odometer\u0026#34;) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) .option(\u0026#34;condition\u0026#34;, \u0026#34;${odometer} \u0026gt; odometer\u0026#34;) .mode(\u0026#34;append\u0026#34;).save(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF.show() writeDF = Seq((\u0026#34;7843321\u0026#34;, \u0026#34;Ford\u0026#34;, 40001)) .toDF(\u0026#34;reg_license\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;odometer\u0026#34;) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) .option(\u0026#34;condition\u0026#34;, \u0026#34;${model} == model\u0026#34;) .mode(\u0026#34;append\u0026#34;).save(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) .load(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF.show()  from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() writeDF = spark.createDataFrame([(\u0026#34;7843321\u0026#34;, \u0026#34;Honda\u0026#34;, 29321)], [\u0026#34;reg_license\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;odometer\u0026#34;]) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF.show() writeDF = spark.createDataFrame([(\u0026#34;7843321\u0026#34;, \u0026#34;Honda\u0026#34;, 31718)], [\u0026#34;reg_license\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;odometer\u0026#34;]) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) \\ .option(\u0026#34;condition\u0026#34;, \u0026#34;${odometer} \u0026gt; odometer\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;).save(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF.show() writeDF = spark.createDataFrame([(\u0026#34;7843321\u0026#34;, \u0026#34;Ford\u0026#34;, 40001)], [\u0026#34;reg_license\u0026#34;, \u0026#34;model\u0026#34;, \u0026#34;odometer\u0026#34;]) writeDF.write.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .option(\u0026#34;key\u0026#34;, \u0026#34;reg_license\u0026#34;) \\ .option(\u0026#34;condition\u0026#34;, \u0026#34;${model} == model\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;).save(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF = spark.read.format(\u0026#34;io.iguaz.v3io.spark.sql.kv\u0026#34;) \\ .load(\u0026#34;v3io://mycontainer/cars/\u0026#34;) readDF.show()    This example demonstrates how to conditionally update NoSQL table items by using the condition write option. Each write call in the example is followed by matching read and show calls to read and display the value of the updated item in the target table after the write operation.\nThe first write command writes an item (row) to a \u0026quot;cars\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container. The item's reg_license primary-key attribute is set to 7843321, the mode attribute is set to \u0026quot;Honda\u0026quot;, and the odometer attribute is set to 29321. The overwrite save mode is used to overwrite the table if it already exists and create it otherwise. Reading the item from the table produces this output:\n+-----------+-----+--------+ |reg_license|model|odometer| +-----------+-----+--------+ | 7843321|Honda| 29321| +-----------+-----+--------+ The second write command uses the condition option to update the value of the item's odometer attribute only if this value is higher than the current value of this attribute in the table — option(\u0026quot;condition\u0026quot;, \u0026quot;${odometer} \u0026gt; odometer\u0026quot;). The append save mode is used to update the specified item rather than overwriting the table. Because the value of odometer in the write DataFrame (31718) is higher than the current value of this attribute in the table (29321), the condition evaluates to true and the write is executed, updating the value of the item's odometer attribute in the table, as shown when reading the item from the table:\n-----------+-----+--------+ |reg_license|model|odometer| +-----------+-----+--------+ | 7843321|Honda| 31718| +-----------+-----+--------+ The third write command uses the condition option (again with the append save mode) to update the value of the item's odometer attribute to 40001 only if it the value of the model attribute remains the same — option(\u0026quot;condition\u0026quot;, \u0026quot;${model} == model\u0026quot;). Because the value of model in the write DataFrame (\u0026quot;Ford\u0026quot;) is different than the current value of this attribute in the table (\u0026quot;Honda\u0026quot;), the condition evaluates to false and the write isn't executed (i.e., the table item isn't updated), as shown when reading the item from the table:\n-----------+-----+--------+ |reg_license|model|odometer| +-----------+-----+--------+ | 7843321|Honda| 31718| +-----------+-----+--------+ See Also  Working with NoSQL Data Ingesting and preparing data using Spark Getting Started with Data Ingestion Using Spark NoSQL Table Schema Reference Spark DataFrame Data Types Condition Expression  ","keywords":["nosql","dataframe,","spark,","spark","dataframes,","NoSQL","Spark","DataFrame,","Iguazio","Spark","connector,","predicate","pushdown,","nosql,","nosql","tables,","tables,","table","path,","v3io,","attributes,","columnUpdate,","counter","attributes,","counter,","counters,","condition,","conditional","updates,","save","modes,","append","mode,","overwrite","mode,","replace","mode,","columns,","partitioning,","partitions,","partition,","item","names,","object","names,","item","keys,","object","keys,","primary","key,","primary-key","attribute,","sharding","key,","sharding-key","attribute,","sorting","key,","sorting-key","attribute,","range","scan,","range-scan-even-distribution,","SQLContext,","sql","context,","table","schema,","schema,","infer","schema,","inferSchema,","allow-overwrite-schema,","sorting-key,","v3io","GitHub,","iguazio_api_examples"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-datasets/nosql-dataframe/","title":"The NoSQL Spark DataFrame"},{"content":" The dashboard is the platform's graphical user interface and is also your entry point to your platform cluster. Go to the dashboard URL from any web browser and log in with your platform login credentials. The dashboard allows managing and monitoring platform activity. You can navigate the dashboard pages from the side navigation menu. NoteSome areas of the dashboard and some UI operations might not be available to you, depending on your user permissions. See Platform Users and Security.    Home — contains useful links to help you get started with common tasks.\n  Projects — allows creating, viewing, and managing projects. You can view MLOps models, features, artifacts (files), jobs, workflows, and functions; run jobs and workflows; and schedule jobs; for more information, see Data Science and MLOps. You can also create, view, and configure Nuclio serverless functions and API gateways; for more information, see Nuclio Serverless Functions.\n  Pipelines — displays a Kubeflow Pipelines dashboard for managing your ML pipelines.   Services — displays information about application services for the logged-in user with options to create, run, configure, monitor, enable, disable, restart, and delete services from a single interface. For more information, see Platform Services. Note Except where otherwise specified, when you open a service from the dashboard you're automatically logged into the service as the active dashboard user. You need to add a security exception upon the first login to any of the HTTPS URLs.      Data — allows viewing information about the data containers that make up the platform data layer, browsing their contents, uploading and downloading files; and managing data-access policies. For more information, see Data Layer.\n  Clusters — displays information about the platform's data and application clusters, including status, alerts, and performance data. For data clusters, you can change the cluster's status (for example, take the cluster offline or shut it down), collect logs, and get data-artifact versions. For application clusters, you can view a graphical cluster-status Grafana dashboard. For more information, see Cluster Management and Deployment.\n  Storage — displays information about the platform's data nodes and related data disks, including available and used capacity and per-container usage.\n  Identity — allows managing the platform's users and user groups, including working with an external identity provider (IdP). For more information, see Platform Users and Security.\n  Events — displays platform event logs, alerts, and audit information. For more information, see Logging, Monitoring, and Debugging.\n  Help — displays useful information and links to assist you in using in using the platform.\n  See Also  Introducing the Platform Iguazio Trial Quick-Start Data Science and MLOps Data Layer Platform Services Cluster Management and Deployment Dashboard (UI) software specifications and restrictions  ","keywords":["dashboard,","platform","dashboard,","GUI,","graphical","user","interface,","UI,","user","interface,","project,","mlops,","mlrun,","serverless,","nuclio,","kuebflow","pipelines,","pipelines,","data,","data","layer,","data","containers,","browsing","data,","clusters,","data","clusters,","application","clusters,","cluster","management,","user","management,","users,","security,","idp,","events,","logs,","debugging,","help"],"path":"https://github.com/jasonnIguazio/intro/ui/","title":"The Platform Dashboard (UI)"},{"content":" The platform's application development ecosystem includes\n Distributed data frameworks and engines — such as Spark, Presto, Horovod, and Hadoop. The Nuclio serverless framework. Enhanced support for time-series databases (TSDBs) — including a CLI tool, serverless functions, and integration with Prometheus. Jupyter Notebook and for development and testing of data science and general data applications. A web-based shell shell) service and Jupyter terminals, which provide bash command-line shells for running application services and performing basic file-system operations. Integration with popular Python machine-learning and scientific-computation packages for development of ML and artificial intelligence (AI) applications — such as TensorFlow, Keras, scikit-learn, pandas, PyTorch, Pyplot, and NumPy. Integration with common Python libraries that enable high-performance Python based data processing — such as Dask and RAPIDS. Support for Data Science Automation (MLOps) Services using the MLRun library and Kubeflow Pipelines — including defining, running, and tracking managed, scalable, and portable ML tasks and full workflow pipelines. The V3IO Frames open-source unified high-performance DataFrame API library for working with NoSQL, stream, and time-series data in the platform. Support for executing code over GPUs. Integration with data analytics, monitoring, and visualizations tools — including built-in integration with the open-source Grafana metric analytics and monitoring tool and easy integration with commercial business-intelligence (BI) analytics and visualization tools such as Tableau, Looker, and QlikView. Logging and monitoring services for monitoring, indexing, and viewing application-service logs — including a log-forwarder service and integration with Elasticsearch.  For basic information about how to manage and create services in the dashboard, see Working with Services. For detailed service specifications, see the platform's Support and Certification Matrix.\nNoteIf you're looking for a specific service or tool, see also the alphabetical list of supported services and tools in the application services overview.  ","keywords":["application","services,","services,","managed","services,","managed","application","services,","integrated","services,","integrated","application","services,","pre-deployed","services,","preinstalled","services,","ecosystem,","development","ecosystem,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/","title":"The Platform's Application Services"},{"content":" The standard platform installations have several predefined containers — \u0026quot;users\u0026quot;, \u0026quot;projects\u0026quot;, and \u0026quot;bigdata\u0026quot;.\nThe \u0026quot;users\u0026quot; Container The \u0026quot;users\u0026quot; container is designed to provide individual user development environments and is used by the platform to manage application services. When creating a new web-based shel or Jupyter Notebook, the platform automatically creates a \u0026lt;username\u0026gt; directory for the service's running user in This container (if it doesn't already exist). This directory serves as the home directory of the service environment ($HOME) and is used to store different files for managing the service.\nNoteSee the restrictions for this container in the Software Specifications and Restrictions.  Predefined Environment Variables The platform's command-line services (Jupyter Notebook and the web shell) predefine the following environment variables for simplifying access to the running-user directory of the predefined \u0026quot;users\u0026quot; container:\n V3IO_USERNAME — set to the username of the running user of the Jupyter Notebook service. V3IO_HOME — set to the running-user directory in the \u0026quot;users\u0026quot; container — users/\u0026lt;running user\u0026gt;. V3IO_HOME_URL — set to the fully qualified v3io path to the running-user directory — v3io://users/\u0026lt;running user\u0026gt;.  The \u0026quot;projects\u0026quot; Container The \u0026quot;projects\u0026quot; container is designed for storing shared project artifacts.\nNote When creating a new shared project, the default project artifacts path is projects/\u0026lt;project name\u0026gt;/artifacts; (in the current release, all projects are shared across the parent tenant). By default, the \u0026quot;projects\u0026quot; container isn't protected by the Data management policy, because the data in this container is designed to be shared. However, a security administrator can select to add such protection.    The \u0026quot;bigdata\u0026quot; Container The bigdata\u0026quot; container has no special significance in the current release, and it will no longer be predefined in future releases. However, you'll still be able to use your existing \u0026quot;bigdata\u0026quot; container and all its data, or create a custom container by this name if it doesn't already exist. See Also  Container Names and IDs Working with Data Containers Data Objects  ","keywords":["predefined","data","containers,","predefined","containers,","data","containers,","containers,","users","container,","users,","projects","container,","project,","bigdata","container,","bigdata"],"path":"https://github.com/jasonnIguazio/data-layer/containers/predefined-containers/","title":"The Predefined Containers"},{"content":" Overview The web-based shell service and the terminals of the Jupyter Notebook service are automatically connected to the Presto service and include a copy of the native Presto command-line interface (CLI) — presto-cli — which you can use to query data in the platform. The native Presto CLI is found in the /usr/local/bin directory, which is included in the environment path ($PATH) to simplify execution from any directory. To facilitate using Presto with the Iguazio Presto connector to query NoSQL tables in the platform's data containers, the environment path also contains a presto wrapper that preconfigures your cluster's Presto server URL, the v3io catalog, the Presto user's username and password (platform access key), and the Presto Java TrustStore file and password. For detailed information about Presto and its CLI, refer to the Presto documentation.\nYou start the Presto CLI by running either presto (recommended) or presto-cli from a web shell or JupyterLab terminal. For information about the supported CLI options, see CLI Options. For example:\npresto You can stop the CLI, at any time, by running the exit command.\nCLI Options When starting the CLI, you can specify any supported native Presto CLI option. Use the --help option to see a full list. The following options are especially relevant when using the CLI in the platform:\n --server Sets the location of the Presto server. The presto wrapper already preconfigures the server location for your platform cluster. However, when running presto-cli, you must set --server to the location of the Presto server in your cluster. The Presto server URL is the API URL of the predefined Presto service (presto), which you can copy from the Services page of the platform dashboard. The following command demonstrates setting the Presto server URL to https://presto-api-presto.default-tenant.app.mycluster.iguazio.com:\npresto-cli --server https://presto-api-presto.default-tenant.app.mycluster.iguazio.com   --catalog Sets the default Presto-connector catalog. If you don't configure a default catalog, you need to specify the catalog in the FROM string of each Presto command; for table commands, the catalog is specified at the start of the table path. (You can override the default configuration by specifying another catalog in specific Presto commands.)\nTo use the Iguazio Presto connector to query platform NoSQL tables, you need to use the v3io catalog (see The v3io Catalog). The presto wrapper already preconfigures the catalog to v3io. When running presto-cli, you can optionally use the --catalog option to set the default Presto-connector catalog to v3io. For example:\npresto-cli --catalog v3io --server https://presto-api-presto.default-tenant.app.mycluster.iguazio.com   --schema Sets the default Presto schema. In the MLOps Platform, the Presto schema is the name of the data container that contains the queried tables. If you don't configure a default container, you need to include the container name as part of the table path in each Presto command. (You can override the default configuration by specifying another container name in specific Presto commands.) This is true for both presto-cli and presto. For example, the following command configures the CLI to query tables in the \u0026quot;projects\u0026quot; container using presto:\npresto --schema projects And this is an example of a similar command using the native Presto CLI (presto-cli):\npresto-cli --schema projects --catalog v3io --server https://presto-api-presto.default-tenant.app.mycluster.iguazio.com    The v3io Catalog To configure Presto to work with the Iguazio Presto connector for querying data in the platform's NoSQL store, you need to use the connector's custom v3io Presto catalog. The presto CLI wrapper already preconfigures this catalog. When running the native Presto CLI (presto-cli), you can set the --catalog option to configure v3io as the default catalog. For example:\npresto-cli --catalog v3io --server https://presto-api-presto.default-tenant.app.mycluster.iguazio.com If you don't configure the v3io catalog when starting the CLI (either by using presto or by explicitly setting --catalog v3io), you need to specify the catalog in the FROM string of each Presto command; for commands that reference platform NoSQL tables, the table path must begin with v3io (see Table Paths for details). The following example queries a NoSQL \u0026quot;mytable\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container:\nSELECT * from v3io.mycontainer.mytable; Supported Commands After starting the Presto CLI, you can run supported commands for your selected catalog from the Presto command line. Version 3.2.1 of the Iguazio Presto connector's v3io catalog supports the Presto CREATE VIEW, DROP VIEW, SELECT, SHOW CATALOGS, SHOW CREATE VIEW, SHOW FUNCTIONS, SHOW SCHEMAS, and SHOW TABLES queries and the custom v3io.schema.infer command. See the v3io query examples. Note  SHOW TABLES returns only tables that reside in the container's root directory, provided the access key includes data-access permissions for this directory.\n  To use the view commands (CREATE VIEW, DROP VIEW, and SHOW CREATE VIEW), you first need to enable Hive for the Presto service. See Enabling Hive. You can then save views of platform NoSQL tables, as well as other supported file types, to the default schema of the Hive presto connector (hive.default).\n    Table Paths When using the Iguazio Presto connector, you can specify table paths in one of two ways:\n  Table name — this is the standard Presto syntax and is currently supported only for tables that reside directly in the root directory of the configured data container (Presto schema).\n When using built-in Presto commands, such as SELECT, you specify the path as v3io.\u0026lt;container name\u0026gt;.\u0026lt;table name\u0026gt;. For example, SELECT * FROM v3io.mycontainer.mytable;. When using the custom v3io.schema.infer command, you pass the container and table names as separate parameters — v3io.schema.infer('\u0026lt;container name\u0026gt;', '\u0026lt;table name\u0026gt;');. For example, call v3io.schema.infer ('mycontainer', 'mytable');.    File path — the relative path to the table within the configured data container (/path/to/table). Currently, nested tables in the platform's data containers must be referenced using this syntax.\n When using built-in Presto commands, such as SELECT, you specify the path as v3io.\u0026lt;container name\u0026gt;.\u0026quot;/path/to/table\u0026quot;. For example, SELECT * FROM v3io.mycontainer.\u0026quot;/mytables/cars/vendors\u0026quot;;. Note that the table path must be embedded within double quotes. When using the custom v3io.schema.infer command, you pass the container name and table path as separate parameters — v3io.schema.infer('\u0026lt;container name\u0026gt;', '/path/to/table');. For example, call v3io.schema.infer ('mycontainer', '/mytables/cards/vendors');.     Note  For both syntax variations, in standard Presto commands you can optionally omit the catalog and container (schema) names if they're already preconfigured; see the CLI --catalog and --schema options. The presto wrapper preconfigures the v3io catalog.\n  Tables in a data container's root directory can be accessed by using either the table-name or file-path syntax. The table-name syntax is simpler but slower. Therefore, it's recommended that you use the path syntax when you need to frequently repeat a specific query.\n  Table-path letter case —\n The table-name syntax (which is supported for tables in the root container directory) ignores the letter case in the table path. Therefore, it also supports uppercase letters in the path; (note that the table names will appear in lowercase letters in query results). The file-path syntax doesn't currently support uppercase letters in the table path.      Defining the NoSQL Table Schema Presto handles structured data. Therefore, it needs to be aware of the schema of the data structure. (Don't confuse this with native Presto schemas, which are used for organizing tables — as explained, for example, for the --schema option.) When writing NoSQL data in the platform using Frames or a Spark DataFrame, the schema of the data table is automatically identified and saved and then retrieved when using Frames, Spark DataFrames, or Presto to read data from the same table (unless you select to explicitly define the schema for the read operation). However, to use Presto, Frames, or Spark DataFrames to read NoSQL data that was written to a table in another way, you first need to define the table schema. You can do this by using the platform's custom Presto v3io.schema.infer command, which generates the required schema file. For more information, see the NoSQL Table Schema Reference. The v3io.schema.infer Command The Iguazio Presto connector exposes a v3io.schema.infer command that can be used to infer the schema of an existing table by analyzing its data. The command has the following syntax — where \u0026lt;container name\u0026gt; is the name of the data container (schema) that contains the table and \u0026lt;relative table path\u0026gt; is the relative path to the table within the container (see Table Paths):\ncall v3io.schema.infer(\u0026#39;\u0026lt;container name\u0026gt;\u0026#39;, \u0026#39;\u0026lt;relative table path\u0026gt;\u0026#39;); When the table resides in the container's root directory, the relative path can be the table name. For example, the following command infers the schema of a \u0026quot;mytable\u0026quot; table in the root directory of a \u0026quot;mycontainer\u0026quot; data container:\ncall v3io.schema.infer(\u0026#39;mycontainer\u0026#39;, \u0026#39;mytable\u0026#39;); For nested tables, you need to specify the table path as '/path/to/table'. For example, the following command infers the schema of a \u0026quot;mytable\u0026quot; table in a mydata directory of a \u0026quot;mycontainer\u0026quot; data container:\ncall v3io.schema.infer(\u0026#39;mycontainer\u0026#39;, \u0026#39;/mydata/mytable\u0026#39;); The infer-schema command creates a JSON schema file (.#schema) in the table directory. You can find more information about this file in the NoSQL Table Schema Reference, although note that you don't typically need to edit this file.\nPartitioned Tables Table partitioning is a common technique for optimizing physical data layout and related queries. In a partitioned table, some item attributes (columns) are used to create partition directories within the root table directory using the format \u0026lt;table path\u0026gt;/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;[/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;/...], and each item is then saved to the respective partition directory based on its attribute values. For example, for a \u0026quot;mytable\u0026quot; table with year and month attribute partitions, an item with attributes year = 2018 and month = 1 will be saved to a mytable/year=2018/month=1/ directory. This allows for more efficient data queries that search for the data only in the relevant partition directories instead of scanning the entire table. This technique is used, for example, by Hive, and is supported for all the built-in Spark Dataset file-based data sources (such as Parquet, CSV, and JSON). See also the Partitioned Tables overview in the Working with NoSQL Data documentation, including best practices.\nThe Iguazio Presto connector supports querying of partitioned NoSQL tables: a partitioned table is queried like any other table, with the table path set to the root table directory and not to a specific partition directory. When processing queries, the platform searches the root table directory that is specified in the read command for nested directories of the format \u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;. If it finds such directories, it searches only the partition directories that match the query. For example, for a \u0026quot;mytable\u0026quot; table in a \u0026quot;mycontainer\u0026quot; data container that's partitioned by year and month attributes, a SELECT * FROM v3io.mycontainer.mytable WHERE month = 12; query will return only the items from the month=12 partition directories in all year=* directories.\nRead Optimization The Iguazio Presto connector supports the following optimized table queries (reads), which are more efficient compared to the standard full table scan:\n Faster item-specific queries Range Scans  For more information about these query types, see NoSQL read optimization.\nFaster Item-Specific Queries The fastest Presto NoSQL table queries are those that uniquely identify a specific item by its primary-key value. See NoSQL faster item-specific queries .\nItem-Specific Query OperatorsThe Iguazio Presto connector executes this faster processing for queries that apply the equal-to (=) or IN (IN) operator to the sharding-key attribute and optionally also apply one of these operators the sorting-key attribute (in the case of a compound primary key).  Faster Item-Specific Query Examples The following commands all identify a specific item by its primary-key value and will be processed more quickly than table-scan processing; (it is assumed that v3io is configured as the default catalog):\n  Retrieve an item with the simple primary-key value \u0026quot;345\u0026quot;:\nSELECT * FROM mycontainer.mytable\u0026#34; WHERE id = 345;   Retrieve an item with the compound primary-key value \u0026quot;myfile.txt\u0026quot;:\nSELECT * FROM mycontainer.mytable\u0026#34; WHERE basename = \u0026#39;myfile\u0026#39; and suffix = \u0026#39;txt\u0026#39;;   Range Scans A Presto NoSQL table query that uses supported sharding-key and optional sorting-key filters to retrieve items with the same sharding-key value, is processed by performing a range scan, which is more efficient than the standard full table scan. See NoSQL range scans. Range-Scan OperatorsThe Iguazio Presto connector uses range scan for compound primary-key table queries that apply the equal-to (=) or IN (IN) operator to the sharding-key attribute, and optionally also apply a comparison operator (=/\u0026gt;/\u0026gt;=/\u0026lt;/\u0026lt;= / BETWEEN) to the sorting-key attribute.  Range-Scan Query Examples The following commands query a \u0026quot;rides\u0026quot; table in a mytaxis table in a \u0026quot;mycontainer\u0026quot; data container. The table is assumed to have compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key, a driver_id sharding-key attribute, a date sorting-key attribute, and a compatible schema — which enable performing range-scan queries that use a sharding-key and optionally also a sorting-key filter. You can find sample Spark DataFrame code for creating a compatible range-scan table in the NoSQL Spark DataFrame reference. The following commands and outputs are compatible with this sample table; (it is assumed that v3io is configured as the default catalog):\n  Retrieve all items with a driver_id sharding-key attribute value of 1 (regardless of the sorting-key value):\nSELECT * FROM mycontainer.\u0026#34;/mytaxis/rides/\u0026#34; WHERE driver_id = 1; Output date | avg_ride_km | avg_ride_passengers | driver_id | total_km | num_rides | total_passengers ----------+--------------------+---------------------+-----------+----------+-----------+------------------ 20180601 | 5.0 | 1.6 | 1 | 125.0 | 25 | 40 20180602 | 5.3 | 2.3 | 1 | 106.0 | 20 | 46 20180701 | 3.8000000000000003 | 1.5 | 1 | 106.4 | 28 | 42 (3 rows) \n  Retrieve all items with a driver_id sharding-key attribute value of 24 and a date sorting-key attribute value within the first six months of 2018:\nSELECT * FROM mycontainer.\u0026#34;/mytaxis/rides/\u0026#34; WHERE driver_id = 24 AND date \u0026gt;= \u0026#39;20180101\u0026#39; AND date \u0026lt; \u0026#39;20180701\u0026#39;; Output date | avg_ride_km | avg_ride_passengers | driver_id | total_km | num_rides | total_passengers ----------+-------------+---------------------+-----------+----------+-----------+------------------ 20180602 | 52.0 | 2.2 | 24 | 260.0 | 5 | 11 20180601 | 41.5 | 2.25 | 24 | 332.0 | 8 | 18 (2 rows) \n  Retrieve all items with a driver_id sharding-key attribute value of 1, 16, or 24 (regardless of the sorting-key value) and an avg_ride_passengers attribute value that is greater or equal to 3:\nSELECT * FROM mycontainer.\u0026#34;/mytaxis/rides/\u0026#34; WHERE driver_id IN (1, 16, 24) AND avg_ride_passengers \u0026gt;= 3; Output date | avg_ride_km | avg_ride_passengers | driver_id | total_km | num_rides | total_passengers ----------+--------------------+---------------------+-----------+----------+-----------+------------------ 20180701 | 32.199999999999996 | 4.0 | 16 | 193.2 | 6 | 24 20180601 | 224.2 | 8.0 | 16 | 224.2 | 1 | 8 20180602 | 24.4 | 4.5 | 16 | 244.0 | 10 | 45 20180701 | 50.300000000000004 | 3.0 | 24 | 352.1 | 7 | 21 (4 rows) \n  v3io Query Examples Note The examples in this section assume that v3io has been configured as the default catalog. If this isn't the case, add \u0026quot;FROM v3io\u0026quot; in SHOW SCHEMAS commands and \u0026quot;v3io.\u0026quot; at the start of the table paths (before the name of the data container) in SHOW TABLES and SELECT commands. For example, replace \u0026quot;projects.mytable\u0026quot; with \u0026quot;v3io.projects.mytable\u0026quot;; see Table Paths for details. See also the separate range-scan query examples.    The following command lists all the data containers (schemas) in the parent platform tenant whose names end in \u0026quot;data\u0026quot;:\nSHOW SCHEMAS LIKE \u0026#39;%data\u0026#39;; The following command lists all the tables in the root directory of the \u0026quot;projects\u0026quot; data container. (Remember that the SHOW TABLES command only identifies tables in the container's root directory.)\nSHOW TABLES IN projects; The following command shows the contents of a \u0026quot;mytable\u0026quot; table in the \u0026quot;projects\u0026quot; data container:\nSELECT * FROM projects.mytable; The following command shows the contents of a nested \u0026quot;tests/nosql/table1\u0026quot; table in the \u0026quot;projects\u0026quot; container; (see the file-path syntax):\nSELECT * FROM projects.\u0026#34;/tests/nosql/table1\u0026#34;; If you configured the default Presto schema when starting the CLI to \u0026quot;projects\u0026quot;, you can optionally run the commands from the previous examples without explicitly specifying the name of the data container:\nSHOW TABLES; SELECT * FROM mytable; SELECT * FROM \u0026#34;/tests/nosql/table1\u0026#34;; v3io Query Examples Using Views The following commands demonstrate how to create and use a query view for a NoSQL table. Note that because the view is saved to the default Hive schema, before you create the view you need to ensure that Hive is enabled for the platform's Presto service: NoteYou can generate a compatible table by running the first steps of the getting-started example in the basic-data-ingestion-and-preparation.ipynb tutorial notebook. Just remember to replace iguazio in the following commands with your platform username.    Create an iguazio_stocks_tab_etc_view view of a SELECT query for all items with the securitytype attribute value \u0026quot;ETC\u0026quot; in a users/\u0026lt;username\u0026gt;/examples/stocks_tab table for user \u0026quot;iguazio\u0026quot;:\nCREATE VIEW hive.default.iguazio_stocks_tab_etc_view AS SELECT * FROM users.\u0026#34;/iguazio/examples/stocks_tab\u0026#34; WHERE securitytype = \u0026#39;ETC\u0026#39;;   Show the view's SQL statement:\nSHOW CREATE VIEW hive.default.iguazio_stocks_tab_etc_view;   Use the view to return all items in the table:\nSELECT * FROM hive.default.iguazio_stocks_tab_etc_view; You can also optionally apply an additional filter to the view query. For example, the following query returns all items with the securitytype attribute value \u0026quot;ETC\u0026quot; (view query) and a numberoftrades attribute value that's greater than 1:\nSELECT * FROM hive.default.iguazio_stocks_tab_etc_view WHERE numberoftrades \u0026gt; 1;   See Also  Presto Overview Working with NoSQL Data NoSQL Table Schema Reference  ","keywords":["presto","cli,","presto-cli,","presto,","cli,","Iguazio","Presto","connector,","v3io","catalog,","v3io,","data","paths,","table","paths,","tables,","nosql,","nosql","tables,","partitioning,","partitions,","primary","key,","sharding","key,","sorting","key,","range","scan"],"path":"https://github.com/jasonnIguazio/data-layer/presto/presto-cli/","title":"The Presto CLI"},{"content":" Presto is an open-source distributed SQL query engine for running interactive analytic queries. The platform has a pre-deployed tenant-wide Presto service that can be used to run SQL queries and perform high-performance low-latency interactive data analytics. You can ingest data into the platform using your preferred method — such as using Spark, the NoSQL Web API, a Nuclio function, or V3IO Frames — and use Presto to analyze the data interactively with the aid of your preferred visualization tool. Running Presto over the platform's data services allows you to filter data as close as possible to the source.\nYou can run SQL commands that use ANSI SQL SELECT statements, which will be executed using Presto, from Jupyter Notebook, a serverless Nuclio function, or a local or remote Presto client. The platform comes pre-deployed with the native Presto CLI client (presto-cli), a convenience wrapper to this CLI that preconfigures some options for local execution (presto), and the Presto web UI — which you can log into from the dashboard's Services page. You can also integrate the platform's Presto service with a remote Presto client — such as Tableau or QlikView \u0026gt;}} — to remotely query and analyze data in the platform over a Java database connectivity (JDBC) connector.\nThe Iguazio Presto connector enables you to use Presto to run queries on data in the platform's NoSQL store — including support for partitioning, predicate pushdown, and column pruning, which enables users to optimize their queries.\nYou can also use Presto's built-in Hive connector to query data of the supported file types, such as Parquet or ORC, or to save table-query views to the default Hive schema. Note that to use the Hive connector, you first need to create a Hive Metastore by enabling Hive for the platform's Presto service. For more information, see Using the Hive Connector in the Presto overview. The platform also has a built-in process that uses Presto SQL to create a Hive view that monitors both real-time data in the platform's NoSQL store and historical data in Parquet or ORC tables [Tech Preview]. For more information about using Presto in the platform, see the Presto Reference. See also the Presto and Hive restrictions in the Software Specifications and Restrictions documentation.\nNoteIf you delete the Presto service, Hive and Mariadb will automatically be disabled.  See Also  Working with Services Using Presto The Presto CLI The Spark Service Working with NoSQL Data Presto and Hive software specifications and restrictions  ","keywords":["presto,","presto","cli,","Iguazio","Presto","connector,","distributed","sql","query","engine,","sql","query","engine,","hive,","hive","metastore,","parquet,","parquez,","orc,","sql","queries,","sql,","nosql,","key-value,","kv,","time-series","databases,","time","series,","tsdb,","jupyter,","jupyter","notebook,","jupyter","terminals,","jupyter","tutorials,","v3io","tutorials,","tutorials,","web","shell,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/presto/","title":"The Presto Service"},{"content":" The platform is integrated with the Apache Spark data engine for large-scale data processing, which is available as a user application service. You can use Spark together with other platform services to run SQL queries, stream data, and perform complex data analysis — both on data that is stored in the platform's data store and on external data sources such as RDBMSs or traditional Hadoop \u0026quot;data lakes\u0026quot;. The support for Spark is powered by a stack of Spark libraries that include Spark SQL and DataFrames for working with structured data, Spark Streaming for streaming data, MLlib for machine learning, and GraphX for graphs and graph-parallel computation. You can combine these libraries seamlessly in the same application. Spark is fully optimized when running on top of the platform's data services, including data filtering as close as possible to the source by implementing predicate pushdown and column-pruning in the processing engine. Predicate pushdown and column pruning can optimize your query, for example, by filtering data before it is transferred over the network, filtering data before loading it into memory, or skipping reading entire files or chunks of files.\nThe platform supports the standard Spark Dataset and DataFrame APIs in Scala, Java, Python, and R. In addition, it extends and enriches these APIs via the Iguazio Spark connector, which features a custom NoSQL data source that enables reading and writing data in the platform's NoSQL store using Spark DataFrames — including support for table partitioning, data pruning and filtering (predicate pushdown), performing \u0026quot;replace\u0026quot; mode and conditional updates, defining and updating counter table attributes (columns), and performing optimized range scans. The platform also supports the Spark Streaming API. For more information, see the Spark APIs reference.\nYou can run Spark jobs on your platform cluster from a Jupyter web notebook; for details, see Running Spark Jobs from a Web Notebook. You can also run Spark jobs by executing spark-submit from a web-based shell or Jupyter terminal or notebook; for details, see Running Spark Jobs with spark-submit. You can find many examples of using Spark in the platform's Jupyter tutorial notebook, Spark data-ingestion quick-start tutorial, and Spark APIs reference. See also the Spark restrictions in the Software Specifications and Restrictions documentation. You can also use the Spark SQL and DataFrames API to run Spark over a Java database connectivity (JDBC) connector. For more information, see Ingesting and Preparing Data.\nNoteIt's good practice to create a Spark session at the start of the execution flow (for example, by calling SparkSession.builder and assigning the result to a spark variable) and stop the session at the end of the flow to release resources (for example, by calling spark.stop()).  See Also  Working with Services Spark API References The Presto Service Working with NoSQL Data Time-Series Databases (TSDB) Working with Data Streams (\u0026quot;Streaming\u0026quot;) Spark software specifications and restrictions  ","keywords":["spark,","spark","sql","and","datafarames,","spark","sql,","sql,","spark","dataframes,","Iguazio","Spark","connector,","dataframes,","spark","nosql","dataframe,","nosql","dataframe,","r","language,","sparkr,","nosql,","key-value,","kv,","spark","streaming,","spark-streaming","integration","api,","streaming,","streams,","jupyter,","jupyter","notebook,","jupyter","terminals,","jupyter","tutorials,","v3io","tutorials,","tutorials,","web","shell,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/spark/","title":"The Spark Service"},{"content":"The platform a service for Tensorboard which is TensorFlow's visualization toolkit. In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\nThe platform writes the outputs of jobs in a TensorBoard log file which allows users to view and compare results and neural networks.\nUsers can create a new Tensorboard service and must configure the following custom parameters:\n Container—select a container from the dropdown menu Path—the path to the loacation of the Tensorboard log file  See Also  Working with Services  ","keywords":["tensorboard,","hdfs,","hcfs,","file","system,","command","line,","jupyter,","jupyter","notebook,","jupyter","terminals,","web","shell,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/tensorboard/","title":"The Tensorboard Service"},{"content":" Overview The V3IO TSDB includes the V3IO TSDB command-line interface (\u0026quot;the TSDB CLI\u0026quot;), which enables users to easily create, update, query, and delete time-series databases (TSDBs), as demonstrated in this tutorial. Before you get started, read the setup and usage information in this section and review the TSDB software specifications and restrictions.\n Setup Reference Mandatory Command Configurations Using a Configuration File  Setup The TSDB CLI can be run locally on a platform application cluster or remotely from any computer with a network connection to the cluster. The platform's web shell and Jupyter Notebook services include a compatible Linux version of the TSDB CLI — tsdbctl, which is found in the $IGUAZIO_HOME/bin directory; the installation directory is included in the shell path ($PATH) to simplify execution from anywhere in the shell. For remote execution, download the CLI from the V3IO TSDB GitHub repository.\nIn the web shell and Jupyter terminal environments there's also a predefined tsdbctl alias to the native CLI that preconfigures the --server flag to the URL of the web-APIs service and the --access-key flag to the authentication access key for the running user of the parent shell or Jupyter Notebook service; you can override the default configurations in your CLI commands. When running the CLI from an on-cluster Jupyter notebook or remotely, you need to configure the web-APIs service and authentication credentials yourself, either in the CLI command or in a configuration file, as outlined in this tutorial.\n Note Version 3.2.1 of the platform is compatible with version 0.11 of the V3IO TSDB. Please consult Iguazio's support team before using another version of the CLI. When using a downloaded version of the CLI (namely for remote execution), it's recommended that you add the file or a symbolic link to it (such as tsdbctl) to the execution path on your machine ($PATH), as done in the platform command-line environments. For the purpose of this tutorial, it's assumed that tsdbctl is found in your path and is used to run the relevant version of the CLI.    Reference Use the CLI's embedded help for a detailed reference:\n  Run the general help command to get information about of all available commands:\ntsdbctl help    Run tsdbctl help \u0026lt;command\u0026gt; or tsdbctl \u0026lt;command\u0026gt; -h to view the help reference for a specific command. For example, use either of the following variations to get help for the query command:\ntsdbctl help query tsdbctl query -h    Mandatory Command Configurations All CLI commands demonstrated in this tutorial require that you configure the following flags. This can be done either in the CLI command itself or in a configuration file. As explained in the Setup section, when running the CLI locally from an on-cluster web shell or Jupyter terminal, you can use the tsdbctl alias, which preconfigures the --server and --access-key flags.\n  User-authentication flags — one of the following alternatives:\n For access-key authentication —  -k|--access-key — a valid access key for logging into the configured web-APIs service. You can get the access key from the Access Keys window that's available from the dashboard user-profile menu, or by copying the value of the V3IO_ACCESS_KEY environment variable in a web-shell or Jupyter Notebook service. Note The tsdbctl alias that's available in the platform's web shell and Jupyter terminal environments preconfigures the --access-key flag for the running user. When running the native V3IO TSDB CLI locally — for example, from a Jupyter notebook, which doesn't have the tsdbctl alias — you can set the -k or --access-key flag to $V3IO_ACCESS_KEY.       For username-password authentication —  -u|--username — a valid username for logging into the configured web-APIs service. -p|--password — the password of the configured web-APIs service user.      -s|--server — the endpoint of your platform's web-APIs (web-gateway) service. The tsdbctl alias that's available in the platform's web shell and Jupyter terminal environments preconfigures this flag for the running user. If you're not using the alias — for example, if you're running the native TSDB CLI from a Jupyter notebook or remotely — set this flag to \u0026lt;web-APIs IP\u0026gt;:\u0026lt;web-APIs HTTP port\u0026gt;:\n \u0026lt;web-APIs IP\u0026gt; — the IP address of the web-APIs service; for example, webapi.default-tenant.app.mycluster.iguazio.com. The IP address is stored in a V3IO_WEBAPI_SERVICE_HOST environment variable in the platform's web shells and Jupyter notebooks and terminals You can also get this address from the web-APIs HTTPS URL: copy the HTTPS API link of the web-APIs service (webapi) from the Services dashboard page, and then remove https:// from the start of the URL. \u0026lt;web-APIs HTTP port\u0026gt; — the HTTP port of the web-APIs service. The port number is stored in a V3IO_WEBAPI_SERVICE_PORT environment variable in the platform's web shells and Jupyter notebooks and terminals.    -c|--container — the name of the parent data container of the TSDB instance (table). For example, \u0026quot;projects\u0026quot; or \u0026quot;mycontainer\u0026quot;.\n  -t|--table-path — the path to the TSDB instance (table) within the configured container. For example \u0026quot;my_metrics_tsdb\u0026quot; or \u0026quot;tsdbs/metrics\u0026quot;. (Any component of the path that doesn't already exist will be created automatically.) The TSDB table path should not be set in a CLI configuration file.\n  Some commands require additional configurations, as detailed in the command-specific documentation.\nUsing a Configuration File Some of the CLI configurations can be defined in a YAML file instead of setting the equivalent flags in the command line. By default, the CLI checks for a v3io-tsdb-config.yaml configuration file in the current directory. You can use the global CLI -g|--config flag to provide a path to a different configuration file. Command-line configurations override file configurations.\nYou can use the template examples/v3io-tsdb-config.yaml.template configuration file in the V3IO TSDB GitHub repository as the basis for your custom configuration file. The template includes descriptive comments to explain each key.\nTo simplify the examples in this tutorial and focus on the unique options of each CLI command, the examples assume that you have created a v3io-tsdb-config.yaml file in the directory from which you're running the CLI (default path) and that this file configures the following keys; note that the web-APIs service and user-authentication configurations aren't required if you use the on-cluster tsdbctl alias, which preconfigures these flags for the running user:\n  webApiEndpoint — the equivalent of the CLI -s|--server flag.\n  container — the equivalent of the CLI -c|--container flag.\n  accesskey — the equivalent of the CLI -k|--access-key flag.\nAlternatively, you can set the following flags for username-password authentication:\n username — the equivalent of the CLI -u|--username flag. password — the equivalent of the CLI -p|--password flag.    Following is an example configuration file. Replace the IP address and access key in the values of the webApiEndpoint and accessKey keys with your specific data; you can also select to replace the accesskey key with username and password keys:\n# File: v3io-tsdb-config.toml# Description: V3IO TSDB Configuration File# Endpoint of an Iguazio MLOps Platform web APIs (web-gateway) service,# consisting of an IP address or resolvable host domain namewebApiEndpoint:\u0026#34;webapi.default-tenant.app.mycluster.iguazio.com\u0026#34;# Name of an Iguazio MLOps Platform container for storing the TSDB tablecontainer:\u0026#34;projects\u0026#34;# Authentication credentials for the web-APIs serviceaccessKey:\u0026#34;MYACCESSKEY\u0026#34;# OR#username: \u0026#34;MYUSER\u0026#34;#password: \u0026#34;MYPASSWORD\u0026#34;For example, the following CLI command for getting information about a \u0026quot;mytsdb\u0026quot; TSDB in the \u0026quot;projects\u0026quot; container —\ntsdbctl info -c projects -t mytsdb -n -m -s webapi.default-tenant.app.mycluster.iguazio.com -k MYACCESSKEY — is equivalent to the following command when the current directory has the aforementioned example v3io-tsdb-config.yaml file:\ntsdbctl info -t mytsdb -n -m As indicated above, you can override any of the file configurations in the command line. For example, you can add -c metrics to the previous command to override the default \u0026quot;projects\u0026quot; container configuration and get information for a \u0026quot;mytsdb\u0026quot; table in a custom \u0026quot;metrics\u0026quot; container:\ntsdbctl info -t mytsdb -n -m -c metrics Creating a New TSDB Use the CLI's create command to create a new TSDB instance (table) — i.e., create a new TSDB. The command receives a mandatory -r|--ingestion-rate flag, which defines the TSDB's metric-samples ingestion rate. The rate is specified as a string of the format \u0026quot;[0-9]+/[smh]\u0026quot; (where 's' = seconds, 'm' = minutes, and 'h' = hours); for example, \u0026quot;1/s\u0026quot; (1 sample per second), \u0026quot;20/m\u0026quot; (20 samples per minute), or \u0026quot;50/h\u0026quot; (50 samples per hour). It's recommended that you set the rate to the average expected ingestion rate for a unique label set (for example, for a single server in a data center), and that the ingestion rates for a given TSDB table don't vary significantly; when there's a big difference in the ingestion rates (for example, x10), consider using separate TSDB tables.  NoteIn the current release, the create command doesn't support the -l|--cross-label flag.  Examples The following command creates a new \u0026quot;tsdb_example\u0026quot; TSDB in the configured \u0026quot;projects\u0026quot; container with an ingestion rate of one sample per second:\ntsdbctl create -t tsdb_example -r 1/s Defining TSDB Aggregates You can optionally use the -a|--aggregates flag of the create CLI command to configure a list of aggregation functions (\u0026quot;aggregators\u0026quot;) that will be executed for each metric item in real time during the ingestion of the metric samples into the TSDB. The aggregations results are stored in the TSDB as array attributes (\u0026quot;pre-aggregates\u0026quot;) and used to handle relevant aggregation queries. The aggregators are provided as a string containing a comma-separated list of one or more supported aggregation functions; for example, \u0026quot;avg\u0026quot; (average sample values) or \u0026quot;max,min,last\u0026quot; (maximum, minimum, and latest sample values).\nWhen configuring the TSDB's pre-aggregates, you should also use the -i|--aggregation-granularity flag to specify the aggregation granularity — a time interval for executing the aggregation functions. The aggregation granularity is provided as a string of the format \u0026quot;[0-9]+[mhd]\u0026quot; (where 'm' = minutes, 'h' = hours, and 'd' = days); for example, \u0026quot;90m\u0026quot; (90 minutes = 1.5 hours) or \u0026quot;2h\u0026quot; (2 hours). The default aggregation granularity is one hour (1h). Aggregation Notes  You can also perform aggregation queries for TSDB tables without pre-aggregates, but when configured correctly, pre-aggregation queries are more efficient. To ensure that pre-aggregation is used to process aggregation queries and improve performance —\n When creating the TSDB table, set its aggregation granularity (-i|--aggregation-granularity) to an interval that's significantly larger than the table's metric-samples ingestion rate (-r|--ingestion-rate). When querying the table, set the aggregation interval (-i|--aggregation-interval) to a sufficient multiplier of the table's aggregation granularity. For example, if the table's ingestion rate is 1 sample per second (\u0026quot;1/s\u0026quot;) and you want to user hourly queries (i.e., use a query aggregation interval of \u0026quot;1h\u0026quot;), you might set the table's pre-aggregation granularity to 20 minutes (\u0026quot;20m\u0026quot;).    When using the aggregates flag, the CLI automatically adds count to the TSDB's aggregators. However, it's recommended to set this aggregator explicitly if you need it.\n  Some aggregates are calculated from other aggregates. For example, the avg aggregate is calculated from the count and sum aggregates.\n     The following command creates a new \u0026quot;tsdb_example_aggr\u0026quot; TSDB with an ingestion rate of one sample per second in a tsdb_tests directory in the default configured \u0026quot;projects\u0026quot; container. The TSDB is created with the count, avg, min, and max aggregators and an aggregation interval of 1 hour:\ntsdbctl create -t tsdb_example_aggr -r 1/s -a \u0026#34;count,avg,min,max\u0026#34; -i 1h  Supported Aggregation Functions Version 0.11 of the CLI supports the following aggregation functions, which are all applied to the samples of each metric item according to the TSDB's aggregation granularity (interval):\n avg — the average of the sample values. count — the number of ingested samples. last — the value of the last sample (i.e., the sample with the latest time). max — the maximal sample value. min — the minimal sample value. rate — the change rate of the sample values, which is calculated as \u0026lt;last sample value of the previous interval\u0026gt; - \u0026lt;last sample value of the current interval\u0026gt;) / \u0026lt;aggregation granularity\u0026gt;. stddev — the standard deviance of the sample values. stdvar — the standard variance of the sample values. sum — the sum of the sample values.  Adding Samples to a TSDB Use the CLI's add command (or its append alias) to add (ingest) metric samples to a TSDB. You must provide the name of the ingested metric and one or more sample values for the metric. You also need to provide the samples' generation times; when ingesting a single sample, the default sample time is the current time. In addition, you can optionally specify metric labels. Each unique metric name and optional labels combination corresponds to a metric item (row) in the TSDB with attributes (columns) for each label.\nThe ingestion input can be provided in one of two ways:\n  Using command-line arguments and flags —\n  metric argument [Required] — a string containing the name of the ingested metric. For example, \u0026quot;cpu\u0026quot;.\n  labels argument [Optional] — a string containing a comma-separated list of \u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt; key-value pairs. The label values must be of type string and cannot contain commas. For example, \u0026quot;os=mac,host=A\u0026quot;.   -d|--values flag [Required] — a string containing a comma-separated list of sample data values. The values can be of type integer or float and cannot contain periods or commas; note that all values for a given metric must be of the same type. For example, \u0026quot;67.0,90.2,70.5\u0026quot;.   -m|--times flag [Optional for a single metric; Required for multiple samples] — a string containing a comma-separated list of sample generation times (\u0026quot;sample times\u0026quot;) for the provided sample values. A sample time can be specified as a Unix timestamp in milliseconds or as a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where 'm' = minutes, 'h' = hours, and 'd' = days). For example, \u0026quot;1537971020000,now-2d,now-95m,now\u0026quot;.  The default sample time when ingesting a single sample is the current time (i.e., the TSDB ingestion time) — now.\nNoteAn ingested sample time cannot be earlier or equal to the latest previously ingested sample time for the same metric item. This applies also to samples ingested in the same command, so specify the ingestion times in ascending chronological order. For example, an add command with -d \u0026quot;1,2\u0026quot; -m \u0026quot;now,now-1m\u0026quot; will ingest only the first sample (1) and not the second sample (2) because the time of the second sample (now-2) is earlier than that of the first sample (now). To ingest both samples, change the order in the command to -d \u0026quot;2,1\u0026quot; \u0026quot;now-1m,now\u0026quot;.    NoteWhen ingesting samples at scale, use a CSV file or a Nuclio function rather than providing the ingestion input in the command line.    Using the -f|--file flag to provide the path to a CSV metric-samples input file that contains one or more items (rows) of the following format:\n\u0026lt;metric name\u0026gt;,[\u0026lt;labels\u0026gt;],\u0026lt;sample data value\u0026gt;[,\u0026lt;sample time\u0026gt;] The CSV columns (attributes) are the equivalent of the arguments and flags described for the command-line arguments method in the previous bullet and their values are subject to the same guidelines. Note that all rows in the CSV file must have the same number of columns. For ingestion of multiple metrics, specify the ingestion times.\n  Examples  The following commands ingest three samples and a label for a temperature metric and multiple samples with different label combinations and no labels for a cpu metric into the tsdb_example TSDB. The sample times are specified using the -m flag:\ntsdbctl add temperature -t tsdb_example \u0026#34;degrees=Celsius\u0026#34; -d \u0026#34;32,29.5,25.3\u0026#34; -m \u0026#34;now-2d,now-1d,now\u0026#34; tsdbctl add cpu -t tsdb_example -d \u0026#34;90,82.5\u0026#34; -m \u0026#34;now-2d,now-1d\u0026#34; tsdbctl add cpu \u0026#34;host=A,os=linux\u0026#34; -t tsdb_example -d \u0026#34;23.87,47.3\u0026#34; -m \u0026#34;now-18h,now-12h\u0026#34; tsdbctl add cpu \u0026#34;host=A\u0026#34; -t tsdb_example -d \u0026#34;50.2\u0026#34; -m \u0026#34;now-6h\u0026#34; tsdbctl add cpu \u0026#34;os=linux\u0026#34; -t tsdb_example -d \u0026#34;88.8,91\u0026#34; -m \u0026#34;now-1h,now-30m\u0026#34; tsdbctl add cpu \u0026#34;host=A,os=linux,arch=amd64\u0026#34; -t tsdb_example -d \u0026#34;70.2,55\u0026#34; -m \u0026#34;now-15m,now\u0026#34; The same ingestion can also be done by providing the samples input in a CSV file, as demonstrated in the following command:\ntsdbctl add -t tsdb_example -f ~/metric_samples.csv The command uses this example metric_samples.csv file, which you can also download here. Copy the file to your home directory (~/) or change the file path in the ingestion command:\ntemperature,degrees=Celsius,32,now-2d temperature,degrees=Celsius,29.5,now-1d temperature,degrees=Celsius,25.3,now cpu,,90,now-2d cpu,,82.5,now-1d cpu,\u0026#34;host=A,os=linux\u0026#34;,23.87,now-18h cpu,\u0026#34;host=A,os=linux\u0026#34;,47.3,now-12h cpu,host=A,50.2,now-6h cpu,os=linux,88.8,now-1h cpu,os=linux,91,now-30m cpu,\u0026#34;host=A,os=linux,arch=amd64\u0026#34;,70.2,now-15m cpu,\u0026#34;host=A,os=linux,arch=amd64\u0026#34;,55,now  The following command demonstrates ingestion of samples for an m1 label with host and os labels using a CSV file that is found in the directory from which the CLI is run:\ntsdbctl add -t tsdb_example_aggr -f tsdb_example_aggr.csv The command uses this example tsdb_example_aggr.csv file, which you can also download here:\nm1,\u0026#34;os=darwin,host=A\u0026#34;,1,1514802220000 m1,\u0026#34;os=darwin,host=A\u0026#34;,2,1514812086000 m1,\u0026#34;os=darwin,host=A\u0026#34;,3,1514877315000 m1,\u0026#34;os=linux,host=A\u0026#34;,1,1514797500000 m1,\u0026#34;os=linux,host=A\u0026#34;,2,1514799605000 m1,\u0026#34;os=linux,host=A\u0026#34;,3,1514804625000 m1,\u0026#34;os=linux,host=A\u0026#34;,4,1514818759000 m1,\u0026#34;os=linux,host=A\u0026#34;,5,1514897354000 m1,\u0026#34;os=linux,host=A\u0026#34;,6,1514897858000 m1,\u0026#34;os=windows,host=A\u0026#34;,1,1514803048000 m1,\u0026#34;os=windows,host=A\u0026#34;,2,1514808826000 m1,\u0026#34;os=windows,host=A\u0026#34;,3,1514812736000 m1,\u0026#34;os=windows,host=A\u0026#34;,4,1514881791000 m1,\u0026#34;os=darwin,host=B\u0026#34;,1,1514802842000 m1,\u0026#34;os=darwin,host=B\u0026#34;,2,1514818576000 m1,\u0026#34;os=darwin,host=B\u0026#34;,3,1514891100000 m1,\u0026#34;os=linux,host=B\u0026#34;,1,1514798275000 m1,\u0026#34;os=linux,host=B\u0026#34;,2,1514816100000 m1,\u0026#34;os=linux,host=B\u0026#34;,3,1514895734000 m1,\u0026#34;os=linux,host=B\u0026#34;,4,1514900599000 m1,\u0026#34;os=windows,host=B\u0026#34;,1,1514799605000 m1,\u0026#34;os=windows,host=B\u0026#34;,2,1514810326000 m1,\u0026#34;os=windows,host=B\u0026#34;,3,1514881791000 m1,\u0026#34;os=windows,host=B\u0026#34;,4,1514900597000 Getting TSDB Configuration and Metrics Information Use the CLI's info command to retrieve basic information about a TSDB. The command returns the TSDB's configuration (schema) — which includes the version, storage class, sample retention period, chunk interval, partitioning interval, pre-aggregates, and aggregation granularity for the entire table and for each partition (currently this is the same for all partitions); the partitions' start times (which are also their names); the number of sharding buckets; and the schema of the TSDB's item attributes.\nYou can optionally use the -n|--names flag to also display the names of the metrics contained in the TSDB, and you can use the -m|--performance flag to display a count of the number of metric items in the TSDB (i.e., the number of unique metric-name and labels combinations).\nThe following command returns the full schema and metrics information for the tsdb_example_aggr TSDB:\ntsdbctl info -t tsdb_example_aggr -m -n Querying a TSDB Use the CLI's query command (or its get alias) to query a TSDB and retrieve filtered information about the ingested metric samples. The command requires that you either set the metric string argument to the name of the queried metric (for example, \u0026quot;noise\u0026quot;), or set the -f|--filter flag to a filter-expression string that defines the scope of the query (see Filter Expression). To reference a metric name in the query filter, use the __name__ attribute (for example, \u0026quot;(__name__=='cpu1') OR (__name__=='cpu2')\u0026quot; or \u0026quot;starts(__name__,'cpu')\u0026quot;). To reference labels in the query filter, just use the label name as the attribute name (for example, \u0026quot;os=='linux' AND arch=='amd64'\u0026quot;).\nNote  Currently, only labels of type string are supported; see the Software Specifications and Restrictions. Therefore, ensure that you embed label attribute values in your filter expression within quotation marks even when the values represent a number (for example, \u0026quot;node == '1'\u0026quot;), and don't apply arithmetic operators to such attributes (unless you want to perform a lexicographic string comparison).\n  Queries that set the metric argument use range scan and are therefore faster.\n  In the current release, the query command doesn't support cross-series aggregation (-a|--aggregates with *_all aggregation functions) or the -w|--aggregation-window and --groupBy flags.\n    When using the -f|--filter flag to define a query filter, you don't necessarily need to include a metric name in the query. You can select, for example, to filter the query only by labels. You can also query all metric samples in the query's time range by omitting the metric argument and using a filter expression that always evaluates to true, such as \u0026quot;1==1\u0026quot;; to query the full TSDB content, also set the -b|--begin flag to 0.\nYou can optionally use the -b|--begin and -e|--end flags to specify start (minimum) and end (maximum) times that restrict the query to a specific time range. The start and end times are each specified as a string that contains an RFC 3339 time string, a Unix timestamp in milliseconds, or a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where 'm' = minutes, 'h' = hours, and 'd' = days); the start time can also be set to zero (0) for the earliest sample time in the TSDB.  Alternatively, you can use the -l|--last flag to define the time range as the last \u0026lt;n\u0026gt; minutes, hours, or days (\u0026quot;[0-9]+[mdh]\u0026quot;).  The default end time is the current time (now) and the default start time is one hour earlier than the end time. Therefore, the default time range when neither flag is set is the last hour. Note that the time range applies to the samples' generation times (\u0026quot;the sample times\u0026quot;) and not to the times at which they were ingested into the TSDB.\nBy default, the command returns the query results in plain-text format (\u0026quot;text\u0026quot;), but you can use the -o|--output flag to specify a different format — \u0026quot;csv\u0026quot; (CSV) or \u0026quot;json\u0026quot; (JSON).\nExamples The following query returns all metric samples contained in the tsdb_example TSDB:\ntsdbctl query -t tsdb_example -f \u0026#34;1==1\u0026#34; -b 0 The following queries both return tsdb_example TSDB cpu metric samples that were generated within the last hour and have a host label whose value is 'A' and an os label whose value is \u0026quot;linux\u0026quot;:\ntsdbctl query cpu -t tsdb_example -f \u0026#34;host==\u0026#39;A\u0026#39; AND os==\u0026#39;linux\u0026#39;\u0026#34; -b now-1h tsdbctl query cpu -t tsdb_example -f \u0026#34;host==\u0026#39;A\u0026#39; AND os==\u0026#39;linux\u0026#39;\u0026#34; -l 1h The following query returns, in CSV format, all tsdb_example TSDB metric samples that have a degrees label and were generated in 2021:\ntsdbctl query -t tsdb_example -f \u0026#34;exists(degrees)\u0026#34; -b 2021-01-01T00:00:00Z -e 2021-12-31T23:59:59Z -o csv Aggregation Queries You can use the optional -a|--aggregates flag of the query command to provide a comma-separated list of aggregation functions (\u0026quot;aggregators\u0026quot;) to apply to the raw samples data; for example, \u0026quot;sum,stddev,stdvar\u0026quot;. See Supported Aggregation Functions for details. You can use the -i|--aggregation-interval flag to specify the time interval for applying the specified aggregators. The interval is specified as a string of the format \u0026quot;[0-9]+[mhd]\u0026quot; (where 'm' = minutes, 'h' = hours, and 'd' = days); for example, \u0026quot;3h\u0026quot; (3 hours). The default aggregation interval is the difference between the query's end and start times; for example, for the default query start and end times of now-1h and now, the default aggregation interval will be one hour (1h).\nYou can submit aggregation queries also for a TSDB without pre-aggregates. However, when the TSDB has pre-aggregates that match the query aggregators and the query's aggregation interval is a sufficient multiplier of the TSDB's aggregation granularity, the query processing is sped-up by using the TSDB's pre-aggregates (the aggregation data that's stored in the TSDB's aggregation attributes) instead of performing a new calculation. See also the Aggregation Notes for the create command.  The following query returns for each tsdb_example TSDB metric item whose metric name begins with \u0026quot;cpu\u0026quot;, the minimal and maximal sample values and the standard deviation over two-hour aggregation intervals for samples that were generated in the last two days:\ntsdbctl query -t tsdb_example -f \u0026#34;starts(__name__,\u0026#39;cpu\u0026#39;)\u0026#34; -a \u0026#34;min,max,stddev\u0026#34; -i 2h -l 2d  The following queries return for each m1 metric item in the tsdb_example_aggr TSDB, the daily, hourly, or bi-hourly samples count and data-values average (depending on the aggregation interval) beginning with 1 Jan 2018 at 00:00 until the current time (default). (Note that results are returned only for interval periods that contain samples.) See the Output (-i \u0026lt;interval\u0026gt;) tabs for example command outputs that match the tsdb_example_aggr.csv ingestion example that was used earlier in this tutorial:\n tsdbctl query m1 -t tsdb_example_aggr -a \u0026#34;count,avg\u0026#34; -i 1d -b 2018-01-01T00:00:00Z  Name: m1 Labels: host=B,os=windows,Aggregate=count 2018-01-01T00:00:00Z v=2.00 2018-01-02T00:00:00Z v=2.00 Name: m1 Labels: host=B,os=windows,Aggregate=avg 2018-01-01T00:00:00Z v=1.50 2018-01-02T00:00:00Z v=3.50 Name: m1 Labels: host=A,os=linux,Aggregate=count 2018-01-01T00:00:00Z v=4.00 2018-01-02T00:00:00Z v=2.00 Name: m1 Labels: host=A,os=linux,Aggregate=avg 2018-01-01T00:00:00Z v=2.50 2018-01-02T00:00:00Z v=5.50 Name: m1 Labels: host=A,os=darwin,Aggregate=count 2018-01-01T00:00:00Z v=2.00 2018-01-02T00:00:00Z v=1.00 Name: m1 Labels: host=A,os=darwin,Aggregate=avg 2018-01-01T00:00:00Z v=1.50 2018-01-02T00:00:00Z v=3.00 Name: m1 Labels: host=A,os=windows,Aggregate=count 2018-01-01T00:00:00Z v=3.00 2018-01-02T00:00:00Z v=1.00 Name: m1 Labels: host=A,os=windows,Aggregate=avg 2018-01-01T00:00:00Z v=2.00 2018-01-02T00:00:00Z v=4.00 Name: m1 Labels: host=B,os=linux,Aggregate=count 2018-01-01T00:00:00Z v=2.00 2018-01-02T00:00:00Z v=2.00 Name: m1 Labels: host=B,os=linux,Aggregate=avg 2018-01-01T00:00:00Z v=1.50 2018-01-02T00:00:00Z v=3.50 Name: m1 Labels: host=B,os=darwin,Aggregate=count 2018-01-01T00:00:00Z v=2.00 2018-01-02T00:00:00Z v=1.00 Name: m1 Labels: host=B,os=darwin,Aggregate=avg 2018-01-01T00:00:00Z v=1.50 2018-01-02T00:00:00Z v=3.00  Name: m1 Labels: host=B,os=windows,Aggregate=count 2018-01-01T08:00:00Z v=1.00 2018-01-01T12:00:00Z v=1.00 2018-01-02T08:00:00Z v=1.00 2018-01-02T12:00:00Z v=1.00 Name: m1 Labels: host=B,os=windows,Aggregate=avg 2018-01-01T08:00:00Z v=1.00 2018-01-01T12:00:00Z v=2.00 2018-01-02T08:00:00Z v=3.00 2018-01-02T12:00:00Z v=4.00 Name: m1 Labels: host=A,os=linux,Aggregate=count 2018-01-01T08:00:00Z v=2.00 2018-01-01T10:00:00Z v=1.00 2018-01-01T14:00:00Z v=1.00 2018-01-02T12:00:00Z v=2.00 Name: m1 Labels: host=A,os=linux,Aggregate=avg 2018-01-01T08:00:00Z v=1.50 2018-01-01T10:00:00Z v=3.00 2018-01-01T14:00:00Z v=4.00 2018-01-02T12:00:00Z v=5.50 Name: m1 Labels: host=A,os=darwin,Aggregate=count 2018-01-01T10:00:00Z v=1.00 2018-01-01T12:00:00Z v=1.00 2018-01-02T06:00:00Z v=1.00 Name: m1 Labels: host=A,os=darwin,Aggregate=avg 2018-01-01T10:00:00Z v=1.00 2018-01-01T12:00:00Z v=2.00 2018-01-02T06:00:00Z v=3.00 Name: m1 Labels: host=A,os=windows,Aggregate=count 2018-01-01T10:00:00Z v=1.00 2018-01-01T12:00:00Z v=2.00 2018-01-02T08:00:00Z v=1.00 Name: m1 Labels: host=A,os=windows,Aggregate=avg 2018-01-01T10:00:00Z v=1.00 2018-01-01T12:00:00Z v=2.50 2018-01-02T08:00:00Z v=4.00 Name: m1 Labels: host=B,os=linux,Aggregate=count 2018-01-01T08:00:00Z v=1.00 2018-01-01T14:00:00Z v=1.00 2018-01-02T12:00:00Z v=2.00 Name: m1 Labels: host=B,os=linux,Aggregate=avg 2018-01-01T08:00:00Z v=1.00 2018-01-01T14:00:00Z v=2.00 2018-01-02T12:00:00Z v=3.50 Name: m1 Labels: host=B,os=darwin,Aggregate=count 2018-01-01T10:00:00Z v=1.00 2018-01-01T14:00:00Z v=1.00 2018-01-02T10:00:00Z v=1.00 Name: m1 Labels: host=B,os=darwin,Aggregate=avg 2018-01-01T10:00:00Z v=1.00 2018-01-01T14:00:00Z v=2.00 2018-01-02T10:00:00Z v=3.00  Name: m1 Labels: host=B,os=windows,Aggregate=count 2018-01-01T09:00:00Z v=1.00 2018-01-01T12:00:00Z v=1.00 2018-01-02T08:00:00Z v=1.00 2018-01-02T13:00:00Z v=1.00 Name: m1 Labels: host=B,os=windows,Aggregate=avg 2018-01-01T09:00:00Z v=1.00 2018-01-01T12:00:00Z v=2.00 2018-01-02T08:00:00Z v=3.00 2018-01-02T13:00:00Z v=4.00 Name: m1 Labels: host=A,os=linux,Aggregate=count 2018-01-01T09:00:00Z v=2.00 2018-01-01T11:00:00Z v=1.00 2018-01-01T14:00:00Z v=1.00 2018-01-02T12:00:00Z v=2.00 Name: m1 Labels: host=A,os=linux,Aggregate=avg 2018-01-01T09:00:00Z v=1.50 2018-01-01T11:00:00Z v=3.00 2018-01-01T14:00:00Z v=4.00 2018-01-02T12:00:00Z v=5.50 Name: m1 Labels: host=A,os=darwin,Aggregate=count 2018-01-01T10:00:00Z v=1.00 2018-01-01T13:00:00Z v=1.00 2018-01-02T07:00:00Z v=1.00 Name: m1 Labels: host=A,os=darwin,Aggregate=avg 2018-01-01T10:00:00Z v=1.00 2018-01-01T13:00:00Z v=2.00 2018-01-02T07:00:00Z v=3.00 Name: m1 Labels: host=A,os=windows,Aggregate=count 2018-01-01T10:00:00Z v=1.00 2018-01-01T12:00:00Z v=1.00 2018-01-01T13:00:00Z v=1.00 2018-01-02T08:00:00Z v=1.00 Name: m1 Labels: host=A,os=windows,Aggregate=avg 2018-01-01T10:00:00Z v=1.00 2018-01-01T12:00:00Z v=2.00 2018-01-01T13:00:00Z v=3.00 2018-01-02T08:00:00Z v=4.00 Name: m1 Labels: host=B,os=linux,Aggregate=count 2018-01-01T09:00:00Z v=1.00 2018-01-01T14:00:00Z v=1.00 2018-01-02T12:00:00Z v=1.00 2018-01-02T13:00:00Z v=1.00 Name: m1 Labels: host=B,os=linux,Aggregate=avg 2018-01-01T09:00:00Z v=1.00 2018-01-01T14:00:00Z v=2.00 2018-01-02T12:00:00Z v=3.00 2018-01-02T13:00:00Z v=4.00 Name: m1 Labels: host=B,os=darwin,Aggregate=count 2018-01-01T10:00:00Z v=1.00 2018-01-01T14:00:00Z v=1.00 2018-01-02T11:00:00Z v=1.00 Name: m1 Labels: host=B,os=darwin,Aggregate=avg 2018-01-01T10:00:00Z v=1.00 2018-01-01T14:00:00Z v=2.00 2018-01-02T11:00:00Z v=3.00     As explained above, you can also submit aggregation queries for TSDBs without pre-aggregates. In such cases, the aggregations are calculated when the query is processed. For example, the following query returns a three-day average for the tsdb_example TSDB's temperature metric samples:\ntsdbctl query temperature -t tsdb_example -a avg -i 3d -b 0 Deleting a TSDB Use the CLI's delete command (or its del alias) to delete a TSDB or delete content from a TSDB.\nUse -a|--all flag to delete the entire TSDB — i.e., delete the TSDB table, including its schema (which contains the configuration information) and all its content.\n You can optionally use the -b|--begin and -e|--end flag to define a sample-times range for the delete operation. As with the query command, the start and end times are each specified as a string that contains an RFC 3339 time string, a Unix timestamp in milliseconds, or a relative time of the format \u0026quot;now\u0026quot; or \u0026quot;now-[0-9]+[mhd]\u0026quot; (where 'm' = minutes, 'h' = hours, and 'd' = days); the start time can also be set to zero (0) for the earliest sample time in the TSDB. The default end (maximum) time is the current time (now) and the default start (minimum) time is one hour earlier than the end time.\nTo avoid inadvertent deletes, by default the command prompts you to confirm the delete operation. You can use the --force flag to perform a forceful deletion without prompting for confirmation.\nYou can also use the -i|--ignore-errors flag to skip errors that might occur during the delete operation and attempt to proceed to the next step. Note  Examples The following command completely deletes the tsdb_example_aggr TSDB (subject to user confirmation in the command line):\ntsdbctl delete -t tsdb_example_aggr -a You can add the --force flag to enforce the delete operation and bypass the confirmation prompt:\ntsdbctl delete -t tsdb_example_aggr -a --force The following command deletes all tsdb_example TSDB partitions (and contained metric items) between the earliest sample time in the TSDB and Unix time 1569887999000 (2019-09-30T23:59:59Z):\ntsdbctl delete -t tsdb_example -b 0 -e 1569887999000 See Also  Time-Series Databases (TSDB) TSDB sofware specifications and restrictions Frames TSDB-Backend API Reference  ","keywords":["tsdb","cli,","v3io","tsdb","cli,","tsdb,","time","series,","cli,","quereies,","metrics,","data","samples,","aggregatrors,","aggregator,","aggregates,","aggregate","functions,","avg,","count,","last,","min,","max,","rate,","stddev,","stdvar"],"path":"https://github.com/jasonnIguazio/data-layer/tsdb/tsdb-cli/","title":"The V3IO TSDB CLI (tsdbctl)"},{"content":" Iguazio V3IO Frames is multi-model open-source data-access library that provides a unified high-performance Python DataFrame API for working with NoSQL, stream, and time-series (TSDB) data in the platform's data store. For more information and detailed usage instructions, see the Frames API reference. As indicated in this reference, you can find many examples of using the Frames API in the platform's tutorial Jupyter notebooks; see specifically the frames.ipynb notebook. See also the Frames restrictions in the Software Specifications and Restrictions documentation.\nFrames is provided as a default (pre-deployed) shared single-instance tenant-wide platform service (framesd).\nSee Also  Working with Services Frames API Reference Python Machine-Learning and Scientific-Computation Packages The Spark Service Dask Working with NoSQL Data Time-Series Databases (TSDB) Frames software specifications and restrictions  ","keywords":["v3io","frames,","frames,","structured","dataframes,","dataframes,","nosql,","key-value,","kv,","time-series","databases,","time","series,","tsdb,","python","libraries,","python","packages,","python","apis,","python,","jupyter,","jupyter","notebook,","jupyter","tutorials,","v3io","tutorials,","tutorials,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/frames/","title":"The V3IO Frames Service"},{"content":" The platform includes a web-based command-line shell (\u0026quot;web shell\u0026quot;) service for running application services and performing basic file-system operations from a web browser. For example, you can use the Presto CLI to run SQL queries on your data; use the TSDB CLI to work with TSDBs; use spark-submit to run Spark jobs; run local and Hadoop file-system commands; or use kubectl CLI to run commands against the platform's application clusters.\nThe custom web-shell service parameters allow you to optionally associate the service with a Spark service and select a Kubernetes service account that will determine the permissions for using the kubectl CLI from the web shell. Following is a list of the kubectl operations that can be performed with each service account:\n \u0026quot;None\u0026quot; — no permission to use kubectl. \u0026quot;Log Reader\u0026quot; — list pods and view logs. \u0026quot;Application Admin\u0026quot; — list pods; view logs; and create or delete secrets and ConfigMaps. \u0026quot;Service Admin\u0026quot; — list pods; view logs and resource metrics; create or delete secrets and ConfigMaps; create, delete, list, or get jobs and cron jobs; and view, edit, list, or delete Persistent Volume Claims (PVCs).  Note The web shell isn't a fully functional Linux shell. See Software Specifications and Restrictions for specific restrictions. To log out of the web shell, run the exit command in the shell.    See Also  Working with Services The Jupyter Notebook Service The Spark Service The Presto Service The Hadoop Service web-shell software specifications and restrictions  ","keywords":["web","shell,","web-based","command-line","shell,","web-based","shell,","command-line","shell,","command","line,","shell,","file","system,","hadoops,","hdfs,","spark,","presto,","presto","cli,","sql","queries,","sql"],"path":"https://github.com/jasonnIguazio/services/app-services/web-shell/","title":"The Web-Shell Service"},{"content":" Learn how to work with time-series databases (TSDB) in the MLOps Platform.\n","keywords":["time-series","databases,","tsdb,","time-series","data,","time","series,","tsdb","apis,","time-series","apis,","tsdb","cli"],"path":"https://github.com/jasonnIguazio/data-layer/tsdb/","title":"Time-Series Databases (TSDB)"},{"content":"Overview The platform installation is done using two sequential deployments. The first deployment creates a virtual network (optional), an empty security group, and a small installer virtual machine (\u0026quot;the installer VM\u0026quot;). After the installer VM initializes, the platform installer (\u0026quot;Provazio\u0026quot;) is executed by running a script that launches a Docker image (quay.io/iguazio/provazio-dashboard:az-market-0) that contains the installer. The script generates a system configuration and posts it to Provazio. Provazio then starts the installation process according to the system configuration, and creates a second deployment that's responsible for bringing up the infrastructure.\nWhen something goes wrong during the installation, relevant information can be found in the logs of the installation Docker container. By default, these failure logs are automatically sent to Iguazio for analysis. However, on rare occasions you might be asked to gather error information and provide it to Iguazio's support team. The following sections provide instructions for gathering the required information for such occasions.\nGetting Information from the Azure Portal  Locating your installation resource group Checking the deployment status and getting error logs  Locating Your Installation Resource Group To get deployment information from the Azure Portal, you first need to locate the resource group that you used for the platform installation: in a web browser, browse to the URL of your Azure Portal and select Resource Groups.    Under Resource groups, find and select the resource group that you used for the platform installation. Then, gather relevant information for this group, as outlined in the following sections.\nChecking the Deployment Status and Getting Error Logs To check the installer VM's deployment status and retrieve error logs, in the Azure portal, find and select your installation resource group and select Settings | Deployments from the resource-group menu.    Check the deployment status. If the status is \u0026quot;Failed\u0026quot;, select the Error details link, copy and save the error information, and send it to Iguazio's support team.\nGathering Logs from the Platform Installer (Provazio) To gather logs from the platform installer (Provazio), you need to create a shell connection to the installer VM, and then run shell commands to gather installer logs.\nConnecting to the Installer VM To access the installer VM and gather logs, you need to create a network connection to the installer VM from a command-line shell, using either of the following alternative methods:\n Create an SSH session Create an Azure serial console  TipWhenever possible, prefer an SSH connection. While it might be easier to create a serial console than an SSH session, because of some Azure Serial Console limitations it's more difficult to extract information when using a serial console compared to an SSH session.  Using SSH to Connect to the Installer VM To establish an SSH connection to the installer VM, you first need to get the VM's private IP address:\n  In the Azure portal, find and select your installation resource group.\n  Select Overview from the resource-group menu. In the displayed resources list, look for a virtual machine whose name ends with \u0026quot;installer-vm\u0026quot; and select it to drill down.\n     Copy the installer VM's private IP address from the IP addresses that are shown in the top-right corner of the VM resources information:\n     NoteDepending on your network configuration, you might have to create a security-group rule to allow the SSH connection to the VM. For more information, see Network Security Groups Configuration (Azure).  When you have the VM's private IP address, use SSH to connect to this address with the login credentials that you received from Iguazio.\nUsing Azure Serial Console to Connect to the Installer VM You can use the Serial Console in the Azure portal to connect to the installer VM from a text-based serial console. Follow the instructions in the Azure Serial Console overview to create a serial console. Select the virtual machine whose name ends with \u0026quot;installer-vm\u0026quot;. When prompted for a storage account, you can use any temporary Azure storage account.\nGathering Logs from the Installer VM After you connect to the installer VM, perform the following steps from the VM command-line shell to gather installer logs:\n  List the Docker containers on the VM:\nsudo docker ps -a The command output should show a \u0026quot;provazio-dashboard\u0026quot; container.\n  Get the logs of the \u0026quot;provazio-dashboard\u0026quot; container:\nsudo docker logs provazio-dashboard Copy and save the logs from the command output.\n  Get installer logs, which are stored in a /tmp/install.log file that's created by an installer-VM script:\ncat /tmp/install.log Copy and save the logs from the command output.\n  When you're done, send the container and installer-VM logs to Iguazio's support team.\nSee Also  Azure cloud installation guide Network Security Groups Configuration (Azure)  ","keywords":["troubleshooting","azure","installation,","azure","installtion","troubleshooting,","azure","troubleshooting,","azure,","troubleshooting"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/cloud/azure/howto/troubleshoot-install/","title":"Troubleshooting Azure Cloud Installations"},{"content":" Time-series databases (TSDBs) are used for storing time-series data — a series of time-based data points. The platform features enhanced built-in support for working with TSDBs, which includes a rich set of features for efficiently analyzing and storing time series data. The platform uses the Iguazio V3IO TSDB open-source library, which exposes a high-performance API for working with TSDBs — including creating and deleting TSDB instances (tables) and ingesting and consuming (querying) TSDB data. This API can be consumed in various ways:\n Use the V3IO TSDB command-line interface (CLI) tool (tsdbctl), which is pre-deployed in the platform, to easily create, delete, and manage TSDB instances (tables) in the platform's data store, ingest metrics into such tables, and issue TSDB queries. The CLI can be run locally on a platform cluster — from a command-line shell interface, such as the web-based shell or a Jupyter terminal or notebook — or remotely from any computer with a network connection to the cluster. The platform's web shell and Jupyter terminal environments predefine a tsdbctl alias to the native CLI that preconfigures the URL of the web-APIs service and the authentication access key for the running user of the parent shell or Jupyter Notebook service. Use the Prometheus service service to run TSDB queries. Prometheus is an open-source systems monitoring and alerting toolkit that features a dimensional data model and a flexible query language. The platform's Prometheus service uses the pre-deployed Iguazio V3IO Prometheus distribution, which packages Prometheus with the V3IO TSDB library for a robust, scalable, and high-performance TSDB solution.  For more information and examples, see Time-Series Databases (TSDB) (including a TSDB CLI guide), as well as the TSDB section in the frames.ipynb tutorial Jupyter notebook. See Also  Working with Services Frames TSDB-Backend API Reference Time-Series Databases (TSDB)  Working with Time-Series Databases (TSDBs) The TSDB CLI   TSDB software specifications and restrictions  ","keywords":["time-series","database","services,","tsdb","services,","v3io","tsdb","library,","v3io","tsdb,","time","series","databases,","time-series,","tsdb,","v3io","tsdb","cli,","tsdb","cli,","tsdbctl,","v3io","prometheus,","prometheus,","v3io","frames,","frames,","v3io","tsdb","nuclio","functions,","tsdb","nuclio","functions,","nuclio,","jupyter,","jupyter","notebook,","jupyter","terminals,","web","shell,","shell,","open","source"],"path":"https://github.com/jasonnIguazio/services/app-services/tsdb/","title":"Time-Series Database (TSDB) Services"},{"content":" Overview An update expression is an expression that is used to update the attributes of an item. This includes adding attributes and initializing their values, changing the value of existing attributes, or removing (deleting) item attributes. For example, the NoSQL Web API UpdateItem operation receives an update-expression request parameter (UpdateExpression), and optionally also an alternate update-expression parameter for an if-then-else update (AlternateUpdateExpression), that define which item attributes to update and how to update them.\nArrays SupportIn the current release, the support for array attributes and the use of array operators and functions in expressions is restricted to the web APIs.  Syntax KEYWORD ACTION[; KEYWORD ACTION; ...] An update expression is composed of one or more update-action expressions, each made up of an action keyword (KEYWORD) followed by a matching action expression (ACTION). (In some cases, the keyword can be omitted, as outlined in the documentation.) The action expressions are separated by semicolons (;). Spaces around the semicolons are ignored. The last update-action expression can optionally be terminated with a semicolon as well.\nNoteExpression keywords are reserved names in the platform. For more information, see Reserved Names.  The following update expression types are supported:\n SET expression — sets the value of an item attribute (including creation of the attribute if it doesn't already exist) or of an element in an array attribute.  REMOVE expression — removes (deletes) an item attribute.  SET Expression SET ELEMENT = VALUE A SET expression updates the value of an attribute or an element in an array attribute. When updating an attribute that isn't already found in the updated item, the platform creates the attribute and initializes it to the specified value. A SET expression can optionally begin with the SET keyword, which is the default keyword for assignment expressions. The expression contains an assignment expression that consists of an assignment operator (=) whose operands are the updated element (ELEMENT) — an attribute (ATTRIBUTE), a slice of an array attribute (ATTRIBUTE[istart..iend]), or an element of an array attribute (ATTRIBUTE[i]) — and the value to assign (VALUE). VALUE can also be an expression that evaluates to the value to assign — for example, \u0026quot;SET myattr = 4 + 5;\u0026quot;. NoteIn the current release, you cannot directly assign Base64 encoded strings to blob attributes in expressions. However, you can define and update blob array attributes in expressions — see Array-Attribute Expression Variables.  REMOVE Expression REMOVE ATTRIBUTE A REMOVE expression removes an attribute from an item (i.e., deletes the attribute).\nThe expression begins with the REMOVE keyword followed by the name of the attribute to remove (ATTRIBUTE).\nExamples   Increment the current value of an existing miles attribute by 1000:\n\u0026#34;SET miles = miles + 1000\u0026#34;   Remove an item's miles attribute:\n\u0026#34;REMOVE miles\u0026#34;   Update the values of two grocery-department income attributes (produce and dairy), and set the value of a sum attribute to the sum of the updated department-income attribute values:\n\u0026#34;produce=20000; dairy=15000; sum = produce + dairy\u0026#34;   Set the value of a rainbow string attribute:\n\u0026#34;SET rainbow=\u0026#39;red, orange, yellow, green, blue, indigo, violet\u0026#39;\u0026#34;   Initialize multiple attributes of different types in a person item:\n\u0026#34;SET name=\u0026#39;Alexander Bell\u0026#39;; country=\u0026#39;Scotland\u0026#39;; isMarried=true; numChildren=4\u0026#34;   Add and initialize a ctr counter attribute to 1 if it doesn't already exist; if the attribute exists, increment its value by 1 (see if_not_exists):\n\u0026#34;SET ctr = if_not_exists(ctr,0) + 1;\u0026#34;   Add new attributes to an item and initialize their values; if the attributes already exist, they will be reassigned their current values (see if_not_exists):\n\u0026#34;SET color = if_not_exists(color, \u0026#39;red\u0026#39;); SET updated = if_not_exists(updated, true)\u0026#34;   Switch the values of two attributes (a and b) using a temporary temp attribute, and then delete the temporary attribute. Attributes a and b must already be defined for the item:\n\u0026#34;temp=b; b=a; a=temp; REMOVE temp\u0026#34;    Create a new counters integer array attribute with five elements, all initialized by default to zero:\n\u0026#34;SET counters = init_array(5, \u0026#39;int\u0026#39;)\u0026#34;   Create a new smallArray array attribute from the first five elements of an existing myArray array attribute:\n\u0026#34;smallArray = myArray[0..4]\u0026#34;   Update the values of the first four elements of an arr array attribute. The fourth attribute (at index 3) is assigned the result of an arithmetic expression that uses the updated values of the first three array elements:\n\u0026#34;arr[2]=-1; arr[0]=6; arr[1]=7; arr[3]=arr[0]+arr[2]*arr[1]\u0026#34;   Define an arrSmallFibonacci integer array attribute of five elements that implements a small Fibonacci series in which each element (beginning with the third element) is the sum of the previous two elements; (the value of the first element, at index 0, is 0 — init_array default):\n\u0026#34;arrSmallFibonacci = init_array(5, \u0026#39;int\u0026#39;); arrSmallFibonacci[1]=1; arrSmallFibonacci[2] = arrSmallFibonacci[0] + arrSmallFibonacci[1]; arrSmallFibonacci[3] = arrSmallFibonacci[1] + arrSmallFibonacci[2]; arrSmallFibonacci[4] = arrSmallFibonacci[2] + arrSmallFibonacci[3];\u0026#34;    Define an arrFibonacci integer array attribute of 100 elements and an i loop-iterator number attribute, and use the attributes to implement a Fibonacci series in which each element (beginning with the third element) is the sum of the previous two elements.\n  Define the attributes and initialize the values of i and of the first three elements of the arrFibonacci array; (the initialization to 0 of the first element, at index 0, could have been omitted because it's the default  value):\n\u0026#34;i=2; arrFibonacci = init_array(100,\u0026#39;int\u0026#39;); arrFibonacci[0]=0; arrFibonacci[1]=1; arrFibonacci[2] = arrFibonacci[0] + arrFibonacci[1]\u0026#34;   Repeatedly update the item using the following update expression to sequentially update the series elements in the array (starting from the fourth element at index 3):\n\u0026#34;i=i+1; arrFibonacci[i] = arrFibonacci[i-1] + arrFibonacci[i-2];\u0026#34; You can use a condition expression to set the maximum number of array elements to update. For example, the following expression limits the update to 55 elements:\n\u0026#34;i\u0026lt;55\u0026#34;   At the end, you can optionally use a REMOVE update expression to delete the i attribute:\n\u0026#34;REMOVE i\u0026#34;     See Also  Condition Expression Attribute Variables Assignment Operator (=) Array Operator ([ ]) Array Functions Reserved Names  ","keywords":["update","expressions,","update","items,","add","items,","delete","items,","remove","items,","UpdateItem,","UpdateExpression,","AlternateUpdateExpression,","nosql,","nosql","web","api,","nosql,","table","items,","attributes,","add","attributes,","delete","attributes,","remove","attributes,","counter","attributes,","increment","attributes,","expression","types,","SET","expressions,","REMOVE","expressions,","ACTION,","KEYWORD,","SET","keyword,","REMOVE","keyword,","if_not_exists,","if-then,","if-then-else,","techpreview,","tech","preview,","arrays,","array","attributes,","array","slices,","blob","attributes,","blob,","array","operator,","[]","operator,","Fibonacci"],"path":"https://github.com/jasonnIguazio/data-layer/reference/expressions/update-expression/","title":"Update Expression"},{"content":"Description Updates the attributes of a table item. If the specified item or table don't exist, the operation creates them.\nRequest Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: UpdateItem \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;UpdateItem\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the item to update. The path includes the table path and the item's name (primary key). You can optionally set the item name in the request's Key JSON parameter instead of in the URL; you can also optionally set the relative table path within the container, or part of the path, in the request's TableName JSON parameter.\nRequest Data Syntax  { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;AlternateUpdateExpression\u0026#34;: \u0026#34;string\u0026#34; }  payload = { \u0026#34;TableName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;string\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;N\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;AlternateUpdateExpression\u0026#34;: \u0026#34;string\u0026#34; }    Parameters  TableName The table (collection) that contains the item — a relative table path within the configured data container or the end of such a path, depending on the resource configuration in the request URL. See Data-Service Web-API General Structure.  If the table doesn't exist, the operation will create it.\n Type: String   Requirement: Required if not set in the request URL   Key A primary-key attribute whose value is the item's primary-key value, which uniquely identifies the item within the table. The primary-key value is also the item's name and is stored by the platform in the __name system attribute. When defining the key for the UpdateItem request by setting the Key JSON request parameter (instead of setting the key in the URL), the platform also automatically defines a primary-key user attribute of the specified name, type, and value (the primary-key value). See also Item Name and Primary Key.\nNote  To support range scans, use a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary-key value. For more information, see Working with NoSQL Data. See the primary-key guidelines in the Best Practices for Defining Primary Keys and Distributing Data Workloads guide.     Type: Attribute object   Requirement: Required if the item's name (primary key) is not set in the URL   UpdateExpression An update expression that specifies the changes to make to the item's attributes. See Update Expression for syntax details and examples. You can optionally use the ConditionExpression request parameter to define a condition for executing the update expression. You can also use the AlternateUpdateExpression parameter to define an alternate update expression to execute if the condition expression evaluates to false. When the update expression is executed, it's evaluated against the table item to be updated, if it exists. If the item doesn't exist, the update creates it (as well as the parent table if it doesn't exist).\nNote  To access the table with Spark DataFrames or Presto, the item must have a sharding-key user attribute, and in the case of a compound primary-key also a sorting-key user attribute; (for more efficient range scans, use a sorting-key attribute of type string). To access the table with V3IO Frames, the item must have a primary-key user attribute. The NoSQL Web API doesn't attach any special significance to such key user attributes, but to work with a structured-data API you must define the required attributes, set their values according to the value of the item's primary key (name) — which is composed of a sharding key and optionally also a sorting key — and refrain from modifying the values of these attributes. See also Sharding and Sorting Keys.\n  If the update or alternate-update expression is an empty string, the item will be created if it doesn't already exist, but no user attributes will be added or modified (other than adding a primary-key attribute for a new item if the Key parameter is set in the request body), and the request will complete successfully (HTTP status code 200).\n     Type: String   Requirement: Required   AlternateUpdateExpression An alternate update expression that specifies the changes to make to the item's attributes when a condition expression, defined in the ConditionExpression request parameter, evaluates to false; (i.e., this parameter defines the else clause of a conditional if-then-else update expression). See Update Expression for syntax details and examples. When the alternate update expression is executed, it's evaluated against the table item to be updated, if it exists. If the item doesn't exist, the update creates it (as well as the parent table if it doesn't exist). See also the UpdateExpression notes, which apply to the alternate update expression as well.\n Type: String   Requirement: Optional   ConditionExpression A Boolean condition expression that defines a conditional logic for executing the update operation. See Condition Expression for syntax details and examples. The condition expression is evaluated against the table item to be updated, if it exists. Note:\n  If the condition expression evaluates to true — including in the case of an empty condition-expression string — the update expression that's assigned to the UpdateExpression request parameter is executed.\n  If the condition expression evaluates to false — including when the expression references a non-existing item attribute — the alternate update expression that's assigned to the AlternateUpdateExpression parameter is executed; if no alternate update expression is provided, the operation completes successfully without an update.\n  NoteIn v3.2.1, when ConditionExpression evaluates to false, UpdateItem returns a 400 error even though the operation is considered successful.   Type: String   Requirement: Optional    Response Response DataNone\nExamples Example 1 — a Basic Update Expression Request For a \u0026quot;MySupermarket\u0026quot; table in the \u0026quot;mycontainer\u0026quot; container, set the value of the manager attribute of the item named \u0026quot;beverages\u0026quot; to \u0026quot;Jackson S.\u0026quot;; if the attribute exists, overwrite its current value, and if it doesn't exist create it. Add an expenses attribute with the value 0 only if the item doesn't already have such an attribute; (if the attribute exists, it will be reassigned its current value). If the item or table don't exist, create them and assign the item the specified update-attribute values.\n POST /mycontainer/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: UpdateItem X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;TableName\u0026#34;: \u0026#34;MySupermarket\u0026#34;, \u0026#34;Key\u0026#34;: {\u0026#34;department\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;beverages\u0026#34;}}, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;SET manager=\u0026#39;Jackson S.\u0026#39;; SET expenses = if_not_exists(expenses,0);\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;UpdateItem\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;TableName\u0026#34;: \u0026#34;MySupermarket\u0026#34;, \u0026#34;Key\u0026#34;: {\u0026#34;department\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;beverages\u0026#34;}}, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;SET manager=\u0026#39;Jackson S.\u0026#39;; SET expenses = if_not_exists(expenses,0);\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json Example 2 — an If-Then Update Expression For a \u0026quot;Cars\u0026quot; table in the \u0026quot;mycontainer\u0026quot; container, set the value of the Alarm attribute of the item named 321234 to \u0026quot;ON\u0026quot; if the item has a State attribute whose value is either \u0026quot;Broken\u0026quot; or \u0026quot;Attention\u0026quot;. If the item doesn't have an Alarm attribute, create the attribute, provided the state condition is fulfilled. If the item or table don't exist, create them and assign the item the specified alarm attribute value.\nRequest  POST /mycontainer/Fleet/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: UpdateItem X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;TableName\u0026#34;: \u0026#34;Cars\u0026#34;, \u0026#34;Key\u0026#34;: {\u0026#34;carReg\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;321234\u0026#34;}}, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;State IN (\u0026#39;Broken\u0026#39;, \u0026#39;Attention\u0026#39;)\u0026#34;, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;SET Alarm=\u0026#39;ON\u0026#39;\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/Fleet/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;UpdateItem\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;TableName\u0026#34;: \u0026#34;Cars\u0026#34;, \u0026#34;Key\u0026#34;: {\u0026#34;carReg\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;321234\u0026#34;}}, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;State IN (\u0026#39;Broken\u0026#39;, \u0026#39;Attention\u0026#39;)\u0026#34;, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;SET Alarm=\u0026#39;ON\u0026#39;\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json Example 3 — an If-Then-Else Update Expression For a \u0026quot;site1\u0026quot; table in a data directory in the \u0026quot;mycontainer\u0026quot; container, update attributes of the item named \u0026quot;13\u0026quot;; (the item path is set as part of the URL in the request header): if the value of the item's is_init attribute is true, increase the current values of its a, b, c, and d attributes by 1. Otherwise (i.e., if is_init is false or isn't defined, or if the item doesn't exist), initialize attributes a, b, c, and d to 0, 10, 0, and 120 (respectively) and set the value of the is_init attribute to true; if the item or table don't exist, create them.\nRequest  POST /mycontainer/data/site1/13 HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: UpdateItem X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;ConditionExpression\u0026#34;: \u0026#34;is_init==true\u0026#34;, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;a=a+1; b=b+1; c=c+1; d=d+1;\u0026#34;, \u0026#34;AlternateUpdateExpression\u0026#34;: \u0026#34;a=0; b=10; c=0; d=120; is_init=true;\u0026#34; }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/data/site1/13\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;UpdateItem\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = { \u0026#34;ConditionExpression\u0026#34;: \u0026#34;is_init==true\u0026#34;, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;a=a+1; b=b+1; c=c+1; d=d+1;\u0026#34;, \u0026#34;AlternateUpdateExpression\u0026#34;: \u0026#34;a=0; b=10; c=0; d=120; is_init=true;\u0026#34; } response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ","keywords":["UpdateItem,","update","item,","update","items,","nosql,","tables,","table","items,","attributes,","change","attributes,","edit","attributes,","edit","items,","condition","expressions,","update","expressions,","if-then,","if-then-else,","item","names,","object","names,","primary","key,","sharding","key,","sorting","key,","range","scan,","AlternateUpdateExpression,","ConditionExpression,","Key,","TableName,","UpdateExpression"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/nosql-web-api/updateitem/","title":"UpdateItem"},{"content":"Description Updates a stream's configuration by increasing its shard count. The changes are applied immediately.\nSpark-Streaming NoteIn v3.2.1, to use the Spark Streaming API to consume records from new shards after a shard-count increase, you must first restart the consumer application.  Request Request Header Syntax  POST /\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt; HTTP/1.1 Host: \u0026lt;web-APIs URL\u0026gt; Content-Type: application/json X-v3io-function: UpdateStream \u0026lt;Authorization OR X-v3io-session-key\u0026gt;: \u0026lt;value\u0026gt;  url = \u0026#34;http://\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container\u0026gt;/\u0026lt;resource\u0026gt;\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;UpdateStream\u0026#34;, \u0026#34;\u0026lt;Authorization OR X-v3io-session-key\u0026gt;\u0026#34;: \u0026#34;\u0026lt;value\u0026gt;\u0026#34; }    URL Resource Parameters The path to the stream to be updated. You can optionally set the stream name in the request's StreamName JSON parameter instead of in the URL.\nRequest Data Syntax  { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardCount\u0026#34;: number, }  payload = { \u0026#34;StreamName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;ShardCount\u0026#34;: number }    Parameters  StreamName The name of the stream to be updated.\n Type: String   Requirement: Required if not set in the request URL   ShardCount The stream's new shard count (total number of shards in the stream). The new shard count cannot be smaller than the current shard count.\nNoteIf you increase a stream's shard count after its creation, new records with a previously used partition key might be assigned to a new shard. See Stream Sharding and Partitioning, and PutRecords.   Type: Number   Requirement: Required    Response Response DataNone\nErrors In the event of an error, the response includes a JSON object with an ErrorCode element that contains a unique numeric error code, and an ErrorMessage element that contains one of the following API error messages: Error Message Description   InvalidArgumentException A provided request parameter is not valid for this request.    InvalidShardCountException The provided shard count is not larger than the current number of shards within the stream.    ResourceNotFoundException The specified resource does not exist.   ResourceNotStreamException The specified resource is not a stream.   Permission denied The sender of the request does not have the required permissions to perform the operation.    Examples Update the shard count of a MyStream stream to 200:\nRequest  POST /mycontainer/MyStream/ HTTP/1.1 Host: https://default-tenant.app.mycluster.iguazio.com:8443 Content-Type: application/json X-v3io-function: UpdateStream X-v3io-session-key: e8bd4ca2-537b-4175-bf01-8c74963e90bf { \u0026#34;ShardCount\u0026#34;: 200 }  import requests url = \u0026#34;https://default-tenant.app.mycluster.iguazio.com:8443/mycontainer/MyStream/\u0026#34; headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-v3io-function\u0026#34;: \u0026#34;UpdateStream\u0026#34;, \u0026#34;X-v3io-session-key\u0026#34;: \u0026#34;e8bd4ca2-537b-4175-bf01-8c74963e90bf\u0026#34; } payload = {\u0026#34;ShardCount\u0026#34;: 200} response = requests.post(url, json=payload, headers=headers) print(response.text)    Response HTTP/1.1 200 OK Content-Type: application/json ","keywords":["UpdateStream,","update","stream,","streaming,","stream","configuration,","stream","shards,","shards,","shard","count,","ShardCount"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/streaming-web-api/updatestream/","title":"UpdateStream"},{"content":" Learn how to use the Presto distributed query engine to analyze data in the platform.\n","keywords":["presto,","presto,","presto","reference,","reference,","sql","query","engine,","sql","engine,","sql","queries,","sql,","nosql"],"path":"https://github.com/jasonnIguazio/data-layer/presto/","title":"Using the Presto SQL-Query Engine"},{"content":"Fully Qualified Name org.apache.spark.streaming.v3io.V3IOUtils\nDescription A singleton utility object for creating Spark input streams that can be used to consume platform stream records via the Spark Streaming API.\nV3IO StreamsTo use this object you must first create one or more V3IO streams using the platform's create-stream API. See the CreateStream operation of the Streaming Web API.  Summary Prototype\nobject V3IOUtils Methods   createDirectStream\ndef createDirectStream[ V: ClassTag, VD \u0026lt;: Decoder[V] : ClassTag, R: ClassTag]( ssc: StreamingContext, v3ioParams: Map[String, String], streamNames: Set[String], messageHandler: RecordAndMetadata[V] =\u0026gt; R) : InputDStream[R]   Prototype object V3IOUtils createDirectStream Method Creates a Spark input-stream object that can be used to consume record data and metadata from the specified platform streams, using the Spark Streaming API (v3.1.2). The new input stream pulls stream records by querying the platform directly, without using a receiver.\nSyntax createDirectStream[ V: ClassTag, VD \u0026lt;: Decoder[V] : ClassTag, R: ClassTag]( ssc: StreamingContext, v3ioParams: Map[String, String], streamNames: Set[String], messageHandler: RecordAndMetadata[V] =\u0026gt; R) : InputDStream[R] Parameters  ssc Spark streaming-context object.\n Type: Spark StreamingContext   Requirement: Required   v3ioParams An optional map of platform configuration properties that configure the creation and behavior of the returned Spark input stream. For example, Map(\u0026quot;default-data-block-size\u0026quot; -\u0026gt; \u0026quot;524288\u0026quot;) configures the block size for read operations to 0.5 MB (524288 bytes).\n Type: Map[String, String] — a map of platform-property name and value string pairs   Requirement: Optional  NoteAll property values are provided as strings. The \u0026quot;Property Type\u0026quot; information in the following descriptions indicates the property type that the string represents.  Start-Position Configuration Properties You can optionally set the following properties to determine the start position for the new Spark input stream:\n  spark.streaming.v3io.streamStartPosition — the stream start-position type.\n Property Type: String Default Value: \u0026quot;latest\u0026quot;  The following property values are supported. The start-position information applies to each of the shards in the source V3IO streams (see streamNames):\n  \u0026quot;earliest\u0026quot; — start at the location of the earliest ingested record in the shard.\n  \u0026quot;latest\u0026quot; (default) — start at the end of the shard.\n  \u0026quot;time\u0026quot; — start at the location of the earliest ingested record in the shard beginning at the base time configured in the spark.streaming.v3io.streamStartTimeMS property. For shards without a matching record ingestion time (i.e., if all records in the shard arrived before the configured base time) the start position is set to the end of the shard.\n  \u0026quot;\u0026lt;record sequence number\u0026gt;\u0026quot; — start at the location of the record whose sequence number matches the property value (for example, \u0026quot;1\u0026quot;). For shards without a matching record sequence number, the start position is set to the end of the shard.\n  spark.streaming.v3io.streamStartTimeMS — the base time for a time-based stream start position (spark.streaming.v3io.streamStartPosition=\u0026quot;time\u0026quot;), as a Unix timestamp in milliseconds. For example, 1511260205000 sets the base time for determining the start position to 21 Nov 2017 at 10:30:05 AM UTC.\n Property Type: Long Default Value: 0 (Unix Epoch — 1 Jan 1970 at 00:00:00 UTC)       streamNames A set of one or more fully qualified V3IO stream paths of the format v3io://\u0026lt;container name\u0026gt;/\u0026lt;stream path\u0026gt; — where \u0026lt;container name\u0026gt; is the name of the stream's parent container and \u0026lt;stream path\u0026gt; is the path to the stream within the specified container. For example, v3io://mycontainer/mydata/metrics_stream.\nNoteAll streams paths in the streamNames set must point to existing V3IO streams that reside in the same container.\n   Type: Set[String]   Requirement: Required   messageHandler An optional record handler function for converting a stream record of type RecordAndMetadata into the desired input-stream record type, which is indicated by the handler's return value (R). The default record handler returns the record's data (also known as the record payload or value), decoded as the value type of the RecordAndMetadata class instance (V).\n Type: A record-handler function\nmessageHandler: RecordAndMetadata[V] =\u0026amp;gt; R    Requirement: Optional    Type Parameters  V \u0026mdash;  the type of the input stream's record data (\u0026quot;value\u0026quot;). VD \u0026mdash;  the decoder class to use for converting the record data into the specified type — the input stream's value type, as set in the method's V type parameter. See Stream-Data Encoding and Decoding Types. R \u0026mdash;  the input stream's record type, which is the type of the record handler's return value (see messageHandler). This type parameter is applicable only when providing a record handler in the method call. The default input-stream record type is the decoding type of the record data, as set in the method's V type parameter.  Return Value Returns an instance of a Spark input stream (InputDStream), which you can use with the Spark Streaming API to consume stream records in the specified input-stream record type (see the method's R type parameter).\nExamples Example 1 Create a Spark input stream for \u0026quot;/DriverStream\u0026quot; and \u0026quot;/Passengers\u0026quot; V3IO streams in a \u0026quot;mycontainer\u0026quot; container using the RecordAndMetadata.payloadWithMetadata method as the record handler. The input stream will return PayloadWithMetadata record objects that contain string-decoded record data and related metadata:\nval batchInterval = Seconds(1) val sparkConf = new SparkConf() .setAppName(\u0026#34;My Car Streams Application\u0026#34;) val ssc = new StreamingContext(sparkConf, batchInterval) val cfgProperties = Map.empty[String, String] val streamNames = Set(\u0026#34;/DriverStream\u0026#34;, \u0026#34;/Passengers\u0026#34;) val carsInputDStream = { val recordHandler = (rmd: RecordAndMetadata[String]) =\u0026gt; rmd.payloadWithMetadata() V3IOUtils.createDirectStream[String, StringDecoder, PayloadWithMetadata[String]]( ssc, cfgProperties, streamNames, recordHandler) } Example 2 Create a Spark input stream for \u0026quot;/WebClicks\u0026quot; and \u0026quot;/ServerLogs\u0026quot; V3IO streams within a \u0026quot;/Web/Streams/\u0026quot; directory in a \u0026quot;mycontainer\u0026quot; container, starting at the earliest ingested record found in each of the stream shards. Use the default record handler to create an input stream that returns the record data as a string:\nval batchInterval = Seconds(2) val sparkConf = new SparkConf() .setAppName(\u0026#34;My Web Streams Application\u0026#34;) val ssc = new StreamingContext(sparkConf, batchInterval) val cfgProperties = Map(\u0026#34;spark.streaming.v3io.streamStartPosition\u0026#34; -\u0026gt; \u0026#34;earliest\u0026#34;) val streamNames = Set(\u0026#34;/Web/Streams/WebClicks\u0026#34;, \u0026#34;/Web/Streams/ServerLogs\u0026#34;) val webInputDStream = { V3IOUtils.createDirectStream[String, StringDecoder]( ssc, cfgProperties, streamNames) } ","keywords":["V3IOUtils,","createDirectStream,","spark","streaming,","spark","streaming","api,","streams,","spark","input","streams,","DStream,","InputDStream,","stream","records,","record","metadata,","stream","consumption,","get","records,","spark-streaming","context,","configuration","properties,","property","types,","configurations,","ssc,","v3ioParams,","streamNames,","messageHandler,","container-id,","container","ID,","container","name,","default-data-block-size"],"path":"https://github.com/jasonnIguazio/data-layer/reference/spark-apis/spark-streaming-integration-api/v3ioutils/","title":"V3IOUtils Object"},{"content":"This section contains the release notes for version 1.0 of the Iguazio Continuous Data Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.0/","title":"Version 1.0 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio Data Platform in version 1.0.0, and known issues in this version.\nNew Features and Enhancements Performance   Improved streaming and NoSQL database performance.\n  Web APIs Streaming Web API\n  Replaced the CreateStream and DescribeStream retention-size request parameter (ShardRetentionPeriodSizeMB) with a retention-time parameter (RetentionPeriodHours).\n  Replaced the GetRecords BytesBehindLatest response parameter with an MSecBehindLatest parameter.\n  Changed location parameter types from number to blob:\n  GetRecords Location request parameter, and NextLocation response parameter\n  Seek Location response parameter\n    Management APIs   Added a Cluster-Information API. The API's Get Cluster Information operation returns information about the endpoints of the platform cluster, including the endpoints' IP addresses, which are used to access the platform's resources.\n  Security   Added support for defining data-policy rules for specific interfaces (for example, Web or Spark and Hadoop).\n  Dashboard (UI)   Added browse filter options.\n  Added an option to export event logs to Excel.\n  Support and Debugging   Added email notifications for\n  System alerts   Creation of new users   Password resets     Added automated report emails to Iguazio support (\u0026quot;call home\u0026quot;).\n  Known Issues General   The file-system change-time attribute (ctime) is not updated as a result of changes to an object's system attributes.\n  Web APIs   Authorization failure may result in response status code 500 instead of 400.\n  Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n  ETAG is not supported for GET actions.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  NoSQL Web API\n  Requests with long filter expressions may fail.\n  PutItem requests with an item that contains unnamed attributes succeed.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  When a PutItem condition expression (ConditionExpression) evaluates to false, the operation returns a 400 error instead of returning successfully.\n  Object (item) size restrictions —\n  Each attribute must be smaller than 64 KB.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8KB.\n    Management APIs Container Management API\n  Container names (set in Create Container) cannot start with a numeric digit.\n  List Containers may hang when there are no containers in the platform.\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, setrep, and truncate.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  Data-policy rules are not applied to streaming operations.\n  Dashboard (UI)   The performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  Opening a large file in the Browse view may cause the dashboard to hang.\n  Spaces in the name of a container directory may cause a browse error.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.0/v1.0.0/","title":"Version 1.0.0 Release Notes"},{"content":"This section contains the release notes for version 1.5 of the Iguazio Continuous Data Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.5/","title":"Version 1.5 Release Notes"},{"content":"This document outlines the main additions and changes to the Iguazio Data Platform in version 1.5.0, and known issues in this version.\nNew Features and Enhancements General   Added support for multi-node clusters that contain up to three nodes.\n  Added support for using the platform with the Amazon EMR framework.\n  Added the following open-source third-party applications to the default platform installation:\n TensorFlow machine-intelligence (MI) library Sparkling Water library for integrating the H2O.ai machine-learning platform with Apache Spark, including support for pysparkling to simplify development of Python applications over Spark Anaconda Python data science distribution    High Availability (HA)   Implemented a no single point of failure (SPOF) design.\n  Update Expressions    Added support for update expressions that simplify and extend the ability to update item attributes, including changing attribute values and deleting attributes. The support includes a new assignment operator (=) and if_not_exists, min, and max expression functions.\n  Web APIs   Added support for sending secure HTTPS requests with user authentication.\n  Streaming Web API\n  Added to the Seek operation a Type=TIME option for retrieving a location within a shard based on the records' ingestion (arrival) time.\n  Changed the behavior of the Seek operation's get-latest-location option (Type=LATEST) to return the location of the end of the shard instead of the location of the latest ingested record.\n  NoSQL Web API\n   Replaced the UpdateItem operation's UpdateMode and Item request parameters with an UpdateExpression parameter to support update expressions.\n  Scala APIs NoSQL Scala API\n   Added a KeyValueOperations update method that receives an expression parameter to support update expressions.\n   Added a V3IOPath class for getting an absolute v3io path to a file or directory within a specific data container.\n  Platform Management   Added major and critical email alerts when the platform capacity exceeds 70% and 90%, respectively. Upon reaching critical capacity (\u0026gt; 90%), the platform state changes to offline mode (\u0026quot;closed gates\u0026quot;).\n  Changed the number of the dashboard port, which is used for sending Management API requests, from 4001 to 8001.\n  Dashboard (UI)   Revamped the dashboard (UI) to provide an improved look and feel.\n  Fixes The following issues were fixed in the current release: General   Authorization failure may result in response status code 500 instead of 400.\n  Platform Management Container Management API\n  Container names (set in Create Container) cannot start with a numeric digit.\n  List Containers may hang when there are no containers in the platform.\n  Dashboard (UI)   Spaces in the name of a container directory may cause a browse error.\n  Known Issues General   CDH, Cloudera's Hadoop distribution, isn't supported. This issue will be fixed in a future release.\n  Automated system-failure recovery for ingested stream records or for data appended to a simple object might result in data duplication.\n  Condition and Update Expressions   Division of an unsigned integer by a negative integer always returns zero.\n  Assignment of a Boolean value in an update expression using the true or false keyword causes a 400 error. Note that assigning an expression that evaluates to a Boolean value, such as NOT false instead of true, works correctly.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n  GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long filter expressions may fail.\n  PutItem requests with an item that contains unnamed attributes succeed.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Object (item) size restrictions —\n  Each attribute must be smaller than 64 KB.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8 KB.\n    When setting an item's primary key using Key request parameter of the PutItem or UpdateItem operation, you cannot define a user attribute with the same name (even if the value is the same). Note that when using the Key parameter, the platform automatically creates a matching user attribute for you. If you wish to create the attribute yourself, pass the item's name (primary key) in the request URL instead of using the JSON Key parameter.\n  Spark APIs Spark Streaming\n  To perform more than one action on the stream data, you must first cache the RDD. Note that this is the recommended practice in Spark Streaming.\n  Spark-Streaming Integration API\n  Stream paths set in the topicNames parameter of the V3IOUtils.CreateDirectStream method must begin with a forward slash (/).\n  Platform Management Container Management API\n  New containers can be created when the platform is in offline mode (\u0026quot;closed gates\u0026quot;).\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, setrep, and truncate.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  Dashboard (UI)   The performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  The performance statistics for multi-node clusters are inaccurate.\n  Opening a large file in the Browse view may cause the dashboard to hang.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.5/v1.5.0/","title":"Version 1.5.0 Release Notes"},{"content":"This document outlines the main additions and changes to the Iguazio Data Platform in version 1.5.1, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements General   [Tech Preview] Added support for using Presto, a distributed SQL query engine for big data. In this version, the platform supports only SELECT queries over its NoSQL database. A version of Presto is included as part of the default platform deployment.\n  Spark APIs Spark DataFrames\n  Added a counter attribute to the NoSQL DataFrame in place of the Counter DataFrame.\n  Fixes The following issues were fixed in the current release: General   Added back support for CDH, Cloudera's Hadoop distribution. The default platform deployment includes CDH v5.13 with Spark v2.1.\n  Fixed an error related to the stability of the v3io daemon (\u0026quot;ERROR client.ClientActor: Unable to signal /tmp/iguazio/daemon/fifo/v3io_daemon_worker_0_consumer_fifo_0\u0026quot;).\n  Dashboard (UI)   The Browse view may become unresponsive when there are many items.\n  Known Issues General   Automated system-failure recovery for ingested stream records or for data appended to a simple object might result in data duplication.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  Presto [Tech Preview]   The SHOW TABLES command may hang for large tables.\n  Condition and Update Expressions   Division of an unsigned integer by a negative integer always returns zero.\n  Assignment of a Boolean value in an update expression using the true or false keyword causes a 400 error. Note that assigning an expression that evaluates to a Boolean value, such as NOT false instead of true, works correctly.\n  The use of parentheses is sometimes required to ensure the correct operator precedence in an expression.\n  In Spark DataFrame and Presto (Tech Preview) queries, the IN operator is supported only for columns of type string.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long filter expressions may fail.\n  PutItem requests with an item that contains unnamed attributes succeed.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Object (item) size restrictions —\n  Each attribute must be smaller than 64 KB.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8 KB.\n    When setting an item's primary key using Key request parameter of the PutItem or UpdateItem operation, you cannot define a user attribute with the same name (even if the value is the same). Note that when using the Key parameter, the platform automatically creates a matching user attribute for you. If you wish to create the attribute yourself, pass the item's name (primary key) in the request URL instead of using the JSON Key parameter.\n  Spark APIs Spark Streaming\n  To perform more than one action on the stream data, you must first cache the RDD. Note that this is the recommended practice in Spark Streaming.\n  Spark-Streaming Integration API\n  Stream paths set in the topicNames parameter of the V3IOUtils.CreateDirectStream method must begin with a forward slash (/).\n  Platform Management Container Management API\n  New containers can be created when the platform is in offline mode (\u0026quot;closed gates\u0026quot;).\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, setrep, and truncate.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  Dashboard (UI)   The performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  The performance statistics for multi-node clusters are inaccurate.\n  Opening a large file in the Browse view may cause the dashboard to hang.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.5/v1.5.1/","title":"Version 1.5.1 Release Notes"},{"content":"This document outlines the main additions and changes to the Iguazio Data Platform in version 1.5.2, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements Presto [Tech Preview]   Improved the performance of Presto queries.\n  Known Issues General   Automated system-failure recovery for ingested stream records or for data appended to a simple object might result in data duplication.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  Presto [Tech Preview]   The SHOW TABLES command may hang for large tables.\n  Condition and Update Expressions   Division of an unsigned integer by a negative integer always returns zero.\n  Assignment of a Boolean value in an update expression using the true or false keyword causes a 400 error. Note that assigning an expression that evaluates to a Boolean value, such as NOT false instead of true, works correctly.\n  The use of parentheses is sometimes required to ensure the correct operator precedence in an expression.\n  In Spark DataFrame and Presto (Tech Preview) queries, the IN operator is supported only for columns of type string.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long filter expressions may fail.\n  PutItem requests with an item that contains unnamed attributes succeed.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Object (item) size restrictions —\n  Each attribute must be smaller than 64 KB.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8 KB.\n    When setting an item's primary key using Key request parameter of the PutItem or UpdateItem operation, you cannot define a user attribute with the same name (even if the value is the same). Note that when using the Key parameter, the platform automatically creates a matching user attribute for you. If you wish to create the attribute yourself, pass the item's name (primary key) in the request URL instead of using the JSON Key parameter.\n  Spark APIs Spark Streaming\n  To perform more than one action on the stream data, you must first cache the RDD. Note that this is the recommended practice in Spark Streaming.\n  Spark-Streaming Integration API\n  Stream paths set in the topicNames parameter of the V3IOUtils.CreateDirectStream method must begin with a forward slash (/).\n  Platform Management Container Management API\n  New containers can be created when the platform is in offline mode (\u0026quot;closed gates\u0026quot;).\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, setrep, and truncate.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  Dashboard (UI)   The performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  The performance statistics for multi-node clusters are inaccurate.\n  Opening a large file in the Browse view may cause the dashboard to hang.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.5/v1.5.2/","title":"Version 1.5.2 Release Notes"},{"content":"This document outlines the main additions and changes to the Iguazio Data Platform in version 1.5.3, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements NoSQL   Added a create_schema utility application for generating platform NoSQL table schemas for use with Spark DataFrames and Presto. The application is available as part of the public open-source v3io GitHub repository.\n  Dashboard (UI)   Allow or restrict access to different dashboard screens based on the management policies of the dashboard user.\n  Fixes The following issues were fixed in the current release:\nStability   v1.5.3 introduced several stability improvements and bug fixes across all platform components.\n  Condition and Update Expressions   Division of an unsigned integer by a negative integer always returns zero.\n  Assignment of a Boolean value in an update expression using the true or false keyword causes a 400 error. Note that assigning an expression that evaluates to a Boolean value, such as NOT false instead of true, works correctly.\n  The use of parentheses is sometimes required to ensure the correct operator precedence in an expression.\n  In Spark DataFrame and Presto (Tech Preview) queries, the IN operator is supported only for columns of type string and does not work with columns of type integer.\n  User Management   The predefined security_admin user can deactivate itself.\n  The default tenant can be deleted, resulting also in the removal of the predefined tenancy_admin user.\n  The default passwords of the predefined security_admin and tenancy_admin users cannot be changed.\n  Dashboard (UI)   The performance statistics for multi-node clusters are inaccurate.\n  Opening a large file in the Browse view may cause the dashboard to hang.\n  Known Issues General   Automated system-failure recovery for ingested stream records or for data appended to a simple object might result in data duplication.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  Condition and Update Expressions   When the min, max, starts, ends, or contains expression function receives too many arguments, the operation is skipped and the function doesn't return an error.\n  Presto [Tech Preview]   The SHOW TABLES command may hang for large tables.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long filter expressions may fail.\n  PutItem requests with an item that contains unnamed attributes succeed.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Object (item) size restrictions —\n  Each attribute must be smaller than 64 KB.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8 KB.\n    When setting an item's primary key using Key request parameter of the PutItem or UpdateItem operation, you cannot define a user attribute with the same name (even if the value is the same). Note that when using the Key parameter, the platform automatically creates a matching user attribute for you. If you wish to create the attribute yourself, pass the item's name (primary key) in the request URL instead of using the JSON Key parameter.\n  Spark APIs Spark Streaming\n  To perform more than one action on the stream data, you must first cache the RDD. Note that this is the recommended practice in Spark Streaming.\n  To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n  Spark-Streaming Integration API\n  Stream paths set in the topicNames parameter of the V3IOUtils.CreateDirectStream method must begin with a forward slash (/).\n  Platform Management Container Management API\n  New containers can be created when the platform is in offline mode (\u0026quot;closed gates\u0026quot;).\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, setrep, and truncate.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  Dashboard (UI)   The performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  Notes   Platform start-up, restart, and shut-down should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.5/v1.5.3/","title":"Version 1.5.3 Release Notes"},{"content":"This document outlines the main additions and changes to the Iguazio Continuous Data Platform in version 1.5.4, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Fixes   In rare cases, when the system is running under a heavy load, a race condition might be triggered, causing loss of access to the platform.\n  Known Issues General   Automated system-failure recovery for ingested stream records or for data appended to a simple object might result in data duplication.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  Condition and Update Expressions   When the min, max, starts, ends, or contains expression function receives too many arguments, the operation is skipped and the function doesn't return an error.\n  Presto [Tech Preview]   The SHOW TABLES command may hang for large tables.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long filter expressions may fail.\n  PutItem requests with an item that contains unnamed attributes succeed.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Object (item) size restrictions —\n  Each attribute must be smaller than 64 KB.\n  The total size of all attributes whose size is smaller or equal to 200 bytes cannot exceed 8 KB.\n    When setting an item's primary key using Key request parameter of the PutItem or UpdateItem operation, you cannot define a user attribute with the same name (even if the value is the same). Note that when using the Key parameter, the platform automatically creates a matching user attribute for you. If you wish to create the attribute yourself, pass the item's name (primary key) in the request URL instead of using the JSON Key parameter.\n  Spark APIs Spark Streaming\n  To perform more than one action on the stream data, you must first cache the RDD. Note that this is the recommended practice in Spark Streaming.\n  To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n  Spark-Streaming Integration API\n  Stream paths set in the topicNames parameter of the V3IOUtils.CreateDirectStream method must begin with a forward slash (/).\n  Platform Management Container Management API\n  New containers can be created when the platform is in offline mode (\u0026quot;closed gates\u0026quot;).\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, setrep, and truncate.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  Dashboard (UI)   The performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  Notes   Platform start-up, restart, and shut-down should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.5/v1.5.4/","title":"Version 1.5.4 Release Notes"},{"content":"This section contains the release notes for version 1.7 of the Iguazio Continuous Data Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.7/","title":"Version 1.7 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio Continuous Data Platform in version 1.7.1 (v1.7 GA release), and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements General   Updated the big-data libraries (such as Spark and Presto) to Java 8.\n  Added the following open-source third-party applications and related support to the default platform installation:\n Apache Hive data warehouse software (version 2.1.1) and a custom Iguazio Hive Metastore for seamlessly sharing a data catalog across multiple applications and defining relational tables and views.  Grafana platform for analytics and monitoring (version 5.0.4) [Tech Preview]. The platform comes with predefined Grafana dashboard for monitoring the application nodes.  Prometheus systems monitoring and alerting toolkit (version 2.0.0) [Tech Preview].     Raised the versions and related support for the following pre-deployed open-source third-party applications:\n Apache Zeppelin web-based notebook [Tech Preview] — version 0.7.3 TensorFlow machine-intelligence (MI) library [Tech Preview] — version 1.4.0 See also the supported Spark and Hadoop version updates in the dedicated enhancements sections.    Added a built-in process that uses Presto SQL to create a Hive view that monitors both real-time data in the platform's NoSQL store and historical data in Parquet tables [Tech Preview].\n  Added global back-pressure support for the big-data libraries.\n  Condition and Update Expressions   Added support for using array attributes in condition expressions [Tech Preview].\n  Added a length function for calculating the size of a string.\n  Extended the support for using array and blob attributes in expressions [Tech Preview]:\n Added support for using the addition operator (+) to concatenate arrays and array slices. Added find, find_all_indices, extract_by_indices, and sort functions for arrays and array slices. Added a blob function for defining blobs from Base64 encoded strings and creating blob attributes. Added a length function for calculating the size of an array or array slice. Added a sum function for summing up the values of all elements in an array or array slice.    See also the new UpdateItem NoSQL Web API option to perform if-else conditional updates.\n  Web APIs NoSQL Web API\n   GetItems now has optional ShardingKey, SortKeyRangeStart, and SortKeyRangeEnd parameters to support faster range scans that scan only for items within a specific range of sort-key attribute values [Tech Preview].\n   UpdateItem now has an optional AlternateUpdateExpression request parameter to support an if-else conditional-update logic.\n  Spark   Added support for Apache Spark version 2.2.1 and stopped the support for version 1.6.3.\n  Spark NoSQL DataFrame\n  Added support for automatic inference of NoSQL table schemas [Tech Preview].\n  Added a \u0026quot;condition\u0026quot; option for performing conditional updates.\n  Added a \u0026quot;partition\u0026quot; option for partitioning of NoSQL tables [Tech Preview].\n  Added support for faster range scans that scan only for items within a specific range of sort-key attribute values [Tech Preview].\n  Added support for the timestamp data type (TimestampType).\n  Spark-Streaming Integration API\n  Added a Python Spark-Streaming Integration API [Tech Preview], similar to the existing Scala API. Public code examples that previously used a preliminary unofficial version of the Python API were updated to accommodate v1.7.0 improvements to the API.\n  Renamed the Scala V3IOUtils.createDirectStream topicNames parameter to streamNames.\n  Consolidated the container-alias and container-id configuration properties into a single container-id property that can receive either a container-name (alias) string or a numeric container ID.\n  Added support for seek by time (search for stream records by their ingestion time) via the new spark.streaming.v3io.streamStartPosition and spark.streaming.v3io.streamStartTimeMS configuration properties.\n  Presto   Added official support for using Presto (version 0.189), a distributed SQL query engine for big data (previously supported as Tech Preview), which is included as part of the default platform installation. In this version, the platform supports only SELECT queries over its NoSQL database.\n  Added support for automatic inference of NoSQL table schemas [Tech Preview].\n  Added support for faster range scans that scan only for items within a specific range of sort-key attribute values [Tech Preview].\n  Added support for the timestamp data type [Tech Preview].\n  Hadoop   Added support for Apache Hadoop version 2.9.0.\n  Accelerated recursive file and directory deletion using hadoop fs commands (hadoop fs -rm -r or hadoop fs -rmdir \u0026lt;directory\u0026gt;).\n  Added support for the truncate Hadoop (hdfs) command [Tech Preview].\n  High Availability (HA)   Added support for failover: in the event of a system failure, the platform can maintain the availability of the service.\n  Security and User Management    Added support for multiple tenants (\u0026quot;multi-tenancy\u0026quot;): a user with the Tenant Admin management policy can now create a dedicated and isolated tenant for each customer in a shared cluster. Users can only view and manage objects in their tenant and do not have any access to other tenants in the cluster.\n  Added official support for management policies: a user with the Security Admin management policy can now assign users and user groups management policies that govern their permissions to perform different management-related operations.\n  Platform Management   Container names can now contain underscores (_).\n  Container names can no longer include upper-case characters (similar to naming restrictions enforced by third parties such as Presto).\n  Dashboard (UI)   Added an option to trigger support-logs collection from the dashboard [Tech Preview]. Please consult the Iguazio support team before triggering a log collection.\n  Improved editing of blob attributes.\n  Moved the IdP configuration to the Identity view, which is accessible only to users with the Security Admin management policy.\n  Added miscellaneous improvements to the dashboard to enhance user experience.\n  Amazon EMR   Added support for Amazon EMR version 5.12.1.\n  Hardware   Made miscellaneous hardware improvements, including support for the low-endurance Intel P4500 4 TB drives.\n  Support   Enhanced the Iguazio service center by adding platform heartbeat monitoring.\n  Fixes The following issues were fixed in the current release:\nCondition and Update Expressions   When the min, max, starts, ends, or contains expression function receives too many arguments, the operation is skipped and the function doesn't return an error. In v1.7.0, the function returns a 400 error.\n  Web APIs NoSQL Web API\n  PutItem requests with an item that contains unnamed attributes succeed.\n  When setting an item's primary key using Key request parameter of the PutItem or UpdateItem operation, you cannot define a user attribute with the same name (even if the value is the same).\n  Spark APIs Spark Streaming\n  To perform more than one action on the stream data, you must first cache the RDD. Note that this is the recommended practice in Spark Streaming.\n  Spark-Streaming Integration API\n  Stream paths set in the topicNames parameter of the V3IOUtils.CreateDirectStream method must begin with a forward slash (/). (In v1.7.0, the parameter was renamed to streamNames.)\n  Presto   The SHOW TABLES command may hang for large tables.\n  Platform Management Container Management API\n  New containers can be created when the platform is in offline mode (\u0026quot;closed gates\u0026quot;).\n  Known Issues Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  I/O requests might fail during failover.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long expressions may fail.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Spark APIs Spark Streaming\n  To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  IdP synchronization might occasionally result in permission issues for imported users.\n  For a data-access policy rule with a Users match condition that names both users and user groups — an \u0026quot;OR\u0026quot; logic is applied separately among all the users and among all the groups, but an \u0026quot;AND\u0026quot; logic is applied between the users and groups. Therefore, the user who submits the request must be both one of the named users and belong to one of the named groups. To bypass this issue, define separate rules for users and user groups.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Dashboard (UI)   The bandwidth and IOPS container performance values are higher than the actual values for Simple-Object Web API Get Object operations.\n  The performance graphs in the container overview might display an incorrect spike in the following instances:\n For the first data point in the graph when initially loading the display (before a periodic refresh occurs). For all data points that are added to the display as a result of the periodic display refresh or when the user changes the display filter. Refreshing the display typically resolves the spike for the currently displayed data points.    The container performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  Amazon EMR   The EMR cluster can be used only with the default tenant.\n  The EMR cluster uses a single node, resulting in limited redundancy.\n  Deprecated Features   Removed the Sparkling Water library for integrating H2O.ai with Apache Spark from the default platform installation.\n  Notes   Platform start-up, restart, and shut-down should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.7/v1.7.1/","title":"Version 1.7.1 Release Notes"},{"content":"This document outlines the main additions and changes to the Iguazio Continuous Data Platform in version 1.7.2, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Fixes The following issues were fixed in the current release:\nSecurity   Deny-all data-access policy rules overpower other rules. In v1.7.2, the data-access policy rules processing was fixed, subject to known issues.\n  Known Issues Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  I/O requests might fail during failover.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  NoSQL Web API\n  Requests with long expressions may fail.\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n  Spark APIs Spark Streaming\n  To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System   File-system operations are not supported for stream shards.\n  Security   POSIX security rules are enforced only on a user's primary group.\n  IdP synchronization might occasionally result in permission issues for imported users.\n  For a data-access policy rule with a Users match condition that names both users and user groups — an \u0026quot;OR\u0026quot; logic is applied separately among all the users and among all the groups, but an \u0026quot;AND\u0026quot; logic is applied between the users and groups. Therefore, the user who submits the request must be both one of the named users and belong to one of the named groups. To bypass this issue, define separate rules for users and user groups.\n  Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The \u0026quot;category\u0026quot; criterion of data-access policy rules is ignored for the NoSQL Web API GetItem operation.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Dashboard (UI)   The bandwidth and IOPS container performance values are higher than the actual values for Simple-Object Web API Get Object operations.\n  The performance graphs in the container overview might display an incorrect spike in the following instances:\n For the first data point in the graph when initially loading the display (before a periodic refresh occurs). For all data points that are added to the display as a result of the periodic display refresh or when the user changes the display filter. Refreshing the display typically resolves the spike for the currently displayed data points.    The container performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  Amazon EMR   The EMR cluster can be used only with the default tenant.\n  The EMR cluster uses a single node, resulting in limited redundancy.\n  Notes   Platform start-up, restart, and shut-down should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.7/v1.7.2/","title":"Version 1.7.2 Release Notes"},{"content":"This section contains the release notes for version 1.9 of the Iguazio Continuous Data Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.9/","title":"Version 1.9 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio Continuous Data Platform in version 1.9.4 (v1.9 GA release), and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements General   Significantly improved the platform's performance by changing internal kernel memory synchronizations.\n  Enabled users other than the predefined \u0026quot;iguazio\u0026quot; user to run application services on the platform's application nodes.\n  Raised the versions and related support for the following pre-deployed open-source third-party applications:\n Apache Zeppelin web-based notebook [Tech Preview] — version 0.8.0 Apache Hive data warehouse software — version 2.3.3 See also the supported Spark version update in the dedicated enhancements section.    Nuclio (Serverless Functions) [Tech Preview]    Integrated Nuclio (version 0.5.6) — Iguazio's open-source serverless framework for development and execution of auto-scaling applications — into the platform, on top of Kubernetes [Tech Preview]. Users with the new Function Admin management policy can use the Nuclio graphical dashboard from within the platform dashboard (see the new Functions dashboard page).\n   Web APIs Streaming Web API\n  Increased the maximum record size to 2 MB.\n   Spark  Updated the Apache Spark support to version 2.3.1  Spark NoSQL DataFrame\n  Added support for the Spark SQL ArrayType data type for representing array NoSQL item attributes [Tech Preview].\n  Write operations no longer overwrite a different existing table schema unless the new \u0026quot;allow-overwrite-schema\u0026quot; write option is set.\n  Added a \u0026quot;range-scan-even-distribution\u0026quot; write option for optimizing storage of non-uniform data (where most of the data applies to only a few sharding keys) and shortening related query times with Spark DataFrames or Presto [Tech Preview]. (Note that using this option changes the way that you retrieve the item with the GetItems NoSQL Web API.)\n   Presto   Added support for the Presto ARRAY data type for representing array NoSQL item attributes [Tech Preview].\n  Added a convenience presto link to the native Presto CLI (presto-cli), which presets the --server option to Presto server URL of the current platform instance and the --catalog option to the v3io catalog for working with Iguazio Continuous Data Platform NoSQL tables.\n   Scala APIs NoSQL Scala API\n  Added a V3IOArray class for representing array NoSQL item attributes [Tech Preview].\n  Security and User Management   Added a Function Admin management policy for managing and developing Nuclio serverless functions.\n  Added security and user-action audit events. See the new Events \u0026gt; Audit dashboard tab.\n  Modified the password-selection procedure for new users and tenants to avoid sending an initial password by email.\n  Added safeguards for secure tenant deletion.\n   Dashboard (UI)   Added a Functions dashboard page (accessible to users with the new Function Admin management policy) for viewing, managing, and developing serverless functions with Nuclio [Tech Preview].\n  Added an Audit tab to the Events dashboard page for viewing security events (such as a failed login) and user actions (such as creation and deletion of a container).\n  Officially support the option to trigger support-logs collection from the dashboard (previously supported as Tech Preview). Please consult the Iguazio support team before triggering a log collection.\n  Added an option to customize the dashboard colors.\n  Made miscellaneous text improvements in the dashboard.\n  Hovering over a performance-statistics graph in the container Overview tab now shows information for the selected data point also from the other graphs.\n  Fixes The following issues were fixed in the current release:\n Web APIs NoSQL Web API\n  PutItem requests with a very large Key attribute value may result in response status code 500 instead of 400.\n   Security   POSIX security rules are enforced only on a user's primary group.\n  IdP synchronization might occasionally result in permission issues for imported users.\n  Dashboard (UI)   The performance graphs in the container overview might display an incorrect spike in the following instances:\n For the first data point in the graph when initially loading the display (before a periodic refresh occurs). For all data points that are added to the display as a result of the periodic display refresh or when the user changes the display filter. Refreshing the display typically resolves the spike for the currently displayed data points.    Known Issues Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  I/O requests might fail during failover.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  Spark Spark NoSQL DataFrame\n  When the value of an item's sharding key ends in a period (.), a range scan [Tech Preview] using this sharding key may return duplicate results.\n  Spark Streaming\n  To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  Hive   Creation of a new table with more than 5M items fails. However, it's possible to create an empty table or a table with fewer items and then add more than 5M items to the table.\n  File System   File-system operations are not supported for stream shards.\n  Security and User Management   For a data-access policy rule with a Users match condition that names both users and user groups — an \u0026quot;OR\u0026quot; logic is applied separately among all the users and among all the groups, but an \u0026quot;AND\u0026quot; logic is applied between the users and groups. Therefore, the user who submits the request must be both one of the named users and belong to one of the named groups. To bypass this issue, define separate rules for users and user groups.\n  Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The \u0026quot;category\u0026quot; criterion of data-access policy rules is ignored for the NoSQL Web API GetItem operation.\n  A user without an assigned group can view other users' data.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI)   The bandwidth and IOPS container performance values are higher than the actual values for Simple-Object Web API Get Object operations.\n  The container performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  The Audit tab on the Events dashboard page doesn't show the Tenant.Creation.FailedPasswordEmail event.\n  Dashboard logins fail for usernames that contain hyphens (-).\n  Amazon EMR   The EMR cluster can be used only with the default tenant.\n  The EMR cluster uses a single node, resulting in limited redundancy.\n  Notes   Platform start-up, restart, and shut-down should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.9/v1.9.4/","title":"Version 1.9.4 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio Continuous Data Platform in version 1.9.5, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements Spark Spark-Streaming Integration API\n  V3IOUtils.createDirectStream was modified to support passing fully qualified stream paths in the streamNames parameter. The option of configuring the streams' parent container via the container-id configuration property is now deprecated.\n  Known Issues Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  I/O requests might fail during failover.\n  Web APIs Simple-Object Web API\n  The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API\n  PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  Spark Spark NoSQL DataFrame\n  When the value of an item's sharding key ends in a period (.), a range scan [Tech Preview] using this sharding key may return duplicate results.\n  Spark Streaming\n  To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  Hive   Creation of a new table with more than 5M items fails. However, it's possible to create an empty table or a table with fewer items and then add more than 5M items to the table.\n  File System   File-system operations are not supported for stream shards.\n  Security and User Management   For a data-access policy rule with a Users match condition that names both users and user groups — an \u0026quot;OR\u0026quot; logic is applied separately among all the users and among all the groups, but an \u0026quot;AND\u0026quot; logic is applied between the users and groups. Therefore, the user who submits the request must both be one of the named users and belong to one of the named groups. To bypass this issue, define separate rules for users and user groups.\n  Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The \u0026quot;category\u0026quot; criterion of data-access policy rules is ignored for the NoSQL Web API GetItem operation.\n  A user without an assigned group can view other users' data.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI)   The bandwidth and IOPS container performance values are higher than the actual values for Simple-Object Web API Get Object operations.\n  The container performance statistics don't include multi-record operations, such as GetItems and GetRecords.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  The Audit tab on the Events dashboard page doesn't show the Tenant.Creation.FailedPasswordEmail event.\n  Dashboard logins fail for usernames that contain hyphens (-).\n  Amazon EMR   The EMR cluster can be used only with the default tenant.\n  The EMR cluster uses a single node, resulting in limited redundancy.\n  Notes   Platform start-up, restart, and shut-down should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-1.9/v1.9.5/","title":"Version 1.9.5 Release Notes"},{"content":"This section contains the release notes for version 2.0 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.0/","title":"Version 2.0 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.0.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Beta NoteThe \u0026quot;[Beta]\u0026quot; notation marks features that are not officially supported in this release.  New Features and Enhancements General   Significantly improved the performance for updates to the NoSQL data store.\n  Increased the maximum attribute size to 1 MB.\n  Managed Application Services   Moved to using Kubernetes (k8s) (version 1.12.3) to deploy, scale, and manage both built-in and integrated application services and to enable users to easily manage services from the platform dashboard; (see also the related dashboard enhancements).\n  Added support for creating Jupyter Notebook services that use the preinstalled JupyterLab web-based user interface (version 0.35.2) — including the integrated bash terminals and the Jupyter Notebook web-based notebook for interactive computing (v5.7.4) with platform tutorial notebooks. [Tech Preview]\n  Added official support for Grafana (previously supported as, raised the preinstalled version to version 5.4.2, and added support for creating Grafana user services for defining real-time dashboards that monitor and visualize data in the platform. Tech Preview)\n  Introduced V3IO Frames — an open-source library, developed by Iguazio, which provides a single high-performance API for accessing NoSQL, stream, and time-series (TSDB) data in the platform's data container. Platform users can create V3IO Frames services that use the preinstalled version of this library (version 0.4.3). [Tech Preview]\n  Added support for creating web-based shell services, which provide a command-line shell for running application services, such as Spark jobs and Presto queries, and performing basic file-system operations.\n  Added a pre-deployed log-forwarder service that forwards application-service logs to be stored and indexed in an instance of the Elasticsearch search and analytics engine. The predefined service is disabled by default, as it requires the user to configure the Elasticsearch URL.\n  Added a new predefined \u0026quot;users\u0026quot; container, which is designed to provide individual user development environments and is used by the platform to manage the application services.  When creating a new web-based shell, Jupyter Notebook, or Zeppelin service (see the managed application-services enhancements), a \u0026lt;username\u0026gt; directory with service configuration files is automatically created in this container for the service's running user. For the Jupyter Notebook service, this directory also contains the pre-deployed platform notebooks.\n  Web APIs   Added an X-v3io-transaction-verifier response header with the last modification time (__mtime_secs and __mtime_nsecs system attributes) to single-object operations:\n Simple-Object Web API (object modification time) — GET Object, PUT Object, POST Object, HEAD Object Streaming Web API (stream-shard modification time) — GetRecords, Seek NoSQL Web API (item modification time / parent data-node modification time for GetItems) — GetItem, GetItems, PutItem, UpdateItem    NoSQL Web API   Added official support for faster range-scan queries (previously supported as Tech Preview).\n  Added support for scanning a container's root directory by sending a GetItems request without a table path; the TableName request parameter is now optional.\n  Improved the returned error messages.\n  Presto   Raised the preinstalled version of Presto to version 0.206 and provided a pre-deployed Presto service over Kubernetes, which is shared by all users with permissions to view services and can be managed from the dashboard; (see also the k8s managed services release note).\n  Added access-key user authentication to the Presto service.\n  Added SSL/TLS authentication over HTTPS to secure access to the Presto coordinator using a server certificate.\n   Added support for faster item-specific queries with an \u0026quot;in\u0026quot; (IN) sharding-key filter (in addition to equal-to (=)).\n  Enhanced the support for faster range-scan queries:\n  Added official support for range scans (previously supported as Tech Preview).\n  Extended the range-scan support to all comparison operators.   Added partial range-scan support, using only the sharding-key attribute, for tables whose sorting-key attributes aren't of type string (such as integer or timestamp).     Spark Spark NoSQL DataFrame   Added a new columnUpdate option for implementing a \u0026quot;replace\u0026quot; update mode — append new item attributes and overwrite the values of existing attributes.\n  Enhanced the support for faster range-scan queries:\n   Added official support for range scans (previously supported as Tech Preview).\n  Extended the range-scan support to all comparison operators.   Added partial range-scan support, using only the sharding-key attribute, for tables whose sorting-key attributes aren't of type string (such as integer or timestamp).     Time-Series Databases (TSDBs)   Extended the platform's multi-model database to add native support for the time-series database (TSDB) structure, and added the following components to the default platform installation to support working with time-series databases over Kubernetes; (see also the k8s managed services release note):\n  The Iguazio V3IO TSDB CLI — a command-line interface tool (tsdbctl version 0.8.13) for creating and managing time-series databases (TSDBs) using the V3IO TSDB library.\n  The Iguazio V3IO TSDB Nuclio functions (version 0.0.20), which users can use to create TSDB Nuclio functions services that simplify TSDB data ingestion and queries using the V3IO TSDB library.\n  The Iguazio V3IO Prometheus distribution (version v2.4.3-v0.8.13), which provides a version of the Prometheus open-source systems monitoring and alerting toolkit (version 2.4.3) that is packaged with the V3IO TSDB library (version 0.8.13). Platform users can create Prometheus services that use V3IO Prometheus to query TSDB tables.     Nuclio Serverless Functions NoteFor information about the TSDB Nuclio functions service, see the TSDB release notes.    Added official support for the Nuclio serverless framework (previously supported as Tech Preview), and raised the preinstalled version to version 1.1.1.\n  Backup and Recovery   Added a predefined \u0026quot;sys\u0026quot; user for performing backups.\n  Security and User Management   Added a Service Admin management policy for managing application services; (see also the managed-services release note).\n  NoteSee the Presto and dashboard sections for additional security and user-management enhancements.  Platform Management   Added event notifications for exceeding a tenant's storage-capacity quota thresholds. See also the new support for configuring and retrieving the quota and notification thresholds from the dashboard or by using the Tenant Management API.\n  Management APIs [Beta] Tenant Management API   Added the ability to set a tenant's storage-capacity quota and related notification thresholds via the Create Tenant and Configure Tenant operations (POST/PUT on /api/tenants/[\u0026lt;tenant ID\u0026gt;/]), and to retrieve this information via the List Tenants operation (GET on /api/tenants/[\u0026lt;tenant ID\u0026gt;/]). See also the similar dashboard capabilities and the exceeded-tenant-quota notification.\n  Tenant-Statistics Management API   Added the ability to retrieve a tenant's used storage capacity within the response of a Get Tenant Statistics operation (GET on /api/tentants/statistics/[\u0026lt;tenant ID\u0026gt;/]).\n  Dashboard (UI)   Added a Services dashboard page for easy management of application services — including viewing, adding, removing, configuring, and restarting services, and single sign-on into services using the logged-in dashboard user; (see also the managed-services release note).\n  Added an Access Keys user-profile window for creating and retrieving access keys that are used for user authentication with services such as Presto and TSDB.\n  Added a new Home page with links to common services and to quick-start demos and documentation.\n  Added a Scanned Items container-overview graph with performance statistics for item read and delete operations.\n  Added support for viewing and setting a tenant's storage-capacity quota and configuring capacity thresholds for receiving quota event notifications. See also the similar management APIs capabilities and the exceeded-tenant-quota notification.\n  Support   Improved the platform heartbeat monitoring by enriching the system data that is being sent to the Iguazio service center for supported machines that are part of the service.\n  Fixes The following issues were fixed in the current release:\nManaged Application Services Spark Spark NoSQL DataFrame   Spark DataFrame range scans for a table with invalid sharding-key attribute values that contain a period may return duplicate results. In v2.0.0, attempting to write a DataFrame with such sharding-key attribute values produces an error.\n  Security and User Management   For a data-access policy rule with a Users match condition that names both users and user groups — an \u0026quot;OR\u0026quot; logic is applied separately among all the users and among all the groups, but an \u0026quot;AND\u0026quot; logic is applied between the users and groups. Therefore, the user who submits the request must both be one of the named users and belong to one of the named groups.\n  A user without an assigned group can view other users' data.\n  High Availability (HA)   I/O streaming requests might fail during failover.\n  Dashboard (UI)   The container performance statistics don't include multi-record operations, such as GetItems and GetRecords. See the dashboard enhancement note for the addition of the scanned-items statistics in v2.0.0.\n  Dashboard logins fail for usernames that contain hyphens (-).\n  Known Issues Managed Application Services   Service dependency changes require restarting the dependent service. For example, if you change the Spark configuration for a web-based shell, Jupyter Notebook, or Zeppelin service, you need to restart the modified service after applying the configuration changes.\n  A user whose username was previously in use in the platform cannot run application services.\n  Web APIs Simple-Object Web API   The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  Jupyter Notebook [Tech Preview]   The tutorial Jupyter notebooks aren't pre-deployed as part of platform installations without an internet connection (offline installations). You can view and download the notebooks from https://github.com/v3io/tutorials. For assistance in adding these notebooks to your platform Jupyter Notebook services, contact Iguazio's customer-success team.\n  Deployment of a Jupyter Notebook or web-shell service for a running user whose name contains an uppercase letter might fail.\n  Running Scala code from a Jupyter notebook [Tech Preview] isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  Zeppelin   A Zeppelin service that isn't configured to work with a Spark service can't be deployed.\n   The platform's default Spark resource configurations are overridden by Zeppelin's built-in configurations, potentially causing the Zeppelin notebook to consume of all available resources of the related Spark service. You can resolve this by manually deleting the spark.cores.max and spark.executor.memory properties of the Zeppelin Spark interpreter and restarting the interpreter to apply the default platform configurations.\n  Spark Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Time-Series Databases (TSDBs)   It's possible to successfully create a TSDB Nuclio functions or Prometheus service for a non-existent TSDB table.\n  Nuclio Serverless Framework NoteThe TSDB Nuclio Functions known issues are documented as part of the TSDB known issues.    Function deployment for the Ruby, Java, and Node.js runtimes isn't supported for platform installations without an internet connection (offline installations).\n  Hadoop   The following Hadoop (hdfs) commands are not supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System   File-system operations are not supported for stream shards.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)    The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Security and User Management   Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The \u0026quot;category\u0026quot; criterion of data-access policy rules is ignored for the NoSQL Web API GetItem operation.\n  The platform doesn't prevent deletion of a user who is the running user of platform services; such deletions can potentially disrupt the platform's application-services management. Before deleting a user, a security administrator should either delete all services that this user is running or reassign the services to a different running user.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI)   In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  Obsolete Features   The following application services were removed from the default platform installation as part of the move to Kubernetes for managing application services:   Hive Metastore — removed temporarily in this release.   Anaconda.  In v2.0.0, the Jupyter Notebook service type includes a pre-deployed version of the Conda binary package and environment manager.\n  TensorFlow.  In the v2.0.0, TensorFlow can be installed independently (tested with version 1.13.1).\n    Notes   Platform start-up and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.0/v2.0.0/","title":"Version 2.0.0 Release Notes"},{"content":"This section contains the release notes for version 2.1 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.1/","title":"Version 2.1 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.1.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements Managed Application Services Spark   Integrated popular Java database connectivity (JDBC) libraries with the platform's Spark service — MySQL, PostgreSQL, Oracle, Microsoft SQL Server, and AWS Redshift.\n  Time-Series Databases (TSDBs)   Added a tsdbctl bash alias to the web-shell and Jupyter terminal environments, which preconfigures the web-API service (-s|--server) and access-key (-k|--access-key) flags for the service's running user.\n  Nuclio Serverless Functions   Exposed the platform's tenant-wide Nuclio serverless-framework service on the dashboard's Services page (\u0026quot;Nuclio\u0026quot;). Users can disable or restart the service and configure the Docker Registry service that it uses to store function images. See also the related dashboard enhancements and the new Docker Registry service.\n  Added support for running Nuclio functions on a graphics processing unit (GPU), including the ability to configure a function's GPU resources.\n  Enhanced the security of V3IO function-code archive imports by adding platform access-key authentication.\n  Replaced the Kubernetes secret authentication for V3IO volumes with platform access-key authentication.\n  Jupyter Notebook [Tech Preview]   Added a tsdbctl bash alias to the Jupyter terminal. See the related TSDB note.\n  Web Shell   Added a tsdbctl bash alias to the web shell. See the related TSDB note.\n  Docker Registry   Added a new Docker Registry service type, which users can use to configure a remote off-cluster Docker Registry to be used by Nuclio to store function images. In the default tenant, also added a default tenant-wide \u0026quot;Docker Registry\u0026quot; service of this new service type, which uses a pre-deployed local on-cluster Docker Registry. See also the related dashboard enhancements and the option to configure a Docker Registry service for the Nuclio service.\n  Security and User Management NoteFor security improvements that are related specifically to Nuclio, see the Nuclio Serverless Framework enhancements.  Dashboard (UI) NoteFor changes to the Functions dashboard page, see the Nuclio Serverless Framework enhancements.    Modified the Services page to expose default Docker Registry and Nuclio services and add a Docker Registry user-service type. See the related Docker Registry and Nuclio managed-services enhancements.\n  Added a chat function in cloud environments to facilitate communication with Iguazio's support team.\n  Fixes The following issues were fixed in the current release:\nManaged Application Services Jupyter Notebook [Tech Preview]   The tutorial Jupyter notebooks aren't pre-deployed as part of platform installations without an internet connection (offline installations).\n  Deployment of a Jupyter Notebook or web-shell service for a running user whose name contains an uppercase letter might fail.\n  Zeppelin   The platform's default Spark resource configurations are overridden by Zeppelin's built-in configurations, potentially causing the Zeppelin notebook to consume of all available resources of the related Spark service.\n  Security and User Management   The platform doesn't prevent deletion of a user who is the running user of platform services; such deletions can potentially disrupt the platform's application-services management.\n  Dashboard cookies aren't marked as secure when transmitted over HTTPS.\n  Some platform APIs might return user input, which could potentially be used for cross-site scripting (XSS).\n  The web-APIs service doesn't implement the HTTP Strict Transport Security (HSTS) policy.\n  HTTP responses are missing the X-Frame-Options header.\n  The TLS 1.0 protocol, which is considered non-compliant by the PCI Security Standards Council, is enabled in the web-APIs service.\n  HTTP requests aren't restricted to the same origin domain, which could potentially result in forbidden cross-domain requests.\n  The version of the web-APIs service is returned in the service's HTTP responses.\n  Known Issues Managed Application Services   Service dependency changes require restarting the dependent service. For example, if you change the Spark configuration for a web-based shell, Jupyter Notebook, or Zeppelin service, you need to restart the modified service after applying the configuration changes.\n  A user whose username was previously in use in the platform cannot run application services.\n  Service display names and descriptions cannot contain apostrophes (').\n  Web APIs Simple-Object Web API   The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  DescribeStream returns stream metadata regardless of the user's access permissions.\n  Jupyter Notebook [Tech Preview]   Running Scala code from a Jupyter notebook [Tech Preview] isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  Zeppelin   A Zeppelin service that isn't configured to work with a Spark service can't be deployed.\n   Spark Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Time-Series Databases (TSDBs)   SQL queries with prev interpolation might not return data for the last data point when no newer sample data is available.\n  TSDB CLI   The TSDB CLI help text for the create command's -r|--ingestion-rate flag refers to an ingestion-rate unit of days (d) instead of seconds (s). V3IO TSDB in the current platform release supports ingestion-rate units of seconds (s), minutes (m), and hours (h).\n  TSDB Nuclio Functions   It's possible to successfully create a TSDB Nuclio functions service for a non-existent TSDB table.\n  Prometheus   It's possible to successfully create a Prometheus service for a non-existent TSDB table.\n  Nuclio Serverless Framework   Function deployment for the Ruby, Java, and Node.js runtimes isn't supported for platform installations without an internet connection (offline installations).\n  Function-configuration build commands that contain apostrophes (') and are defined in the platform dashboard might fail to run.\n  Docker Registry   You cannot concurrently deploy more than one remote off-cluster Docker Registry service.\n  Hadoop   A recursive directory remove command (rm -r) that's issued without proper user permissions hangs instead of returning a relevant error.\n  The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.  The truncate command is supported as Tech Preview.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   In case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n   The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Security and User Management   Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The \u0026quot;category\u0026quot; criterion of data-access policy rules is ignored for the NoSQL Web API GetItem operation.\n  Access-token authentication ignores user-group permissions.\n  The internal name of the web-APIs service is returned in the service's HTTP responses.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI) NoteFor known issues related to the dashboard's Functions Nuclio serverless functions page, see the Nuclio known issues.    The dashboard might occasionally display an unreachable server or site error, specifically after an expired browser session, which can be resolved only by refreshing the browser page.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  When logging into a service as a different user than the logged in dashboard user, previously opened dashboard tabs need to be manually refreshed to reflect the new user.\n  It's possible to edit the memory and CPU resources of the default Nuclio service from the dashboard's Common Parameters service tab. Users should refrain from editing the resources of the Nuclio service.\n  When creating a new directory, spaces at the start or end of the directory name are automatically trimmed. Consequently, it's also not possible to create from the dashboard a directory whose name contains only spaces.\n  Notes   Platform start-up and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.1/v2.1.0/","title":"Version 2.1.0 Release Notes"},{"content":"This section contains the release notes for version 2.10 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.10/","title":"Version 2.10 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.10.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Highlights Version 2.10.0 introduces many new powerful features and enhancements, as well as bug fixes, as detailed in the release notes. Following are some of the main highlights of this release:\n  Upgraded the MLRun version, including these enhancements [Tech Preview] —\n New job-creation dashboard interface, including marketplace functions selection and support for advanced configuration Enhanced the hyperparameters support, including a new tuning_strategy run parameter for determining the tuning algorithm — list, grid search, or random search New method for loading a function as an inline Python module (function_to_module)    Upgraded the Frames version, including these enhancements —\n Automatic command-history logging and retrieval method (history) for troubleshooting Support for writing partitioned NoSQL tables [Tech Preview]    Support for filtering performance reports by interface in the dashboard's data-container overview\n  Support for the Spark cluster deploy mode\n  Support for the V3IO Python SDK (v3io-py) NoSQL and streaming client APIS\n  Enhanced security with stricter password restrictions and support for API-gateway access-key authentication\n  Support for adding an Oracle Presto connector [Tech Preview]\n  New Features and Enhancements Application Services | Dashboard (UI | Frames | General | Grafana | Horovod | Jupyter | Logging and Monitoring Services | MLRun | MPI Operator | Nuclio | Presto | Prometheus | Python SDK | Security | Spark | TSDB | TSDB Nuclio Functions | User Management | Web Shell\nGeneral   Added a __last_sequence_num system attribute that stores the sequence number of the last record in a stream shard; applicable only to stream-shard objects.\n  Managed Application Services NoteSee also the dashboard new features and enhancements for UI improvements related to managing application services.  Jupyter Notebook   Pre-deployed version 0.3.20 of the platform's Python SDK — the Iguazio V3IO Python SDK (v3io-py) — as part of the Jupyter Notebook service.\n  Upgraded versions of pre-deployed and certified software —\n Python version 3.7  Nuclio Jupyter library (nuclio-jupyter) version 0.8.5  Platform tutorial Jupyter notebooks — version 2.10.5  V3IO Frames client — version 0.7.36     Disabled the Scale to zero option by default for new Jupyter Notebook services. You can still select to enable this option from the Services dashboard, at any stage.\n  Blocked moving (renaming) of directories with nested directories from the Jupyter UI, to prevent possible data loss as a result of the recursive copy that's used to implement the move. You can still move such directories, when necessary, by running recursive file-system copy and delete commands from a command line. For more information, see the Jupyter Notebook directory copy and move software restrictions.\n  V3IO Python SDK   Added support for the NoSQL (key-value) and streaming client APIs of the platform's Python SDK — the Iguazio V3IO Python SDK (v3io-py), version 0.3.20. The SDK is also pre-deployed in the platform's Jupyter Notebook service (see the Jupyter Notebook enhancements).\n  V3IO Frames   Upgraded to Frames server version 0.7.37 and client version 0.7.36.\n  Added automatic logging of Frames operations (commands history), and a history method for retrieving information from the history logs.\n  Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   [Tech Preview] Added support for writing partitioned tables.\n  MLRun [Tech Preview]   Upgraded to MLRun version 0.5.1. For future upgrades to newer MLRun releases, consult Iguazio's support team.\n  Added a default (pre-deployed) shared single-instance tenant-wide MLRun service (mlrun) to on-prem deployments (VM and bare-metal). (Previously, this service was pre-deployed only in the cloud.)\n  Added a new job-creation interface to the MLRun dashboard, including marketplace functions selection and support for advanced configuration.\n  Added support for Git and Slack notifications.\n  Enhanced the hyperparameters support: added a new tuning_strategy run parameter for determining the tuning algorithm — list, grid search, or random search. You can also provide the input as a file.\n  Added a function_to_module method for loading a function as an inline Python module.\n  Simplified and improved remote access and submission of jobs and workflows.\n  Added support for Kubeflow v1 MPI Operator for distributed machine learning using Horovod.\n  Added support for setting the log level for each run.\n  Improved the scheduling mechanism.\n  Nuclio Serverless Functions   Upgraded to Nuclio version 1.4.17.\n  Added support for cross-origin resource sharing (CORS) for Nuclio functions, allowing access to selected web-page resources from a different origin (domain) than that on which the resources are being served.\n  Upgraded to Go version 1.4.3, including the use of Go modules to manage module dependencies. NoteGo modules require an internet connection for downloading module files. Therefore, building Go functions in an air-gapped environment without an internet connection (a.k.a. \u0026quot;a dark site\u0026quot;) requires special treatment. See additional information in the Nuclio software specifications and restrictions.    Kafka trigger (kafka-cluster) — added Kafka headers and timestamps to the event information.\n  TSDB Nuclio Functions   Upgraded to V3IO TSDB Nuclio Functions version 0.5.11.\n  Time-Series Databases (TSDBs)   Upgraded to V3IO TSDB version version 0.10.12.\n  Prometheus   Upgraded to V3IO Prometheus version 3.3.6.\n  Presto   Added execution-plan information for SQL queries on platform NoSQL tables to the Presto coordinator logs.\n   [Tech Preview] Added support for adding an Oracle connector. For details, contact Iguazio's support team.\n  Spark   Added support for the Spark cluster deploy mode for submitting Spark jobs by adding --deploy-mode=cluster to the spark-submit call. This mode is supported for Scala and Java; Spark doesn't currently support Python in standalone clusters. In the cluster mode, the driver is launched from a worker process in the cluster, while in the default client deploy mode the driver is launched in the same worker process as the client that submits the application (such as Jupyter Notebook, a web shell, or Zeppelin). Cluster deployment provides advantages such as the ability to automate jobs execution and run Spark jobs remotely on the cluster — which is useful, for example, for running ongoing Spark jobs, such as streaming.\n  Enabled spark workers to clean up old logs after a week.\n  Web Shell   Upgraded to Python version 3.7.\n  Horovod / MPI Operator   Upgraded the mpi-operator Horovod service to Kubeflow MPI Operator version 0.2.3.\n  Logging and Monitoring Services   [Tech Preview] Added support for sending Grafana email alerts via SMTP.\n  File System   Newly created data containers can now be accessed using local file-system commands without restarting the command-line service (such as a web shell or Jupyter Notebook).\n  Security and User Management   Enforced stricter password restrictions. User passwords in v2.10.0 must conform to the following restrictions:\n Contain at least one uppercase letter (A–Z) Contain at least one lowercase letter (a–-z) Contain at least one special character (!, @, #, $, %, ^, \u0026amp;, *) Contain at least one digit (0–9) Length of at least 8 characters    Dashboard (UI)   In the Data container overview, added support for filtering the container performance reports by API interface.\n  Added a Help page with getting-started and additional information to facilitate the user experience.\n  Added support for IT administrators to switch between read-only and read-write cluster data-access modes.\n  On the API Gateway project tab, added an Access Key authentication type for access-key authentication.\n  On the Projects project Configuration tab, added consumer-group configuration options for the Nuclio Kafka trigger (kafka-cluster) and the platform trigger (v3ioStream).\n  Added a Last sequence field to the General object browse metadata for displaying the sequence number of the last record in a stream shard; applicable only to stream-shard objects.\n  On the SMTP settings page, added an optional Users to notify parameter for specifying users with the IT Admin management policy who will receive alert email notifications for the cluster. In the current release, this option is available only to users who have both the IT Admin and Tenant Admin management policies, such as the predefined tenancy_admin user; see the related known issue.   Display a visible message on the dashboard when the cluster is not in a normal online operational status.\n  Fixes Application Services | Frames | Jupyter | Logging and Monitoring Services | MLRun | Nuclio | Presto\nManaged Application Services   Fixed assignment of service-administration permissions to users who are members of a user group with the Service Admin management policy but aren't assigned this policy directly. Previously, such users could see only their services on the Services dashboard page, and any changes that they made to their services impacted other users' services.\n  Jupyter Notebook   Allowed a running user of a scaled-to-zero Jupyter Notebook service to wake up the service by selecting its name on the Services dashboard page, even when the user doesn't have the Service Admin management policy.\n  Fixed concurrent execution of multiple notebooks by the same running user.\n  V3IO Frames   Removed the default value for the read client method's mandatory backend parameter.\n  Frames TSDB Backend (\u0026quot;tsdb\u0026quot;/\u0026quot; Backend)   Setting the delete client method's metrics-name (metrics) or filter-expression (filter) parameter without a time-filter parameter (start and/or end) now applies the correct filter instead of deleting the entire table.\n  MLRun [Tech Preview]   The MLRun service can now be disabled from the dashboard.\n  Nuclio Serverless Functions   Changes to a Nuclio function's volume configuration from the dashboard (see the Volumes section of the function Configuration tab) are now reflected immediately in the UI, without the need to first collapse the volume entry in the dashboard.\n  Presto   It's now possible to edit a configuration file with the same name for different types of resources (namely, coordinator and worker).\n  Logging and Monitoring Services   Keeping the log-forwarder service enabled over time no longer causes extensive memory consumption or disruption of other application services.\n  Changes to the log-forwarder resource configuration that are done from the dashboard are no longer ignored.\n  Known Issues Application Services | Backup and Recovery | Dashboard (UI | File System | Frames | Hadoop | High Availability (HA)| Hive | Jupyter | Nuclio | Presto | Prometheus | Spark | TSDB | Web APIs\nManaged Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  You cannot currently delete directories from the Jupyter UI. You can still delete directories by running a file-system command in a command-line interface, such as a Jupyter terminal or notebook or a web shell.\n  The Services dashboard page displays an incorrect version for the Jupyter Notebook service (version 1.0.2 instead of the installed JupyterLab version 1.0.10).\n  V3IO Frames Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   The write client method's \u0026quot;overwriteTable\u0026quot; save mode isn't supported for partitioned tables [Tech Preview].\n  Nuclio Serverless Functions   Deploying a Nuclio function named dashboard renders the Projects dashboard page (the Nuclio dashboard) inaccessible. To avoid this, refrain from naming Nuclio functions dashboard.\n  Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  Time-Series Databases (TSDBs) Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n   Invalid Presto configuration files or connector definitions might cause the Presto service and all dependent services to fail. This is the expected behavior. To recover, delete the problematic configuration or manually revert relevant changes to the default Presto configuration files, save the changes to the Presto service, and select Apply Changes on the dashboard Services page.   Running the Hive CLI from a Jupyter Notebook service when there's no web-shell service in the cluster might fail if another user had previously run the CLI from another instance of Jupyter. To resolve this, ensure that there's at least one web-shell service in the cluster.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Web APIs Simple-Object Web API   GET Object and HEAD Object don't return correct ETag and Content-Type metadata.\n  A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup, Recovery, and High Availability (HA)   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Dashboard (UI) NoteSee the Nuclio known issues for Nuclio-specific dashboard issues.    On the SMTP settings page, you might see an \u0026quot;Error: Failed to fetch user list (you can still update the rest)\u0026quot; error message, and the Users to notify field might be disabled. However, you can still configure SMTP without email notifications. To avoid the error and use the Users to notify parameter to define IT Admin users who'll receive cluster-alert email notifications, use a user with the IT Admin and Tenant Admin management policies, such as the predefined tenancy_admin user.   When browsing a NoSQL table with an attribute named closed, the Table view is distorted.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  Notes   Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.10/v2.10.0/","title":"Version 2.10.0 Release Notes"},{"content":"This section contains the release notes for version 2.2 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.2/","title":"Version 2.2 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.2.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements Managed Application Services   Enhanced service status monitoring and reporting.\n  Web APIs   Improved error handling by rephrasing existing messages and adding messages for a broader set of operations.\n  Jupyter Notebook   Added official support for the Jupyter Notebook service (previously supported as Tech Preview).\n  Extended the custom parameters of the Jupyter Notebook service to allow users to select which Jupyter flavor (Docker image) to use, and to optionally define environment variables.\n  Web Shell   Added the kubectl command-line interface (CLI) to the platform's web-shell service to allow users to run commands against their platform's Kubernetes application clusters. The operations that can be performed with kubectl are restricted according to the permissions set in the new service-accounts parameter of the web-shell service. See also the Kubernetes service-accounts security note.\n  Presto and Hive   Added support for running SQL queries using built-in Presto and Apache Hive connectors, such as Parquet or ORC. Users can enable the Hive support from the custom parameters of the Presto service, and run SQL queries on the supported file formats from a Jupyter Notebook or web-shell service.   Time-Series Databases (TSDBs)   Added support for label pre-aggregation, which significantly accelerates group-by queries on time-series data.\n  TSDB Nuclio Functions   Added support for ingesting multiple metrics and labels in a single operation.\n  Grafana   Added an \u0026quot;iguazio\u0026quot; Grafana data source for accessing and visualizing data in the platform's data containers from a Grafana dashboard. This data source is defined only when the new optional username parameter of the Grafana service is set to a platform username, which will be used by the \u0026quot;iguazio\u0026quot; data source.\n  Logging Services   In cloud environments, improved the configuration of a local on-cluster Elasticsearch service for the log-forwarder service.\n  Security and User Management   Added Kubernetes service accounts for running kubectl from a web-shell service — \u0026quot;None\u0026quot;, \u0026quot;Log Reader\u0026quot;, \u0026quot;Application Admin\u0026quot;, and \u0026quot;Service Admin\u0026quot;. See also the kubectl web-shell note.\n  Dashboard (UI) NoteSee also the dashboard enhancements in the Managed Application Services new features and enhancements.    Added a pop-up welcome page for initial login guidance.\n  Added information about the platform's application clusters to the Clusters dashboard page.\n  Extended the folder (directory) metadata in the Browse container-data tab to add the name of the user who created the folder.\n  Added a Capacity Over Time graph to the Data \u0026gt; \u0026lt;container\u0026gt; \u0026gt; Overview dashboard tab.\n  Improved the design of the action toolbar of the Data Access Policy container data tab.\n  Support   Added application-node configuration information to the data that is sent to the Iguazio service center.\n  Fixes The following issues were fixed in the current release:\nManaged Application Services   Service dependency changes require restarting the dependent service. For example, if you change the Spark configuration for a web-based shell, Jupyter Notebook, or Zeppelin service, you need to restart the modified service after applying the configuration changes.\n  Service display names and descriptions cannot contain apostrophes (').\n  Time-Series Databases (TSDBs) TSDB Nuclio Functions   It's possible to successfully create a TSDB Nuclio functions service for a non-existent TSDB table.\n  Prometheus   It's possible to successfully create a Prometheus service for a non-existent TSDB table.\n  Nuclio Serverless Framework   Function-configuration build commands that contain apostrophes (') and are defined in the platform dashboard might fail to run.\n  Docker Registry   You cannot concurrently deploy more than one remote off-cluster Docker Registry service.\n  Hadoop   A recursive directory remove command (hadoop fs rm -r) that's issued without proper user permissions hangs instead of returning a relevant error.\n  Security and User Management   Access-token authentication ignores user-group permissions.\n  The name of the web-APIs server is returned in the service's HTTP responses.\n  Known Issues Managed Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Web APIs Simple-Object Web API   The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  Presto and Hive   To change the Hive configuration parameters of the Presto service (such as changing the path to the MySQL DB), you need to first disable Hive for Presto, apply your changes, re-enable Hive for Presto and set your new Hive configuration, and apply your changes.\n  You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user), subject to the Hive reconfiguration restrictions; or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n  A NoSQL-table view can be used only by the user who created the view.\n  After enabling Hive for the Presto service, existing web-shell and Jupyter Notebook services need to be restarted in order to use Hive.\n  Spark Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Time-Series Databases (TSDBs)   SQL queries with prev interpolation might not return data for the last data point when no newer sample data is available.\n  TSDB CLI   The TSDB CLI help text for the create command's -r|--ingestion-rate flag refers to an ingestion-rate unit of days (d) instead of seconds (s). V3IO TSDB in the current platform release supports ingestion-rate units of seconds (s), minutes (m), and hours (h).\n  Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Nuclio Serverless Framework   Function deployment for the Ruby, Java, and Node.js runtimes isn't supported for platform installations without an internet connection (offline installations).\n  Changes to an input field on the Functions dashboard page might not be applied if the field is focused when selecting to deploy the function.\n  Deploying a function from the dashboard fails when the memory resource is set in KB.\n  After failing to deploy a function from the dashboard using the Archive code-entry type because of an invalid URL parameter, subsequent deployments also fail even after the URL is fixed.\n  The default Readiness timeout value in the dashboard's function build configuration (0) doesn't reflect the actual default timeout (30 seconds).\n  V3IO Frames [Tech Preview]   User authentication for TSDB table-creation commands fails. Use the TSDB CLI to create TSDB tables.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.  The truncate command is supported as Tech Preview.\n  Logging Services   In cloud environments, it's possible to delete the on-cluster Elasticsearch service while it's being used by the log-forwarder service (despite the related deployment error).\n  When the log-forwarder service is in the \u0026quot;Disabled\u0026quot; or \u0026quot;Failed\u0026quot; state, the Logs dashboard page is displayed with an unknown error.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   In case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n   The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Security and User Management   Data-access policy permissions are checked only for the current directory and not for the entire path.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI) NoteSee also the Nuclio and Logging Services dashboard known issues.    The dashboard might occasionally display an unreachable server or site error, specifically after an expired browser session, which can be resolved only by refreshing the browser page.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  When logging into a service as a different user than the logged in dashboard user, previously opened dashboard tabs need to be manually refreshed to reflect the new user.\n  It's possible to edit the memory and CPU resources of the default Nuclio service from the dashboard's Common Parameters service tab. Users should refrain from editing the resources of the Nuclio service.\n  When creating a new directory, spaces at the start or end of the directory name are automatically trimmed. Consequently, it's also not possible to create from the dashboard a directory whose name contains only spaces.\n  The dashboard doesn't prevent creating a Zeppelin service without a Spark-service configuration, even though such a service will fail to deploy.   Creation of a Spark service from the configuration of another service (Jupyter Notebook / Zeppelin / web shell) occasionally fails.\n  Notes   Platform start-up and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.2/v2.2.0/","title":"Version 2.2.0 Release Notes"},{"content":"This section contains the release notes for version 2.3 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.3/","title":"Version 2.3 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.3.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements General   V2.3.0 is the general-availability (GA) release of Iguazio MLOps Platform for the Amazon Web Services (AWS) cloud.\n  Managed Application Services Web APIs   Added support for authenticating web-API requests by using the authentication syntax of the Amazon Simple Storage Service (S3) API with a platform access key.\n  Jupyter Notebook   Raised the preinstalled version of the platform tutorial Jupyter notebooks to v2.3.3.\n  Added GPU support for Jupyter Notebook [Tech Preview]. The support includes the option to use the Horovod open-source framework to run distributed deep-learning jobs, at scale, over NVIDIA GPUs, or to use NVIDIA's RAPIDS open-source libraries suite to run ML models over GPU. For more information and assistance, contact Iguazio Support.\n  Presto   Optimized performance of NoSQL table queries.\n  Spark Spark NoSQL DataFrame   Optimized performance of NoSQL table queries.\n  Time-Series Databases (TSDBs) TSDB CLI   Raised the preinstalled version of the V3IO TSDB CLI (tsdbctl) to v0.9.5.\n  TSDB Nuclio Functions   Raised the preinstalled version of the V3IO TSDB Nuclio functions to v0.2.1-v0.9.5.   V3IO Frames [Tech Preview]   Raised the preinstalled version of the V3IO Frames service to v0.5.8-v0.9.5.\n  Added an aggregationWindow parameter to the client.read DataFrame method to support TSDB sliding-window aggregation queries in which the aggregation window differs from the data interval [Tech Preview].\n  Nuclio Serverless Functions   Raised the preinstalled version of the Nuclio serverless framework to 1.1.11.   Added a new S3 code-entry type for retrieving function code from an Amazon S3 bucket, including support for S3 authentication.\n  Logging and Monitoring Services   Added a default tenant-wide \u0026quot;Monitoring\u0026quot; service for viewing performance statistics of serverless Nuclio functions from a Grafana dashboard. To view the logs — from the dashboard Services page, enable the \u0026quot;Monitoring\u0026quot; service (which is disabled by default); create a Grafana service; and select the Nuclio Dashboard UI link for the \u0026quot;Monitoring\u0026quot; service to view the auto-generated Nuclio Grafana dashboard. (The Nuclio dashboard can be viewed from the UI of any active platform Grafana service.)\n  Dashboard (UI) NoteSee the dashboard enhancements in the Managed Application Services new features and enhancements.    Implemented miscellaneous cosmetic and text improvements to the dashboard.\n  Added the ability for IT administrators to collect and download support logs for the platform clusters from the dashboard. Log collection can be triggered for a data cluster from the Clusters home page or from the new data-cluster Support Logs tab, which also displays running and completed log-collection tasks and allows you to download logs for specific data nodes.\n  Fixes The following issues were fixed in the current release:\nManaged Application Services Presto and Hive   To change the Hive configuration parameters of the Presto service (such as changing the path to the MySQL DB), you need to first disable Hive for Presto, apply your changes, re-enable Hive for Presto and set your new Hive configuration, and apply your changes.\n  After enabling Hive for the Presto service, existing web-shell and Jupyter Notebook services need to be restarted in order to use Hive.\n  Time-Series Databases (TSDBs) TSDB CLI   The TSDB CLI help text for the create command's -r|--ingestion-rate flag refers to an ingestion-rate unit of days (d) instead of seconds (s). V3IO TSDB in the current platform release supports ingestion-rate units of seconds (s), minutes (m), and hours (h).\n  Nuclio Serverless Functions   Changes to an input field on the Functions dashboard page might not be applied if the field is focused when selecting to deploy the function.\n  Deploying a function from the dashboard fails when the memory resource is set in KB.\n  After failing to deploy a function from the dashboard using the Archive code-entry type because of an invalid URL parameter, subsequent deployments also fail even after the URL is fixed.\n  The default Readiness timeout value in the dashboard's function build configuration (0) doesn't reflect the actual default timeout.\n  V3IO Frames [Tech Preview]   User authentication for TSDB table-creation commands fails. Use the TSDB CLI to create TSDB tables.\n  Logging Services   In cloud environments, it's possible to delete the on-cluster Elasticsearch service while it's being used by the log-forwarder service (despite the related deployment error).\n  When the log-forwarder service is in the \u0026quot;Disabled\u0026quot; or \u0026quot;Failed\u0026quot; state, the Logs dashboard page is displayed with an unknown error.\n  High Availability (HA)   In non-cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  Platform Management   Deletion of very large containers, which contain hundreds of millions of objects, might fail.\n  Dashboard (UI) NoteFor Functions dashboard-page fixes, see the Nuclio Serverless Functions fixes.    When creating a new directory, spaces at the start or end of the directory name are automatically trimmed. Consequently, it's also not possible to create from the dashboard a directory whose name contains only spaces.\n  Creation of a Spark service from the configuration of another service (Jupyter Notebook / Zeppelin / web shell) occasionally fails.\n  The dashboard doesn't prevent creating a Zeppelin service without a Spark-service configuration, even though such a service will fail to deploy.   Known Issues Managed Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Web APIs Simple-Object Web API   The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  Successful deployment of a Jupyter Notebook service with the Jupyter Deep Learning with Rapids flavor [Tech Preview] requires preliminary installation. Contact Iguazio Support before attempting to use this flavor.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.   A NoSQL-table view can be used only by the user who created the view.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Time-Series Databases (TSDBs)   SQL queries with prev interpolation might not return data for the last data point when no newer sample data is available.\n  Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Nuclio Serverless Functions   Function deployment for the Ruby, Java, and Node.js runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  V3IO Frames [Tech Preview]   Item attributes (DataFrame columns) of type blob are not supported.\n  Frames NoSQL DataFrame (\u0026quot;kv\u0026quot; Backend)   When writing a DataFrame, the index column (primary-key attribute) isn't written to the table as a user attribute and therefore isn't returned when reading from the table with another API, such as Spark DataFrames or Presto.\n  pandas datetime item attributes that are written with Frames cannot be read with Spark DataFrames or Presto.\n  Timestamp item attributes that are written with a Spark DataFrame aren't returned when reading the table with Frames.\n  When writing a DataFrame without explicitly setting the index column, the item names in the table are set incorrectly to the values of the first regular DataFrame column (user attribute) and the table doesn't contain a primary-key user attribute.\n  When inferring the table schema without setting the key parameter of the client execute infer command, the schema key isn't set to the name of the table's primary-key attribute. Consequently, the table can't be read with Spark DataFrames or Presto, and reading the table with Frames returns an extra __name attribute (column), in addition to the primary-key attribute that contains the same values.\n  Integer attribute values are read by Frames as floating-point numbers.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.  The truncate command is supported as Tech Preview.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   In cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Security and User Management   Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The data-access policy rule Sources match condition, for specifying interface or network sources, isn't supported.\n  Platform Management   Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI) NoteSee the Nuclio known issues for Nuclio-specific dashboard issues.    The dashboard might occasionally display an unreachable server or site error, specifically after an expired browser session, which can be resolved only by refreshing the browser page.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  When logging into a service as a different user than the logged in dashboard user, previously opened dashboard tabs need to be manually refreshed to reflect the new user.\n  It's possible to edit the memory and CPU resources of the default Nuclio and Monitoring services from the dashboard's Common Parameters service tab. Users should refrain from editing the resources of these services.\n  An item attribute whose names contains an underscore (_) cannot be edited from the dashboard.\n  Notes   Platform start-up and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.3/v2.3.0/","title":"Version 2.3.0 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.3.1, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements Managed Application Services Jupyter Notebook   Raised the preinstalled version of the platform tutorial Jupyter notebooks to v2.3.4.\n  Time-Series Databases (TSDBs) TSDB CLI   Raised the preinstalled version of the V3IO TSDB library and CLI (tsdbctl) to v0.9.6.\n  TSDB Nuclio Functions   Raised the preinstalled version of the V3IO TSDB Nuclio functions to v0.2.1-v0.9.6.\n  Prometheus   Raised the preinstalled version of the V3IO Prometheus distribution to v2.8.0-igz3-v0.9.6.\n  V3IO Frames [Tech Preview]   Raised the preinstalled version of the V3IO Frames service to v0.5.9-v0.9.6.\n  Fixes The following issues were fixed in the current release:\nManaged Application Services   Optimized service-configuration handling.\n  V3IO Frames [Tech Preview] Frames NoSQL DataFrame (\u0026quot;kv\u0026quot; Backend)   There's a typo in the name of the Client read method's aggragators parameter. In Frames v0.5.9-v0.9.6 / platform v2.3.1, the parameter was renamed to aggregators.\n  When writing a DataFrame, the index column (primary-key attribute) isn't written to the table as a user attribute and therefore isn't returned when reading from the table with another API, such as Spark DataFrames or Presto.\n  When writing a DataFrame without explicitly setting the index column, the item names in the table are set incorrectly to the values of the first regular DataFrame column (user attribute) and the table doesn't contain a primary-key user attribute.\n  Integer attribute values are read by Frames as floating-point numbers.\n  Dashboard (UI)   An item attribute whose names contains an underscore (_) cannot be edited from the dashboard.\n  Known Issues Managed Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Web APIs Simple-Object Web API   The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  Successful deployment of a Jupyter Notebook service with the Jupyter Deep Learning with Rapids flavor [Tech Preview] requires preliminary installation. Contact Iguazio Support before attempting to use this flavor.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n  A Presto Hive view of a NoSQL table can be used only by the user who created the view.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Time-Series Databases (TSDBs)   SQL queries with prev interpolation might not return data for the last data point when no newer sample data is available.\n  Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Nuclio Serverless Functions   Function deployment for the Ruby, Java, and Node.js runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  V3IO Frames [Tech Preview]   Item attributes (DataFrame columns) of type blob are not supported.\n  Frames NoSQL DataFrame (\u0026quot;kv\u0026quot; Backend)   pandas datetime item attributes that are written with Frames cannot be read with Spark DataFrames or Presto.\n  Timestamp item attributes that are written with a Spark DataFrame aren't returned when reading the table with Frames.\n  When inferring the table schema without setting the key parameter of the client execute infer command, the schema key isn't set to the name of the table's primary-key attribute. Consequently, the table can't be read with Spark DataFrames or Presto, and reading the table with Frames returns an extra __name attribute (column), in addition to the primary-key attribute that contains the same values.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.  The truncate command is supported as Tech Preview.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup and Recovery   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  High Availability (HA)   In cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Security and User Management   Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The data-access policy rule Sources match condition, for specifying interface or network sources, isn't supported.\n  Platform Management   Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI) NoteSee the Nuclio known issues for Nuclio-specific dashboard issues.    In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  When logging into a service as a different user than the logged in dashboard user, previously opened dashboard tabs need to be manually refreshed to reflect the new user.\n  It's possible to edit the memory and CPU resources of the default Nuclio and Monitoring services from the dashboard's Common Parameters service tab. Users should refrain from editing the resources of these services.\n  Notes   Platform start-up and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.3/v2.3.1/","title":"Version 2.3.1 Release Notes"},{"content":" This section contains the release notes for version 2.5 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.5/","title":"Version 2.5 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.5.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  New Features and Enhancements Application Services | Backup and Recovery | Dashboard (UI | Frames | General | High Availability (HA) | Horovod | Jupyter Notebook | Logging and Monitoring Services | Nuclio | Pipelines | Presto | TSDB Nuclio Functions | Web APIs | Web Shell\nGeneral   Upgraded to CentOS version 7.6.1810.\n  Managed Application Services NoteSee also the dashboard new features and enhancements for UI improvements related to managing application services.    Upgraded to Kubernetes version 1.13.5 and Helm version 2.14.1.\n  Web APIs Symbolic-Link Web API   Added web-API support for symbolic links:\n  Added a Symbolic-Link Web API to the data-service web APIs: the X-v3io-function request header now supports a CreateSymlink operation for creating a symbolic link to a platform file-system data path (a file or directory a platform data container). Such links can then be used from a file-system interface (for example, from a web-shell or Jupyter Notebook service).\n  Added support for retrieving a platform file-system symbolic link by using the GET Object operation of the Simple-Object Web API. A new X-v3io-object-type response header indicates the type of the returned object — regular for a regular data path and symlink for symbolic links.\n    Jupyter Notebook   Upgraded versions of preinstalled software —\n JupyterLab — version 1.0.2 Jupyter Notebook — version 6.0.01 Conda — version 4.7.9 Nuclio Jupyter package — version v0.7.6 Platform tutorial Jupyter notebooks — version v2.5.5    Added official support for running GPU applications that use the open-source Horovod or RAPIDS libraries from Jupyter Notebook (previously supported only as Tech Preview). The Horovod support uses the new Horovod platform service.\n  Consolidated the Jupyter flavors into two flavors — Jupyter Full Stack for CPU execution and Jupyter Full Stack with GPU for GPU execution.\n  Added support for monitoring the pod resource consumption of your Kubernetes application cluster by running the kubectl top pod command from a Jupyter Notebook service. See also the similar web-shell support.\n  Web Shell   Added support for monitoring the pod resource consumption of your Kubernetes application cluster by running the kubectl top pod command from a web-shell service with the \u0026quot;Service Admin\u0026quot; service account. See also the similar Jupyter Notebook support.\n  Added the vi text editor to the web shell service to enable file editing from the shell command line.\n  Presto   Added support for configuring Presto from the platform dashboard — including editing the default Presto and pre-deployed connector configuration properties and adding more connectors from the Presto distribution. To edit the configuration, select the Presto service from the Services dashboard page, navigate to the Custom Parameters tab, and expand the Advanced section.\n  V3IO Frames [Tech Preview]   Upgraded to V3IO Frames version v0.6.5-v0.9.10.\n  Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   Added support for Boolean item attributes.\n  Added support for conditional writes by setting the condition parameter of the write client method to a condition expression.\n  Nuclio Serverless Functions   Upgraded to Nuclio version 1.1.20.\n  Added a Nuclio Java SDK artifact to the Maven central repository.\n  Added real-time performance statistics for deployed Nuclio functions to the project page of the Nuclio dashboard, including number of invocations and CPU and memory consumption. Note that you must enable the monitoring service of the parent tenant to display updated performance data.\n  Added support for auto and zero scaling of deployed Nuclio functions. Note that in the current release, functions that have been scaled to zero can only be awoken by using an HTTP trigger, and that support for auto scaling is available only when the monitoring service of the parent tenant is enabled.\n   Improved the procedure for duplicating a Nuclio function from the dashboard.\n  TSDB Nuclio Functions   Added the option to copy the API URLs of the ingestion and query functions of a TSDB Nuclio functions service from the API column on the Services dashboard page.\n  Pipelines   Added a default tenant-wide \u0026quot;Pipelines\u0026quot; platform service for using the open-source Google Kubeflow Pipelines framework for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.\n   Horovod   Added a default tenant-wide \u0026quot;Horovod\u0026quot; platform service for using Uber's Horovod open-source framework to run distributed deep-learning jobs, at scale, over NVIDIA GPUs (previously supported only as Tech Preview and without a dedicated service). See also the Jupyter Notebook GPU enhancements.\n  Upgraded to Horovod version 0.2.0.\n  Logging and Monitoring Services   The monitoring service (\u0026quot;Monitoring\u0026quot;) now monitors and collects performance data for all of the platform application services and not just for Nuclio. The collected data is displayed in auto-generated Grafana dashboards: the Nuclio dashboard was renamed to Nuclio Functions Monitoring - Overview and new Application Services Monitoring and Nuclio Functions Monitoring - Per Function dashboards were added. To view these Grafana dashboards, enable the monitoring service, create a Grafana service, and then open the UI of your Grafana service and select the relevant dashboard.\n  Backup, Recovery, and High Availability (HA)   Added support for online failback upon data-node failure.\n  Dashboard (UI)   Added a Table view option to the container-data Browse tab for displaying the contents of a directory as a table. This option is especially relevant for NoSQL tables and is applied by default when browsing a directory that has a table schema file (.#schema).\n  Added a new filter option to the container-data Browse tab for running custom filter expressions.\n  Added real-time performance statistics for deployed application services; see the new CPU (cores) and Memory columns on the Services page. Note that you must enable the monitoring service of the parent tenant to display updated performance data.\n  Implemented miscellaneous improvements to the Services page, including the following:\n Replaced the UI column with service-name links in the Name column. Removed the Description column. To view the description, hover over the service name in the Name column. Added direct links to service logs in the Logs column when the log-forwarder service is enabled.    The Logs dashboard page is now available also for bare-metal and VM deployments. Note that this page is available (in all environments) only for users with the Service Admin management policy and only when the log-forwarder service (\u0026quot;Log Forwarder\u0026quot;) is enabled.\n  When browsing a data container, the General tab of the directory-metadata panel now has a new Number of objects field for displaying the number of objects (items) in the directory (not including objects in subdirectories). Note that this information isn't returned when the cluster is in degraded mode.\n  Added a Scanned Items column to the containers overview on the Data dashboard page to display the average number of item read and delete operations per second.\n  Fixes The following issues were fixed in the current release:\nApplication Services | Dashboard (UI | Frames | Jupyter Notebook | Nuclio\nManaged Application Services Jupyter Notebook   Successful deployment of a Jupyter Notebook service with the Jupyter Deep Learning with Rapids flavor [Tech Preview] requires preliminary installation. In v2.5.0, RAPIDS is officially supported with the new Jupyter Full Stack with GPU Jupyter flavor.\n  Nuclio Serverless Functions   Function deployment for the Java runtimes isn't supported for platform installations without an internet connection (offline installations).\n  V3IO Frames [Tech Preview] Frames NoSQL Backend (\u0026quot;nosql\u0026quot/\u0026quot;kv\u0026quot;)   pandas datetime item attributes that are written with Frames cannot be read with Spark DataFrames or Presto.\n  Timestamp item attributes that are written with a Spark DataFrame aren't returned when reading the table with Frames.\n  Dashboard (UI)   When logging into a service as a different user than the logged in dashboard user, previously opened dashboard tabs need to be manually refreshed to reflect the new user.\n  It's possible to edit the memory and CPU resources of the default Nuclio and Monitoring services from the dashboard's Common Parameters service tab. In v2.5.0, the tab is disabled for these services.\n  Known Issues Application Services | Backup and Recovery | Dashboard (UI | File System | Frames | Hadoop | High Availability (HA)| Hive | Jupyter Notebook | Nuclio | Platform Management | Pipelines | Prometheus | Presto | Security | Spark| TSDB | TSDB Nuclio Functions | User Management | Web APIs\nManaged Application Services   Users who are members of a user group with the Service Admin management policy, but aren't assigned this policy directly, don't have proper service-administration permissions: such users can see only their services on the Services dashboard page, and any changes that they make to their services impact other users' services. To bypass this issue, either assign the Service Admin management policy directly to the users; remove the users from groups with this policy; or remove the policy from the relevant user groups.\n  A user whose username was previously in use in the platform cannot run application services.\n  Web APIs Simple-Object Web API   The GET Service operation for listing containers may sometimes return the container ID in place of the container name.\n   GET Object and HEAD Object operations don't return correct ETag and Content-Type metadata, and GET Object also doesn't return the correct Last-Modified metadata.\n  A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n  A Presto Hive view of a NoSQL table can be used only by the user who created the view.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Time-Series Databases (TSDBs)  NoteSee also the TSDB Nuclio functions section under the Nuclio known issues.    SQL queries with prev interpolation might not return data for the last data point when no newer sample data is available.\n  Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Nuclio Serverless Functions   Changes to a Nuclio function's volume configuration from the dashboard (see the Volumes section of the function Configuration tab) are applied only after collapsing the volume entry in the dashboard.\n  If you use the same container-image name for multiple Nuclio functions, the last deployed image might override a previous deployment of another function that uses the same image name.\n  Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  TSDB Nuclio Functions   It's possible to change the minimum and maximum replicas configuration for a TSDB Nuclio function from the Projects dashboard page, even though such changes aren't supported and aren't applied when the function is deployed.\n  Pipelines   Some of the sample pipelines in the Pipelines dashboard cannot be executed unless you modify the code to configure Google Cloud credentials.\n  V3IO Frames [Tech Preview]   Item attributes (DataFrame columns) of type blob are not supported.\n  Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   Null item-attribute values are replaced in the pandas DataFrame that's returned by the read method with the default value for the attribute type (for example, 0 for an integer attribute or false for a Boolean attribute). Note: Take care not to write the incorrect read values back to the table.\n  When inferring the table schema without setting the key parameter of the client execute infer command, the schema key isn't set to the name of the table's primary-key attribute. Consequently, the table can't be read with Spark DataFrames or Presto, and reading the table with Frames returns an extra __name attribute (column), in addition to the primary-key attribute that contains the same values.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup, Recovery, and High Availability (HA)   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  In cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Security and User Management   Data-access policy permissions are checked only for the current directory and not for the entire path.\n  The data-access policy rule Sources match condition, for specifying interface or network sources, isn't supported.\n  Platform Management   Create Container requests with a negative container ID may result in response status code 500 instead of 400 and a cumbersome error message.\n  Dashboard (UI) NoteSee the Nuclio known issues for Nuclio-specific dashboard issues.    It's not possible to make changes to object (item) attributes from the dashboard while there are unloaded attribute values. You can resolve this by selecting Load All to load all attribute values.\n  When entering a NoSQL table directory that contains a schema file (.#schema) from the container-data browse tab, the table-view load time might be relatively long.\n  When browsing a data container in a multi-node environment, the Number of objects value in the General tab of the directory-metadata panel is twice the actual number of objects in the directory.\n  When browsing a data container, if an object in a container directory is deleted while the directory metadata is displayed in the dashboard, the Number of objects value isn't updated when selecting the dashboard refresh button. To update the metadata information, close and reopen the metadata panel or select the browser refresh button (which automatically closes the information panel).\n  The performance columns on the Services page — CPU (cores) and Memory — show data for the last time at which the monitoring service was enabled or 0 if this service hasn't been enabled yet. Note that the monitoring service must be enabled to allow gathering performance data.\n  The CPU column on the Clusters page is missing the performance graph.\n  The action menu of the monitoring service might display broken links to Grafana monitoring dashboards. Note that you can still view the monitoring dashboards by selecting them from the Grafana UI.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  Notes   Platform start-up and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.5/v2.5.0/","title":"Version 2.5.0 Release Notes"},{"content":"This section contains the release notes for version 2.8 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.8/","title":"Version 2.8 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 2.8.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Highlights Version 2.8.0 introduces many new powerful features and enhancements, as well as bug fixes, as detailed in the release notes. Following are some of the main highlights of this release:\n  Enhanced support for MLRun [Tech Preview], including integration with MLRun as a service, allowing you to easily leverage MLRun's capabilities —\n Experiments tracking Artifacts and functions management (code and data versioning) A functions marketplace Model monitoring    Load balancing of Nuclio functions via API gateways, which allows function A/B testing and canary deployment (rollout).\n  Nuclio version upgrade, including new consumer-group support for the platform (v3io) stream trigger, and Sarama support for the Kafka trigger.\n  Improved error handling for failed deployment of Nuclio functions.\n  Scale-to-zero support for Jupyter Notebook.\n  Support for connecting PVCs to Jupyter Notebook.\n  Official support for the V3IO Frames library (previously supported as Tech Preview) and a version upgrade that features multiple improvements, including\n Improved performance. Client API changes to improve the user experience. Support multiple write modes (NoSQL). Support deletion of specific items using different types of filters (NoSQL and TSDB); similar support was also added to the TSDB CLI.    Web-APIs support for array attributes, including usage in update and condition expressions.\n  Enhanced application-services monitoring and related Grafana visualization [Tech Preview], including\n New application-cluster monitoring. New GPU monitoring, both at the application-cluster and Kubernetes namespace levels. Enhanced Nuclio monitoring.    New Features and Enhancements Application Services | Dashboard (UI | Frames | Grafana | Jupyter | Logging and Monitoring Services | MLRun | Nuclio | Pipelines | Presto | Prometheus | Security | Spark | TSDB | TSDB Nuclio Functions | User Management | Web APIs | Web Shell | Zeppelin\nManaged Application Services NoteSee also the dashboard new features and enhancements for UI improvements related to managing application services.    Upgraded to Kubernetes version 1.15.5 and Grafana version 6.6.0.\n  Jupyter Notebook   Upgraded versions of preinstalled and certified software —\n JupyterLab — version 1.0.10 Jupyter Notebook — version 6.0.3 Conda — version 4.7.11 Nuclio Jupyter package (nuclio-jupyter) — version v0.8.1 Platform tutorial Jupyter notebooks — version v2.8.11 NVIDIA CUDA version 10.1 RAPIDS version 0.12    Added scale-to-zero support: when the Scale to zero option is enabled for a Jupyter Notebook service (from the common service parameters on the dashboard Services page), the platform automatically frees up Jupyter Notebook resources when the service becomes idle. This feature is recommended mainly when working with GPUs.\n  Added a variable-inspector extension (jupyterlab_variableinspector) to the UI of the Jupyter Notebook service. The right-click context menu of the notebook cells now has an Open Variable inspector option that opens a Variable Inspector window with detailed information about the variables used in executed cell code (including the variables' types, sizes, and values).\n  Moved to using separate Jupyter pip environments for each instance of the Jupyter Notebook service. In previous releases, the pip environment was shared among all Jupyter Notebook instances for the same running user.\n  Added support for connecting existing cluster Persistent Volume Claims (PVCs) to a Jupyter Notebook service. You can select the PVCs from the dashboard's Services page — see the new Persistent Volume Claims (PVCs) custom-parameters section of the Jupyter Notebook service.\n  Removed NVIDIA RAPIDS from the default Jupyter Notebook environment. You can easily install RAPIDS manually from Jupyter Notebook using Conda. For example:\nconda create -n rapids -c rapidsai -c nvidia -c anaconda -c conda-forge -c defaults rapids=0.12 python=3.7 cudatoolkit=10.1    V3IO Frames   Added official support for the V3IO Frames service (previously supported as Tech Preview), and upgraded to Frames version v0.6.18-v0.9.24.\n  Improved the performance of the NoSQL and TSDB backends.\n  Modified the Frames client API to improve the user experience. Note that the changes break backwards compatibility with the previous Tech Preview API:\n Client constructor — moved the mandatory container parameter before the optional data_url parameter. create method —  Replaced the attrs dictionary parameter with a **kw parameter to support passing backend-specific keyword arguments as regular function parameters. Renamed the aggregation-granularity argument (previously an attrs key) to aggregation_granularity (TSDB backend).   write and read methods — renamed the max_in_message parameter to max_rows_in_msg. execute method — renamed the inferschema command to infer_schema (NoSQL backend).    Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   Added a nosql backend type — an alias to the existing kv type.\n  Added support for deleting specific items according to a filter expression, which can be applied to item attributes.\n  Added a new save_mode parameter to the write method to support different types of write modes — \u0026quot;errorIfTableExists\u0026quot;, \u0026quot;overwriteTable\u0026quot;, \u0026quot;overwriteItem\u0026quot;, \u0026quot;createNewItemsOnly\u0026quot;.\n  Added support for reading partitioned tables.\n  Added support for reading the following system attributes: __gid, __mode, __mtime_nsecs, __mtime_secs, __size, __uid, __ctime_nsecs, __ctime_secs, __atime_secs\u0026quot;, __atime_nsecs, __obj_type, __collection_id.\n  [Tech Preview] Added support for range scans by using the read method's sort_key_range_start and sort_key_range_end keyword arguments.\n  Frames TSDB Backend (\u0026quot;tsdb\u0026quot;)   Added support for deleting specific TSDB table items using different types of filters:\n start and end parameters for deleting only the items whose data sample times are within the specified time range. metrics parameter for deleting items by metric names. filter parameter for deleting items according to a filter expression, which can be applied to labels.    MLRun [Tech Preview]   Extended and enhanced the Tech Preview support for MLRun — Iguazio's open-source library for automating and tracking data science tasks and full workflows, which integrates seamlessly with Kubeflow Pipelines and the Nuclio serverless framework and affords the following capabilities:\n Experiments tracking Artifacts and functions management (code and data versioning) A functions marketplace Model monitoring  Among the MLRun enhancements in v2.8.0 of the platform —\n  Added a shared single-instance tenant-wide MLRun service (mlrun). In the current release, the service is pre-deployed as a default service in cloud deployments. To add the service for other types of deployments, contact Iguazio's support team.   Upgraded to MLRun version 0.4.7. For future upgrades to newer MLRun releases, consult Iguazio's support team.\n    Pipelines   Upgraded to Kubeflow Pipelines version 0.2.5.\n  Nuclio Serverless Functions   Upgraded to Nuclio version 1.3.25, featuring multiple enhancements, including the following:\n Iguazio MLOps Platform stream trigger (v3ioStream) — added support for using consumer groups. Kafka trigger (kafka-cluster) — added support for using the Sarama Go client library (sarama) to read from Kafka.    Added support for creating API gateways for load balancing of Nuclio functions, which allows function A/B testing and canary deployment (rollout). You can create an API gateway from the new API Gateways dashboard tab for your project — see the dashboard project enhancements.\n  Improved error handling for failed deployment of Nuclio functions.\n  Exposed an HTTP web-API endpoint for remote access to the Nuclio service from outside of the platform. The endpoint URL for your cluster is available from the API column for the Nuclio service (nuclio) on the dashboard's Services page. The requests are authenticated with a session cookie that's returned for a Create Session Management Sessions API request.\n  TSDB Nuclio Functions   Upgraded to V3IO TSDB Nuclio Functions version v0.2.5-v0.9.24.\n  Time-Series Databases (TSDBs)  NoteSee also the TSDB Nuclio functions section under the Nuclio new features and enhancements.    Upgraded to V3IO TSDB version v0.9.24.\n  Improved and enhanced the support for deleting TSDB tables and specific table items — including new support for a filtered deletion by time range, metric names, or a filter expression. See also the related TSDB CLI and Frames changes.\n  TSDB CLI   Added support for deleting specific TSDB table items using different types of filters:\n start and end flags for deleting only the items whose data sample times are within the specified time range. -m|--metrics flag for deleting items by metric names. -f|--filter flag for deleting items according to a filter expression, which can be applied to labels.    Changed the meaning of the -f flag from forced deletion to deletion by filter expression. You can still use the --force flag to perform a forced deletion.\n  Prometheus   Upgraded to V3IO Prometheus version v2.8.0-igz4-v0.9.24.\n  Presto   Moved from prestodb to prestosql — version 323.\n  Improved the support for editing and adding new Presto configuration files and connectors, and added support for viewing and editing the existing configurations. The configuration is done from the Advanced custom-parameters section of the Presto service on the Services dashboard page.\n  Spark   Upgraded to Spark version 2.4.4.\n  Web APIs   Added the option to configure the number of workers for the web-APIs service (webapi) from the custom service parameters on the Services dashboard page.\n  Added support for array attributes, including usage in update and condition expressions.\n  Web Shell   Extended the permissions of the Service Admin web-shell service account to include viewing, editing, listing, and deleting Persistent Volume Claims (PVCs).\n  Zeppelin   Upgraded to Zeppelin version 0.8.2.\n  Logging and Monitoring Services   Added support for secure logging by defining Elasticsearch user credentials as part of the configuration of the log-forwarder service in the dashboard.\n  Linked the Monitoring service on the Services dashboard page to the Application Services Monitoring dashboard of a shared Grafana user service when such a service is defined and enabled.\n  Moved the predefined dashboard of the Grafana service to a default dashboards directory.\n  [Tech Preview] Added an external URL for the Grafana service (available from the API column on the Services dashboard page), which can be shared with anyone (along with a related password) to allow access to the monitoring dashboards without platform user authentication.\n  [Tech Preview] Added application-cluster monitoring and a related Grafana service with predefined monitoring dashboards that include performance statistics and additional data. This Grafana service is available to users with the IT Admin management policy via a link in the new Status dashboard\u0026quot; column on the Clusters | Application dashboard tab. The link opens the Kubernetes Cluster Status dashboard, but you can find additional dashboards under the parent default Grafana dashboards directory.\n  [Tech Preview] Added GPU monitoring using NVIDIA DeepOps:\n At the Kubernetes namespace level — added GPU occupancy and utilization data to the predefined dashboards of the Grafana user service. At the Kubernetes application-cluster level — added GPU dashboards to the new application-cluster Grafana service.    [Tech Preview] Added support for filtering the cluster log collection both by type and by the log level.\n  [Tech Preview] Improved and enhanced the predefined Nuclio-monitoring Grafana dashboards.\n  Security and User Management   Enabled all users to view information for their own user profile and edit relevant properties — such as the password, email address, or first and last names — from the Identity dashboard page.\n  [Tech Preview] Added the option to define an LDAP filter for synchronizing only with specific user groups from the external IdP. See the new Person filter field in the Remote host settings section of the dashboard's Identity | IdP tab.\n  Dashboard (UI)   Restyled the dashboard to match the new product brand.\n  Improved the Projects page (previously available from the Functions menu), including the following changes:\n  Renamed the Functions side navigation-menu option to Projects.   Added an API Gateway project tab for creating new API gateways for load balancing of Nuclio functions. See the API-gateways Nuclio enhancement.   Added different methods for copying a function's invocation URL to the clipboard.   Removed the option to create a new function from the Projects home page. Functions should be created for a specific project.   In the Functions project tab, added a function-hover tooltip with the function name and description (if exists), and removed the Description column.     Made miscellaneous improvements to the Identity UI.\n  Changed the service names on the Services page to display the unique name of the service instance instead of the general service type.\n  On the Clusters page, separated the data and application nodes into new Data and Application tabs.\n  Added a GPU column to the Clusters | \u0026lt;application cluster\u0026gt; | Nodes table, which displays the number of GPUs on each node.\n  Changed the behavior when selecting an HTML file in the container Data | Browse tab to open the file in the default web browser instead of downloading it.\n  Improved browsing of NoSQL tables by freezing the table header so that it's always visible.\n  Added a client_ip parameter to the event information for login-related events. This parameter shows the IP address used by the client when logging into the platform, provided the address can be retrieved. The information is available for relevant events on the Events dashboard page; see, for example, the audit events on the Audit tab.\n  Removed the Networks dashboard page.\n  Made miscellaneous enhancements to the dashboard.\n  Fixes Application Services | Backup and Recovery | Dashboard (UI | Frames | Hive | Nuclio | Platform Management | Presto | Security\nManaged Application Services V3IO Frames   Fixed the handling of null item-attribute values.\n  Frames NoSQL DataFrame (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot; Backend)   Fixed the schema key when inferring the table schema with the infer_schema or infer command of the execute client method. Consequently, tables with a schema file that was inferred with this command can now be read also with Spark DataFrames or Presto, and the Frames read client method no longer returns an extra __name attribute (column).\n  Nuclio Serverless Functions   Fixed the support for using the same container-image name for multiple Nuclio functions.\n  Presto and Hive   Fixed an issue whereby a Presto Hive view of a NoSQL table could be used only by the user who created the view.\n  Security   Returned the support for the Sources match criteria of the data-access-policy rules, and modified the available criteria — renamed the interfaces and removed the unsupported network source.\n  Backup, Recovery, and High Availability (HA)   Added VIP support for Amazon Web Services (AWS) cloud environments, which resolves the previous known issue related to handling failures in the main master application node of a Kubernetes application cluster.\n  Platform Management   Fixed the error handling for Create Container requests with a negative container ID. Previously such requests could have returned status code 500 instead of 400 and had a cumbersome error message.\n  Dashboard (UI)   Enabled making changes to object (item) attributes from the dashboard while there are unloaded attribute values.\n  Fixed occasional long table-view load times when entering a NoSQL table directory that contains a schema file (.#schema) from the container-data browse tab.\n  Fixed the Number of objects directory metadata when browsing a data container in a multi-node environment.\n  Fixed the displayed Number of objects directory metadata when deleting an object in a data container while browsing the container.\n  Fixed the performance CPU (cores) and Memory data updates on the Services page when the monitoring service is disabled.\n  Added missing performance graph to the CPU column on the Clusters page.\n  Fixed occasional broken Grafana dashboard links in the action menu of the monitoring service on the Services page.\n  Known Issues Application Services | Backup and Recovery | Dashboard (UI | File System Frames | Hadoop | High Availability (HA)| Hive | Jupyter | Logging and Monitoring Services | MLRun | Nuclio | Presto | Prometheus | Spark| TSDB | Web APIs\nManaged Application Services   Users who are members of a user group with the Service Admin management policy, but aren't assigned this policy directly, don't have proper service-administration permissions: such users can see only their services on the Services dashboard page, and any changes that they make to their services impact other users' services. To bypass this issue, either assign the Service Admin management policy directly to the users; remove the users from groups with this policy; or remove the policy from the relevant user groups.\n  A user whose username was previously in use in the platform cannot run application services.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  A running user of a scaled-to-zero Jupyter Notebook service cannot wake up the service by selecting its name on the Services dashboard page, unless the user has the Service Admin management policy. To bypass this issue and wake up the service, right-click the service-name link in the Name column of the services table; select to copy the link address (location); paste the copied service URL in the browser's address bar and open it in a new tab. You might also need to refresh the Jupyter Notebook browser page to properly load the JupyterLab UI. Note that Jupyter scale-to-zero is recommended mainly when working with GPUs.\n  Concurrent execution of multiple notebooks by the same running user might result in an error.\n    You cannot currently delete directories from the Jupyter UI. You can still delete directories by running a file-system command in a command-line interface, such as a Jupyter terminal or notebook or a web shell.\n  The Services dashboard page displays an incorrect version for the Jupyter Notebook service (version 1.0.2 instead of the installed JupyterLab version 1.0.10).\n  V3IO Frames Frames TSDB Backend (\u0026quot;tsdb\u0026quot;/\u0026quot; Backend)   When calling the delete method with a metrics-name (metrics) or filter-expression (filter) parameter but without a time-filter parameter (start and/or end), the specified filter is ignored and the entire table is deleted. To bypass this and apply the specified filter to all TSDB table items, also set the start parameter to 0 and optionally set the end parameter to \u0026quot;now\u0026quot; (default).\n  MLRun [Tech Preview]   The MLRun service cannot currently be disabled from the dashboard.\n  Nuclio Serverless Functions   Deploying a Nuclio function named dashboard renders the Projects dashboard page (the Nuclio dashboard) inaccessible. To avoid this, refrain from naming Nuclio functions dashboard.\n  Changes to a Nuclio function's volume configuration from the dashboard (see the Volumes section of the function Configuration tab) are applied only after collapsing the volume entry in the dashboard.\n  Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  Time-Series Databases (TSDBs) Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n   It's not possible to edit a configuration file with the same name for different types of resources (namely, coordinator and worker).\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Web APIs Simple-Object Web API   GET Object and HEAD Object don't return correct ETag and Content-Type metadata.\n  A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  Logging and Monitoring Services   Keeping the log-forwarder service enabled over time might result in extensive memory consumption and disruption of other application services. To avoid this, disable the service when logging isn't required.\n  Changes to the log-forwarder resource configuration that are done from the dashboard are ignored.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Backup, Recovery, and High Availability (HA)   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Dashboard (UI) NoteSee the Nuclio known issues for Nuclio-specific dashboard issues.    When browsing a NoSQL table with an attribute named closed, the Table view is distorted.\n  In heavily loaded systems, the container used-capacity information might take a long time to refresh.\n  Notes   Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-2.8/v2.8.0/","title":"Version 2.8.0 Release Notes"},{"content":"This section contains the release notes for version 3.0 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.0/","title":"Version 3.0 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 3.0.0, and known issues in this version.\nTech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Highlights Version 3.0.0 introduces many new powerful features and enhancements, as well as bug fixes, as detailed in the release notes. Following are some of the main highlights of this release:\n  Official support for MLRun and integration of the MLRun UI into the platform UI\n  Extensive project improvements, including UI and API changes for working in the context of a project. Among the improvements —\n Dashboard (UI) Projects redesign with new views and features A new predefined \u0026quot;projects\u0026quot; container    Support for Spark Operator\n  Support for Amazon EKS deployment, including usage-based pricing\n  Added more Grafana dashboards for application-cluster monitoring\n  Support for SSH connection to the web-shell service, which enables secured connections from remote IDEs\n  A new OAuth2 (OIDC) Authenticator service (authenticator), which enables secured access of external (non-platform) users to Nuclio API gateways and shared Grafana dashboards in the platform\n  Support for small AWS data nodes (i3.2xlarge EC2 instance types)\n  Enhanced the support for asynchronous file upload from the dashboard (UI)\n  Upgraded versions of pre-deployed and certified software, including Spark 2.4.5 and Presto 332\n  New Features and Enhancements Application Services | Authenticator | AWS specifications | Azure specifications | Dashboard (UI Deployment Specifications | Frames | General | Grafana | Hive | Jupyter | Logging and Monitoring Services | MLRun | Nuclio | Presto | Prometheus | Python SDK | Security | Spark | TSDB | TSDB Nuclio Functions | User Management | Web Shell\nGeneral   Added a new predefined data container named \u0026quot;projects\u0026quot; for storing shared project artifacts.\n  Added a new if_else expression function for applying if-else conditional logic in both update and condition expressions:\nif_else(\u0026lt;Boolean condition expression\u0026gt;, \u0026lt;\u0026#34;then\u0026#34; expression\u0026gt;, \u0026lt;\u0026#34;else\u0026#34; expression\u0026gt;)    Improved table-scan performance for large data containers.\n  Restructured the platform documentation site to improve the user experience. As part of these changes, the MLRun documentation is now available also as part of the platform documentation (see Data Science and MLOps).\n  Managed Application Services Note  For information about the new default OAuth2 (OIDC) Authenticator service (authenticator), see the Security and User Management new features and enhancements.\n  For information about UI improvements related to managing application services, see the Dashboard (UI) new features and enhancements.\n      Upgraded to Kubernetes version 1.17.14.\n  Jupyter Notebook  NoteSee also the MLRun new features and enhancements for improvements related to using MLRun from Jupyter Notebook.    Upgraded versions of pre-deployed and certified software —\n JupyterLab version 2.2.0  Platform tutorial Jupyter notebooks — version 3.0 V3IO Python SDK — version 0.5  V3IO Frames client — version 0.8  Nuclio Jupyter library (nuclio-jupyter) version 0.8  Conda version 4.8.3  NVIDIA CUDA version 11.0  NVIDIA RAPIDS version 0.17  Additional pre-deployed Python packages upgrade     Removed the dependency of the Jupyter Notebook service on the Presto service.\n  In environments with GPUs, enabled selecting the Jupyter Full Stack with GPU flavor in the dashboard service configuration without reserving any GPU resources (i.e., without setting Resources | GPU | Limit).\n  Platform Jupyter tutorials and demos infrastructure improvements —\n Consolidated the sources for the platform's end-to-end use-case application demos and moved all demo sources and the getting-started tutorial to the MLRun demos GitHub repository (mlrun_demos). See also the MLRun-demos enhancements note.  Improved the script for retrieving and updating local demo copies, including new options for downloading a specific version, and renamed the script from get-additional-demos.sh to update-demos.sh. By default, the script now attempts to download the latest version of the demos that matches the version of the installed MLRun Python package (mlrun).  Improved the igz-tutorials-get.sh script for updating the local tutorial copies from the tutorials GitHub repository (v3io/tutorials) to support downloading a specific version and to download the latest compatible version for the current platform version by default.  Pre-deployed the MLRun demos in the platform. Upon the creation of the first Jupyter Notebook service for a given user, the platform now automatically downloads the demos version that matches the version of the installed MLRun service and copies them to a demos directory in the running user's home directory (/User/demos). If the download fails, such as in offline environments without an internet connection, the demo files are copied from a backup in the platform image (in which case you might not get the latest compatible version).     V3IO Python SDK   Upgraded to V3IO Python SDK (v3io-py) version 0.5.\n  V3IO Frames   Upgraded to Frames server and client versions 0.8.\n  Added write support for new pandas 1.0 data types — BooleanDType, Integer32DType, Integer64DType, and StringDtype. Note that as with the data types used in previous pandas releases, values of these types are implicitly converted to the corresponding platform table-schema data types — \u0026quot;boolean\u0026quot;, \u0026quot;long\u0026quot;, and \u0026quot;string\u0026quot;.\n  MLRun  NoteSee also the dashboard (UI) Projects new features and enhancements, including the MLRun UI integration into the platform dashboard.    Added official support for MLRun (previously supported as Tech Preview) and upgraded to version 0.6.\n  Added an align_mlrun.sh script for installing the MLRun Python package (mlrun) and updating the package version to match the version of the platform's MLRun service. The script is available in the running-user directory (/User) upon creation of a Jupyter Notebook service for the relevant user. You can find information about this script and related execution code in the platform's welcome.ipynb tutorial Jupyter notebook, which is also available in the running-user directory.\n  Changed the default artifacts path for new projects to a \u0026lt;project name\u0026gt;/artifacts directory in the new predefined \u0026quot;projects\u0026quot; data container. The artifacts directory is created when the first artifact is logged for the project.\n  Added support for scheduling jobs and viewing scheduled jobs from the code (using the MLRun API) and from the dashboard (UI).\n  Added an API for using the platform's new Spark Operator service to simplify submission and scheduling of Spark jobs.\n  Added support for serving graphs that are composed of predefined graph blocks (such as model servers, routers, ensembles, and data-engineering tasks) or based on native Python classes or functions. See the MLRun documentation.\n  Added support for remote MLRun environments: develop your code locally and run it on a remote cluster. See the MLRun documentation.   Improved the documentation for distributed runtimes — Dask, MPI (Horovod), and Spark.\n  Added V3IO Frames to MLRun images.\n  Enforced MLRun project-name restrictions — RFC 1123 DNS label-name requirements (DNS-1123 label) — both from the MLRun API and from the dashboard (UI). See the related known issue for old projects that don't conform to these restrictions.\n   Restructured and improved the MLRun demos, including directories and files renaming; getting-started tutorial rewrite; and new how-to demos that demonstrate how to convert existing ML code to an MLRun project and how to work with Spark. (See also the Jupyter Notebook tutorials and demos infrastructure-improvements note — including moving all platform demos sources and the getting-started tutorial to the mlrun_demos GitHub repository, pre-deploying the demos in the platform, and improving the demos-update script.)\n  Pipelines   Upgraded to Kubeflow Pipelines version 1.0.1.\n  Nuclio Serverless Functions   Upgraded to Nuclio version 1.5.\n  Changed the default Kubernetes service type for Nuclio functions from NodePort to ClusterIP to avoid exposing the function outside of the platform cluster.\n  Added support for authenticating external (non-platform) user access to Nuclio API gateways by selecting the OAuth2 authentication method, which uses the new OAuth2 authenticator service (authenticator).\n  Added support for running Nuclio functions as any user, not just as a root user, to enable access to external volumes (such a NetApp storage) using relevant user permissions.\n  Enabled importing a Nuclio function with the same name as an existing function from the dashboard to override the existing function.\n  TSDB Nuclio Functions   Upgraded to V3IO TSDB Nuclio Functions version 0.6.\n  Time-Series Databases (TSDBs)   Upgraded to V3IO TSDB version 0.11.\n  Prometheus   Upgraded to V3IO Prometheus version 3.5 and Prometheus version 2.15.2.\n  Provided easy access to the Prometheus UI by linking to it from the monitoring service on the Services dashboard page.\n  Presto and Hive   Upgraded to Presto version 332.\n  Added automatic creation of the default Hive database path (/user/hive/warehouse in the \u0026quot;bigdata\u0026quot; container), if it doesn't already exist, when enabling Hive for the Presto service.\n  Spark   Upgraded to Spark version 2.4.5.\n  Added a new Spark Operator default (pre-deployed) shared single-instance tenant-wide service named spark-operator (version v2.4.5). The service uses the spark-on-k8s-operator Kubernetes Operator for Spark (\u0026quot;Spark Operator\u0026quot;) to simplify submission and scheduling of Spark jobs. Use the service via the new MLRun Spark Operator API.  [Tech Preview] Note that the use of this service over GPUs is supported only as Tech Preview.\n  Web Shell   Added support for secure connectivity to the platform cluster using SSH, which enables debugging from remote IDEs such as PyCharm and VSCode. You can enable SSH and configure the port from the web-shell's custom service configuration on the dashboard's Services page. When SSH is configured, you can get the authentication key from the new SSH menu option for this service.  Note that the SSH port must be in the range of 30000–32767, and the SSH connection must be done with user \u0026quot;iguazio\u0026quot; regardless of the identity of the running user of the web-shell service.\n  Logging and Monitoring Services   Upgraded the Elasticsearch version supported by the log-forwarder service to version 7.10.\n  Upgraded to Grafana version 7.2.\n  Added a new predefined MLRun Jobs Monitoring Grafana service dashboard for monitoring MLRun jobs' resource consumption (such as memory and CPU).\n  Added more Grafana dashboards for application-cluster monitoring. The dashboards are available in the private dashboards folder of the application-cluster Grafana service, which is accessible to users with the IT Admin management policy via a Status dashboard link on the Clusters | Application platform-dashboard tab:\n Application-services and Nuclio-functions monitoring dashboards (similar to the existing predefined dashboards of the same names for the Grafana service that's accessible from the Services dashboard page):  Application Services Monitoring Nuclio Functions Monitoring - Overview Nuclio Functions Monitoring - Per Function   Kubernetes Resource Usage Analysis — a new dashboard that displays resource-consumption information for the cluster, such as CPU and memory utilization.    Added support for authenticating external (non-platform) user access to the dashboards of shared platform Grafana services by using the new OAuth2 (OIDC) Authenticator service (authenticator).\n  Moved the platform's predefined Grafana dashboards to a private folder that isn't visible to viewers with the Viewer role — namely, external (non-platform) users who are given access to the Grafana UI via the authenticator service. In addition, predefined an empty public Grafana dashboards folder with default permissions for storing custom dashboards that you wish to share also with external users.\n  Changed the display of the monitoring service's name (monitoring) on the Services dashboard page to link to the UI of the pre-deployed Prometheus service.\n  Renamed the AppNode.Offline event to AppNode.NotReady.\n  Security and User Management  Added a new OAuth2 (OIDC) Authenticator default (pre-deployed) shared single-instance tenant-wide service named authenticator (version 2.23.0). This service is used for OAuth2 authentication of user access to Nuclio API gateways and shared Grafana dashboards, including access by external (non-platform) users (for example, using GitHub).   Dashboard (UI) Projects   Redesigned the Projects dashboard area to integrate the MLRun dashboard (UI), synchronize MLRun and Nuclio projects, and extend project information and functionality. As part of these changes —  Redesigned the Projects home page to display new project summaries and action menus.  Added a new Projects | \u0026lt;project name\u0026gt; project-overview page, which is available when selecting a project from the Projects home page. The project overview includes a high-level overview of the project's assets, status of running jobs and functions, ability to add new project components such as jobs, and links to additional project information.  Changed access to the Functions project page, which displays the project's Nuclio serverless functions and API gateways. This information is now available via Real-time functions (Nuclio) and API gateways (Nuclio) links in the project overview's Quick Links sidebar section.     Added a new Jobs | Schedule project tab for scheduling MLRun jobs and viewing scheduled jobs (part of the new MLRun job-scheduling support). As part of this change, the job-monitoring information moved to a new Jobs | Monitor tab.\n  Added usability improvements to the Create Job wizard for creating new MLRun jobs.\n  Added support for archiving projects from the dashboard (new Archive project-menu option) and for filtering out archived projects from the projects view (see the All Projects filter options).\n  Data   Enhanced the support for asynchronous file upload. You can now issue new upload requests while previous requests are in progress, and see the upload status in the new File Uploads pop-up window. Note that when uploading multiple files in the same upload request, some files might not be visible in the file-uploads window until the upload of other files in the request has completed.\n  Enhanced table browsing by displaying the data as it is loaded instead of waiting for the entire table to load.\n  Miscellaneous   Added warning messages when the cluster is in maintenance mode.\n  Deployment Specifications Cloud Deployments   Increased the minimal required OS boot-disk size for all cloud data-node types to 400 GB.\n  AWS — added support for deploying the platform on the Amazon Elastic Kubernetes Service (Amazon EKS), including usage-based pricing.\n  AWS — added support for using smaller data nodes of EC2 instance type i3.2xlarge.\n  Azure — added support for using the GPU-optimized NCv3-series VM instance sizes for the application-nodes.\n  Fixes Application Services | Dashboard (UI) | Jupyter | Logging and Monitoring Services | MLRun | Nuclio | TSDB\nManaged Application Services Jupyter Notebook   Added support for deleting directories (folders) from the Jupyter UI. Note that this feature required disabling the Jupyter trash mechanism for both files and directories, so deleted items are no longer moved to the trash and cannot be restored.\n  Fixed the version of the Jupyter Notebook service on the Services dashboard page.\n  MLRun   Added MPIJob logs to the dashboard (UI).\n  Nuclio Serverless Functions   Enforced reserved-names restrictions for creation of new Nuclio functions and API gateways for the following names — controller, dashboard, dlx, scaler. This also fixes the previous known issue of no access to the Projects dashboard page when deploying a Nuclio function named dashboard.\n  Fixed the Nuclio function status on the dashboard (UI) to always reflect the current deployment state.\n  Time-Series Databases (TSDBs)   Improved error messages for invalid TSDB queries, which are displayed when working with TSDB tables in Grafana dashboards.\n  Logging and Monitoring Services   Fixed a memory issue in the monitoring service.\n  Dashboard (UI) NoteSee also the MLRun and Nuclio fixes for dashboard changes that are specific to these services.    Fixed the Table view distortion when browsing a NoSQL table with an attribute named closed.\n  Fixed SMTP settings-page issues: users with the IT Admin management policy but without the Tenant Admin policy no longer see an \u0026quot;Error: Failed to fetch user list (you can still update the rest)\u0026quot; error message on this page, and can now successfully use the Users to notify field to define IT Admin users who'll receive cluster-alert email notifications.\n  Known Issues Application Services | Backup and Recovery | Dashboard (UI | File System | Frames | Hadoop | High Availability (HA) | Hive | Jupyter | MLRun | Logging and Monitoring Services | Nuclio | Presto | Prometheus | Security | Spark | TSDB | User Management | Web APIs\nManaged Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  V3IO Frames Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   The write client method's \u0026quot;overwriteTable\u0026quot; save mode isn't supported for partitioned tables [Tech Preview].\n  MLRun   The default-project directory (default) is shared for read-only and execution.\n  Beginning with platform version 3.0.0 / MLRun version 0.6.0, project-name restrictions are enforced for MLRun projects both from the dashboard (UI) and the MLRun API. Project names must conform to the RFC 1123 DNS label-name requirements (DNS-1123 label) — 1–63 characters, contain only lowercase alphanumeric characters (a–z, 0–9) or hyphens (-), and begin and end with a lowercase alphanumeric character. Older MLRun projects whose names don't conform to these restrictions will be in a degraded mode and won't have Nuclio functionality (no serverless functions or API gateways). To fix this you need to replace the old projects (you cannot simply rename them). For assistance in upgrading old projects, contact Iguazio's support team.\n  Nuclio Serverless Functions   Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  Time-Series Databases (TSDBs) Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n   Invalid Presto configuration files or connector definitions might cause the Presto service and all dependent services to fail. This is the expected behavior. To recover, delete the problematic configuration or manually revert relevant changes to the default Presto configuration files, save the changes to the Presto service, and select Apply Changes on the dashboard Services page.   In rare cases, after failing to deploy incorrect edits to a default configuration file, it might not be possible to revert the default Presto configuration from the dashboard.\n  Running the Hive CLI from a Jupyter Notebook service when there's no web-shell service in the cluster might fail if another user had previously run the CLI from another instance of Jupyter. To resolve this, ensure that there's at least one web-shell service in the cluster.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Spark Operator   Spark jobs whose names exceed 63 characters might fail because of a Spark bug when running on Kubernetes — see Spark Bug SPARK-24894, which was resolved in Spark version 3.0.0.\n  Web APIs Simple-Object Web API   GET Object and HEAD Object don't return correct ETag and Content-Type metadata.\n  A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.\n  NoSQL Web API   Range-scan requests following recreation of a data container or a container directory might return a 513 error; in such cases, reissue the request.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Logging and Monitoring Services   Data-cluster statistics are not available when the platform is in offline mode.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Security and User Management   Users who are members of a user group with the IT Admin management policy but are not assigned this policy directly cannot access the application-cluster status dashboard. To bypass this issue, assign the IT Admin management policy directly to any user who require such access..\n  Changing the token-expiration period for the authenticator service (id_tokens_expiration configuration) doesn't affect existing tokens. For further assistance, contact Iguazio's support team.\n  Backup, Recovery, and High Availability (HA)   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Dashboard (UI)  NoteSee the MLRun and Nuclio known issues for dashboard issues related specifically to these frameworks.  Notes   Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.0/v3.0.0/","title":"Version 3.0.0 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 3.0.2, and known issues in this version.\nHighlights The following are the main highlights of this release:\n Extended support for Amazon EKS deployments including auto scale up and down, support for one secondary node group, and attaching services to a specific EKS node group.  New Features and Enhancements Deployment Specifications\nDeployment Specifications Cloud Deployments   Extended support for Amazon EKS deployments —\n  Support scale up/down for any node group. Scale up/down occurs automatically based on Kubernetes need for resources. The initial group cannot scale to zero.\n  Users have the ability to configure one secondary node group in EKS using the platform installer (Provazio).\n  Users are able to assign services to specific node groups to leverage node groups that are suitable for specific service requirements, or to prevent scale to zero of specific node groups.\n    Known Issues Application Services | Backup and Recovery | Dashboard (UI | File System | Frames | Hadoop | High Availability (HA) | Hive | Jupyter | MLRun | Logging and Monitoring Services | Nuclio | Presto | Prometheus | Security | Spark | TSDB | User Management | Web APIs\nManaged Application Services   A user whose username was previously in use in the platform cannot run application services.\n  Jupyter Notebook   Running Scala code from a Jupyter notebook isn't supported in the current release. Instead, run Scala code from a Zeppelin notebook or by using spark-submit, or use Python.\n  V3IO Frames Frames NoSQL Backend (\u0026quot;nosql\u0026quot;/\u0026quot;kv\u0026quot;)   The write client method's \u0026quot;overwriteTable\u0026quot; save mode isn't supported for partitioned tables [Tech Preview].\n  MLRun   The default-project directory (default) is shared for read-only and execution.\n  Beginning with platform version 3.0.0 / MLRun version 0.6.0, project-name restrictions are enforced for MLRun projects both from the dashboard (UI) and the MLRun API. Project names must conform to the RFC 1123 DNS label-name requirements (DNS-1123 label) — 1–63 characters, contain only lowercase alphanumeric characters (a–z, 0–9) or hyphens (-), and begin and end with a lowercase alphanumeric character. Older MLRun projects whose names don't conform to these restrictions will be in a degraded mode and won't have Nuclio functionality (no serverless functions or API gateways). To fix this you need to replace the old projects (you cannot simply rename them). For assistance in upgrading old projects, contact Iguazio's support team.\n  Nuclio Serverless Functions   Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).\n  In case of a successful automatic deployment of a Nuclio function for a request that previously timed out — for example, if a requested resource, such as a GPU, becomes available after the time-out failure — the function status in the dashboard is still Error.\n  Time-Series Databases (TSDBs) Prometheus   Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.\n  Presto and Hive   You can't change the MySQL DB user for an existing Hive MySQL DB. To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.\n   Invalid Presto configuration files or connector definitions might cause the Presto service and all dependent services to fail. This is the expected behavior. To recover, delete the problematic configuration or manually revert relevant changes to the default Presto configuration files, save the changes to the Presto service, and select Apply Changes on the dashboard Services page.   In rare cases, after failing to deploy incorrect edits to a default configuration file, it might not be possible to revert the default Presto configuration from the dashboard.\n  Running the Hive CLI from a Jupyter Notebook service when there's no web-shell service in the cluster might fail if another user had previously run the CLI from another instance of Jupyter. To resolve this, ensure that there's at least one web-shell service in the cluster.\n  Spark Spark UI   The Spark UI might display broken links after killing a running application from the UI. To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.\n  Spark Streaming   To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.\n   Spark Operator   Spark jobs whose names exceed 63 characters might fail because of a Spark bug when running on Kubernetes — see Spark Bug SPARK-24894, which was resolved in Spark version 3.0.0.\n  Web APIs Simple-Object Web API   GET Object and HEAD Object don't return correct ETag and Content-Type metadata.\n  A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.\n  NoSQL Web API   Range-scan requests following recreation of a data container or a container directory might return a 513 error; in such cases, reissue the request.\n  Streaming Web API   PutRecords with a stream path that points to an existing non-stream directory returns error ResourceNotFoundException instead of ResourceIsnotStream.\n  UpdateStream with an invalid request-body JSON format may not return the expected InvalidArgumentException error.\n  Logging and Monitoring Services   Data-cluster statistics are not available when the platform is in offline mode.\n  Hadoop   The following Hadoop (hdfs) commands aren't supported: createSnapshot, deleteSnapshot, getfattr, setfattr, setfacl, and setrep.\n  File System NoteFor Hadoop FS known issues, see the Hadoop known issues.    File-system operations are not supported for stream shards.\n  Security and User Management   Users who are members of a user group with the IT Admin management policy but are not assigned this policy directly cannot access the application-cluster status dashboard. To bypass this issue, assign the IT Admin management policy directly to any user who require such access..\n  Changing the token-expiration period for the authenticator service (id_tokens_expiration configuration) doesn't affect existing tokens. For further assistance, contact Iguazio's support team.\n  Backup, Recovery, and High Availability (HA)   Stream data is not backed up and restored as part of the platform's backup and upgrade operations.\n  In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services. Previously run application services will continue to run but cannot be stopped, and new services cannot be created.\n  The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.\n  The GetItems NoSQL Web API operation might fail during failover.\n  Dashboard (UI)  NoteSee the MLRun and Nuclio known issues for dashboard issues related specifically to these frameworks.  Notes   Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.\n  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.0/v3.0.2/","title":"Version 3.0.2 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 3.0.3, and known issues in this version.\nHighlights The following are the main highlights of this release: - Support Spot instances for EKS - Support Azure Kubernetes Service (AKS) - Node Selector to assign MLRun jobs and functions to specific nodes - Support GCP GPU machine types -- New Features and Enhancements Deployment Specifications Deployment Specifications Cloud Deployments  Support Spot instances for EKS.   Added support to configure and deploy AKS.   Enable users to assign nuclio functions to specific nodes using the node selector.   Support GCP GPU machine types.  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.0/v3.0.3/","title":"Version 3.0.3 Release Notes"},{"content":"This section contains the release notes for version 3.2 of the Iguazio MLOps Platform.\n","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.2/","title":"Version 3.2 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 3.2.0. Tech Preview NoteThe \u0026quot;[Tech Preview]\u0026quot; notation marks features that are included in the current release as a sneak peek to future release features but without official support in this release. Note that Tech Preview features don't go through QA cycles and might result in unexpected behavior. Please consult the Iguazio support team before using these features.  Highlights Version 3.2.1 introduces many new powerful features and enhancements detailed in the release notes. The following are the main highlights of this release:\n Project membership GKE auto scaling AKS auto scaling Modified pre-defined management policies Access keys for both data and control plane Upgraded to Presto to 359 Support spark operator 3.1.2 Upgraded to spark service to 3.1.2 Added support for Java 11 and Hadoop, Supports Linux Unified Key Setup (LUKS) encryption Added MLRun node selector features including a default node selection  New Features and Enhancements General   New option to set conda and pip repository from the UI, which are applied to MLRun, Nuclio, Jupyter, and Web Shell services. See Configuring the Python Repositories and Configuring Environmental Variables. Removed tenant-redirection in login screen so that usernames can contain @ signs.   For Iguazio key value operations, the square root function accepts int64, uint64, or a double as input. The output is a double. The square root can also can be used in filters or to update expressions.   When deploying in a dark site, new images are automatically enriched with a local docker registry prefix.  Managed Application Services  The Label Studio service is available, as [Tech Preview].   Support TensorBoard as an application service, as [Tech Preview]  V3IO Python SDK  Support for Python SDK Object read/write/delete functions.  MLRun  The MLRun service can now be configured with a default node for running mlrun jobs. Configure the node selector in the custom parameters of the service.  Nuclio Serverless Functions   Nuclio trigger supports Kafka 2.5.\n  Support nuclio integration with gitlab and Bitbucket.\n    Add default node selector to nuclio service. The pods of a Nuclio function can only run on nodes whose labels match the node selector entries configured for the specific function. You can configure the key-value node selector pairs in the Custom Parameters tab of the service. If there is no specified node selector, it defaults to the Kubernetes default behavior, and runs on a random node.\n  Nuclio no longer supports Python 2.7 runtime.   Nuclio has been upgraded to support python 3.7, 3.8, 3.9 runtime.\n  Invocation URLs are now labeled as internal or external, and are displayed in a drop-down menu.\n  Time-Series Databases (TSDBs) Prometheus  The preinstalled version of the V3IO Prometheus distribution is v3.5.8.  Presto and Hive  Added support for Presto and Hive versions:  Presto 359 Hive 3.1.2    Spark \n Upgrade Presto to 359 Support spark operator 3.1.2 Upgrade spark service to 3.1.2  Hadoop  Added support for Hadoop 3.2.0  File System  Moving directories is now supported. (Renaming of directories is not supported, as in previous releases.)  Security and User Management  Changes to the pre-defined management policies:  The Function Admin policy has been renamed to Developer policy. This is the only policy whose users can create projects. Developers can only see the projects that they are members of. New Project Admin policy. Users with this policy can view all projects, and change a project owner to any user. This policy does not give rights to modify the entities underneath such as features, or jobs, etc., and does not give rights to delete a project. New Project Read Only policy. Users with this policy can view all projects but cannot edit any aspect of the projects.     Project Members:  A new mechanism controls user and user group access to projects. Only users that are members of a given project can view and manage the project. See Project members. Access to project data is controlled by the data access policy, and not the project roles/membership. During an upgrade to v3.2.0, you must designate one user to own all the migrated projects. After an upgrade to v3.2.0, all pre-existing projects are associated with a (dynamically updated) group of all users named all users allowing all the users in this group to have access to all projects. (This is the pre-upgrade behavior.) After the upgrade you can assign members to each project.     Access Keys are now specific to the data plane, the control plane, or both. In previous releases this distinction did not exist, and there was only a single type of access keys. Access keys are used mainly for programmatic purposes, and are not time-bound, giving you longer than the 24 hours of the HTTP session. See Access keys.   Added Linux Unified Key Setup (LUKS) encryption for all cloud deployments. [Tech Preview].  Dashboard (UI) Projects These features are available when integrated with MLRun 0.8.0 and higher.\n The Real-time functions (Nuclio) report has a new column listing the user that deployed a function. This is particularly useful for identifying the \u0026quot;owner\u0026quot; of a function that is taking significant resources.   You can select the volume mount when creating a new function in the Projects page. The options are:  Auto—Uses the configuration that you defined by API. Manual—opens the volume section to fill in the relevant setting (no change from previous release). None—no need to create a volume mount-option.     In the Projects page, the Create New button has new menu items: Job, ML Function, Feature Set, Register File, Register Model, Register Dataset.   In the Projects page \u0026gt; Job \u0026gt; Monitor Jobs tab: The default job filter set to past 7 days.   In the Projects page \u0026gt; Models \u0026gt; Model Endpoints tab:  The “Model” column was removed. Added the model's URI to the tooltip when hovering over the \u0026quot;Name\u0026quot; in the column entry. The table is now sortable by name and by function (defaults to function).     Added the custom parameter “Function marketplace URL” when editing the service.  Miscellaneous  The Iguazio version in the About window now includes the patch version. For example, 3.0.2, instead of the previous 3.0.  Services  You can configure resources for Nuclio and Web API services in the Common Parameters tab of the service.  Deployment Specifications Cloud Deployments  Azure Kubernetes Service (AKS) is supported in Azure cloud deployments for application nodes, with the current configurations. Elastic Kubernetes Service (EKS) supports multiple node groups. Google Cloud Deployment (GCP) is supported. See the Installation and deployment guide. The supported Kubernetes version for managed Kubernetes (EKS, AKS, GKE) is 1.20.  Deprecated features  TSDB Nuclio is no longer supported. Zeppelin service is no longer supported.  Notes  Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.2/v3.2.0/","title":"Version 3.2.0 Release Notes"},{"content":" This document outlines the main additions and changes to the Iguazio MLOps Platform (\u0026quot;the platform\u0026quot;) in version 3.2.1, and known issues in this version.\nNew Features and Enhancements Cloud Deployments  Added support IGZTOP Performance Reporting Tool.   Upgraded managed Kubernetes for EKS only.   Improved performance when loading services screen.   Added support for private IP on GKE deployments.    The Presto service is no longer the default service and users are able to delete it. If the Presto service is deleted, Hive and MariaDB services will automatically be disabled.   In the GCP deployment page, when creating new nodes, custom labels can be added to node resources.   Enable users to set label names in AKS/GKE/EKS node groups in the deployment screens.   Updated Kubernetes support to version 1.20.   Added the role of Developer as part of the default roles when creating a new user.   [Tech Preview] Extend Nuclio Kafka to support the commercial Confluent version.   Nuclio now supports windowed ack only in v3io stream.  ","keywords":["release","notes"],"path":"https://github.com/jasonnIguazio/release-notes/version-3.2/v3.2.1/","title":"Version 3.2.1 Release Notes"},{"content":" This section contains installation and how-to guides for installing (deploying) and configuring the MLOps Platform (\u0026quot;the platform\u0026quot;) on-premises on supported virtualization platforms.\n","keywords":["virtual","machine","installation,","vm","installation,","virtual","machine","on-prem","installation,","vm","on-prem","installation,","vm","setup,","vm","installation,","vm","on-prem,","vm,","on-prem","configuration,","on-prem"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/","title":"Virtual-Machine (VM) Deployment"},{"content":" This section contains guides for installing (deploying) the MLOps Platform (\u0026quot;the platform\u0026quot;) on-prem on virtual machines (VMs) using the VMware vSphere virtualization platform.\n","keywords":["vmware","vsphere","vm","installation,","vmware","vsphere","instalaltion,","vsphere","instalaltion,","vmware","installation,","vmware","vsphere","setup,","vsphere","setup,","vmware","setup,","vmware","on-prem","installation,","vmware","vsphere,","vsphere,","vmware"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/","title":"VMware vSphere VM Deployment"},{"content":" This section contains how-to guides for on-prem deployments of the MLOps Platform (\u0026quot;the platform\u0026quot;) on virtual machines (VMs) using the VMware vSphere virtualization platform.\n","keywords":["vmware","vsphere","setup","how-to,","vsphere","setup","how-to,","vmware","setup","how-to,","vsphere","installation","how-to,","vmware","installation","how-to,","vsphere","how-to,","vmware","how-to,","vsphere","setup,","vmware","setup,","vsphere","installation,","vmware","installation,","vsphere","configuration,","vmware","configuration"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/","title":"VMware vSphere Deployment How-Tos"},{"content":" Overview This guide outlines the required steps for installing (deploying) an instance of the MLOps Platform (\u0026quot;the platform\u0026quot;) to virtual machines (VMs) using the VMware vSphere virtualization platform (vSphere). When you complete the procedure, you'll have a platform instance running on your vSphere virtual environment. The installation is done by using the platform installer — Provazio.\n Provisioning (deployment) of the platform's node VMs is done by using a dedicated virtualization package, provided by Iguazio.Don't attempt to provision the servers yourself prior to the deployment. Platform shutdown should be coordinated with Iguazio's support team. Don't shut down the data-node VMs non gracefully, as this might erase the data.    Prerequisites Before you begin, ensure that you have the following:\n  A Provazio API key and a Provazio vault URL, received from Iguazio.\n  Administrative access to the platform's vSphere cluster — an operational vSphere environment with one or more servers running the VMware ESXi hypervisor version 6.5, 6.7, 7.0 (\u0026quot;ESXi hosts\u0026quot;).\n  A vSphere platform virtualization package with OVF and VMDK files for each of the platform nodes, received from Iguazio.\n  The ESXi hosts have data stores with a minimum of 400 GB available storage for each of the platform nodes (to be used for running the nodes' VM boot-disk images).\n  Sufficient dedicated physical resources on the ESXi hosts to allow running the platform's node VMs without over-provisioning.\n  At least two 1 TB enterprise-grade SSDs for each of the platform's data-node VM.\n  The platform's ESXi hosts have the following network interfaces:\n A single-port 1 Gb (minimum) NIC for the management network For hosting data-node VMs — a dual-port 10 Gb (minimum) NIC for the data-path (client) and interconnect networks For hosting application-node VMs only — a single-port 10 Gb (minimum) NIC for the data-path (client) network  Each network must be on a different subnet, and the management network's subnet must be able to accommodate allocation of IP addresses for each of the platform's node VMs.\n  User credentials for configuring management IP addresses, received from Iguazio.\n  A machine running Docker.\n  For installations without internet connectivity (\u0026quot;offline installation\u0026quot;) —\n A platform installation package (\u0026lt;build name\u0026gt;.tar.gz), received from Iguazio. A preloaded Provazio dashboard Docker image (gcr.io/iguazio/provazio-dashboard:stable), received from Iguazio as an image archive (provazio-latest.tar.gz).    Deployment Steps To deploy an instance of the platform to a vSphere cluster, execute the following steps.\nStep 1: Configure virtual networking | Step 2: Deploy the platform nodes | Step 3: Attach data disks to the data nodes | Step 4: Configure management IP addresses | Step 5: Run the platform installer | Step 6: Access the installer dashboard | Step 7: Choose the VM scenario | Step 8: Configure general parameters | Step 9: Configure cluster parameters | Step 10: Configure node parameters | Step 11: Review the settings | Step 12: Wait for completion\nStep 1: Configure Virtual Networking Follow the Configuring Virtual Networking (vSphere) guide to configure virtual networking on the platform's vSphere cluster.\nStep 2: Deploy the Platform Nodes Follow the Deploying the Platform Nodes (vSphere) guide to deploy a set of VMs that will serve as the platform's data and application nodes.\nStep 3: Attach Data Disks to the Data Nodes Follow the Attaching Data Disks to the Data-Node VMs (vSphere) guide to attach data disks (storage devices) to the platform's data-node VMs.\nStep 4: Configure Management IP Addresses Follow the Configuring IP Addresses (vSphere) guide to configure IP addresses for the management network (\u0026quot;management IP addresses\u0026quot;) on the platform nodes, to allow the installer to connect to the node VMs.\nStep 5: Run the Platform Installer Offline-Installation NoteTo deploy the platform in environments without an internet connection (offline deployment), follow the instructions in the offline-installation guide instead of the procedure in this step, and then proceed to the next installation step.  To deploy In platform in environments with an internet connection (online deployment), run the platform installer, Provazio, by running the following command from a command-line shell on a server or a machine that is running Docker and has connectivity to the platform's data and application nodes:\ndocker pull gcr.io/iguazio/provazio-dashboard:stable \u0026\u0026 docker run --rm --name provazio-dashboard \\ -p 8060:8060 \\ -e PROVAZIO_API_KEY=\u0026lt;Provazio API Key -e PROVAZIO_VAULT_URL=\u0026lt;Provazio Vault URL \\ gcr.io/iguazio/provazio-dashboard:stable     API Key A Provazio API key, received from Iguazio (see the installation prerequisites).  Vault URL A Provazio vault URL, received from Iguazio (see the installation prerequisites). NoteDon\u0026#39;t include a slash (`\u0026#39;/\u0026#39;`) at the end of the vault URL.    Step 6: Access the Installer Dashboard In a web browser, browse to localhost:8060 to view the Provazio dashboard.\n   Select the plus-sign button (+) to create a new system.\nStep 7: Choose the VM Scenario On the Installation Scenario page, check Bare metal / virtual machines, and then select Next.\n   Step 8: Configure General Parameters On the General page, fill in the configuration parameters, and then select Next.\n    System Name A system ID of your choice, which will be part of the URL of your platform instance.  Valid Values: A string of 1–12 characters; can contain lowercase letters (a–z) and hyphens (-); must begin with a lowercase letter   Default Value: A randomly generated lowercase string   Description  A free-text string that describes the platform instance.  System Version  The platform version.  For online installations \u0026mdash; insert the release build number that you received from Iguazio (for example, \"3.2.0-b19.20211107120205\"). The installer will implicitly download the appropriate installation package for the specified build version.  For offline installations (in environments without an internet connection) \u0026mdash; set the value of this parameter to \"file://igzconf:igzconf@\u0026lt;IP of the installation-package data node\u0026gt;:/home/iguazio/installer/\u0026lt;platform version, as received from Iguazio\u0026gt;\"; replace the \u0026lt;...\u0026gt; placeholders with your specific data. Note that you first need to create a /home/iguazio/installer data-node directory and extract to this directory the platform installation-package archive that you received from Iguazio, as outlined in the offline-installation guide, which should have been executed in Step 5.    Owner Full Name An owner-name string, containing the full name of the platform owner, for bookkeeping.  Owner Email An owner-email string, containing the email address of the platform owner, for bookkeeping.  Username Username for a platform user to be created by the installation. This username will be used to log into the platform dashboard. You can add additional users after the platform is provisioned.  User Password Platform password for the user generated by the installation \u0026mdash; to be used with the configured username to log into platform dashboard; see the password restrictions. You can change this password after the platform is provisioned.  System Domain Custom domain (for example, \"customer.com\"). The installer prepends the value of the System Name parameter to this value to create the full system domain.  Termination Protection The protection level for terminating the platform installation from the installer dashboard.   Step 9: Configure Cluster Parameters On the Clusters page, fill in the configuration parameters, and then select Next.\n   Common Parameters (Data and Application Clusters) The following parameters are set for both the data and application clusters. Node references in the parameter descriptions apply to the platform's data-node VMs for the data cluster and the application-node VMs for the application cluster.\n Hypervisor  The hypervisor running on the cluster's hypervisor host machine. For vSphere, select VMware.  # of Cores  The number of CPU cores to allocate for each node.  Valid Values: 8 or 16   Memory (GB)  The amount of RAM to allocate for each node.  Valid Values: 122 or 244    Data-Cluster Parameters The following parameters are applicable only to the platform's data cluster:\n Dashboard Virtual IP Address  An IP address that will be used internally to load-balance access to the platform dashboard. Choose an available address from the subnet of the client network. NoteThis parameter isn\u0026#39;t required for platforms with a single data node.   Storage Devices  The names of the block-storage devices (data disks) that are attached to the data nodes, as configured in Step 3.   Application-Cluster Parameters The following parameters are applicable only to the platform's application cluster:\n Kubernetes Kind  Leave this set to New Vanilla Cluster (Iguazio Managed).  API Server Virtual IP Address  An IP address that will be used internally to load-balance access to the API server of the Kubernetes cluster. Choose an available address from the subnet of the client network. NoteThis parameter isn\u0026#39;t required for platforms with a single application node.   GPU Support  Check this option to configure GPU support for the application cluster. NoteThis option is applicable only when there are GPUs attached to the application nodes.    Step 10: Configure Node Parameters On the Nodes page, fill in the configuration parameters, and then select Next.\n   The configuration includes\n Adding nodes Configuring the nodes' IP addresses  Add Nodes Select Add Nodes to display the Add Nodes window. Configure the new-node parameters in this window, and then select Add.\n     # of Data Nodes The number of data nodes in the platform's data cluster; must be at least 3 to support high availability (HA).  Valid Values: 1 or 3   # of App Nodes The number of application nodes in the platform's application cluster; must be at least 3 to support high availability (HA).  Client Network Prefix The subnet of the data-path (client) network. Either use the default value or specify a custom private subnet.  Data Management Interface Name Leave this set to eth0.  App Management Interface Name Leave this set to eth0.  Interconnect Network Prefix The subnet of the interconnect network. Either use the default value or specify a custom private subnet.  Management Network MTU Leave this set to 1500.  Client Network MTU Leave this set to 1500.  Interconnect Network MTU Leave this set to 1500.   Configure the Nodes' IP Addresses On the Nodes page, for each node that you added, select the adjacent edit icon (), enter the node's management IP address, and select Save.\nStep 11: Review the Settings On the Review page, review and verify your configuration; go back and make edits, as needed; and then select Create to provision a new instance of the platform.\n   Step 12: Wait for Completion Provisioning a new platform instance typically takes around 30–40 minutes, regardless of the cluster sizes. You can download the provisioning logs, at any stage, by selecting Download logs from the instance's action menu.\n   You can also follow the installation progress by tracking the Provazio Docker container logs.\nWhen the installation completes, you should have a running instance of the platform on your vSphere cluster. You can use the Provazio dashboard to view the installed nodes (VMs). Then, proceed to the post-deployment steps.\nPost-Deployment Steps When the deployment completes, follow the post-deployment steps.\nSee Also  vSphere Deployment How-Tos On-Prem Deployment Specifications  ","keywords":["vmware","vsphere","installation,","vsphere","installation,","vmware","installation,","vsphere,","vmware,","vsphere","deployment,","vmware","deployment,","provazio,","platform","installer"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/vmware-installation-guide/","title":"Installing the Platform on VMware vSphere VMs"},{"content":" Overview Installing (deploying) the platform on virtual machines (VMs) in environments without an internet connection (a.k.a. \u0026quot;dark sites\u0026quot;) requires a variation on the online procedure that's described in the vSphere Installation Guide, as outlined in this guide.\nPrerequisites Before you begin, ensure that you have the following:\n A platform installation package (\u0026lt;build name\u0026gt;.tar.gz), received from Iguazio. A preloaded Provazio dashboard Docker image (gcr.io/iguazio/provazio-dashboard:stable), received from Iguazio as an image archive (provazio-latest.tar.gz). A Provazio dashboard API key, received from Iguazio. Administrative access to a platform vSphere cluster with the required networks configuration (see Configuring Virtual Networking (vSphere) and Configuring IP Addresses (vSphere)) and deployed VMs for each of the platform nodes (see Deploying the Platform Nodes (vSphere)).  Run The Platform Installer Offline Execute the following steps to run the platform installer (Provazio) without internet connectivity (offline installation):\n  Copy and extract the installation package:\n Establish an SSH connection to one of the platform's data-node VMs. Create an installer directory under /home/iguazio ($HOME): Copy the \u0026lt;build name\u0026gt;.tar.gz installation-package archive (see the prerequisites) to the new /home/iguazio/installer directory. Extract the package archive to the installer directory.    Copy and extract the Provazio dashboard Docker image:\n  Copy the provazio-latest.tar.gz preloaded Provazio dashboard Docker-image archive (see the prerequisites) to the installation machine that is running Docker.\n  Extract the provazio-latest.tar.gz Docker-image archive by running the following command on the machine that is running Docker:\ndocker load -i The output for a successful execution should look like this:\nLoaded image: gcr.io/iguazio/provazio-dashboard:stable     Run the platform installer, Provazio, by running the following command from a command-line shell; replace the \u0026lt;Provazio API Key\u0026gt; placeholder with the Provazio API key that you received from Iguazio:\ndocker run --rm --name provazio-dashboard \\ -p 8060:8060 \\ -e PROVAZIO_API_KEY=\u0026lt;Provazio API Key \\ \u0026#8201;gcr.io/iguazio/provazio-dashboard:stable    What's Next? After successfully running the platform installer, proceed to the installer dashboard-access step in the vSphere installation guide to configure installation parameters from the installer dashboard.\n","keywords":["offline","installation","on","vmware","vsphere","vms,","vmware","vsphere","offline","installation,","vsphere","offline","installation,","vmware","offline","installation,","vm","offline","installation,","vsphere","offline","deployment,","vmware","offline","deployment,","vm","offline","deployment,","dark-site","installation,","dark","site,","no","internet,","provazio"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/offline-install/","title":"Installing the Platform without an Internet Connection (Offline Installation) (vSphere)"},{"content":"Overview After following the VMware vSphere installation guide, you should have a running instance of the platform on your vSphere cluster. In some cases, additional post-deployment steps are required before you can properly use the platform, as outlined in this guide.\nRegistering a Custom Platform Domain If you chose to install the platform under a custom domain, you must register a few DNS records. If you need assistance, contact Iguazio's support team. Creating an HTTPS Certificate In some cases, you might need to create an HTTPS certificate for your platform installation. For more information, contact Iguazio's support team. Importing IdP Users and Groups from an Active Directory To import users and groups from an external Microsoft Active Directory, see the platform's IdP documentation. For additional assistance, contact Iguazio's support team. See Also  VMware vSphere installation guide  ","keywords":["vmware","vsphere","post","deployment,","vsphere","post","deployment","vmware","post","deployment,","vsphere","post","installation,","vmware","post","installation,","custom","domain","registration,","domain","registration,","ip","addresses,","network,","dns,","idp,","microsoft","active","directory,","active","directory,","microsoft","ad,","idp","users,","http","certificates"],"path":"https://github.com/jasonnIguazio/cluster-mgmt/deployment/on-prem/vm/vmware/howto/post-install-steps/","title":"Post-Installation Steps (vSphere)"},{"content":" Browse reference documentation for the MLOps Platform's RESTful web APIs for working with simple data objects and NoSQL and stream data.\n","keywords":["api","reference,","web","apis,","REST,","RESTful,","http,","python,","web","gateway,","nginx"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/","title":"RESTful Web-API References"},{"content":"The platform provides the following web APIs:\n  Simple-Object Web API Reference for working with data as simple data objects using an S3-like interface.\n  Data-service web APIs for complex manipulation of specific data types (see the Data-Service Web-API General Structure section for high-level structure and syntax details) —\n  Streaming Web API Reference for working with data streams.\n  NoSQL Web API Reference for working with NoSQL data objects.\n    See Also  Securing Your Web-API Requests RESTful Web and Management API Data Paths Working with Data Containers and Ingesting and Consuming Files tutorials — including web-API examples Web-APIs software specifications and restrictions  ","keywords":["web","apis,","REST,","RESTful,","simple-obejct","web","api,","data-service","web","apis,","web-api","structure,","api","structure,","streaming","web","api,","nosql","web","api,","streaming,","nosql,","http,","python"],"path":"https://github.com/jasonnIguazio/data-layer/reference/web-apis/overview/","title":"Overview of the Web APIs"},{"content":" Overview This tutorial outlines different methods for performing basic data-container tasks in the platform — listing the existing containers, creating and deleting containers and container directories, and browsing container contents.\nBefore You Begin This tutorial demonstrates how to perform basic tasks using different platform interfaces — dashboard, file system, web APIs, and cluster-management APIs.\n To use the file-system interface, you need to create a command-line service from which to run your code — such as a web-based shell, or Jupyter Notebook. See Creating a New Service. To understand how data paths are set in file-system commands, see File-System Data Paths. To send web-API requests, you need to have the URL of your tenant's web-APIs service (webapi), and either a platform username and password or an access key for authentication. To learn more and to understand how to structure and secure the web-API requests, see the web-APIs reference, and especially Data-Service Web-API General Structure and Securing Your Web-API Requests. (The tutorial Postman examples use the username-password authentication method, but if you prefer, you can replace this with access-key authentication, as explained in the documentation.) To understand how data paths are set in web-API requests, see RESTful Web and Management API Data Paths. To send cluster-management-API (\u0026quot;management-API\u0026quot;) requests, you need to have the URL of the management-API service and a session cookie for authentication. To learn more and understand how to structure and secure management-API requests, see the management-APIs reference, and especially General Management-API Structure and Create Session (including specific instructions for Postman). To understand how data paths are set in management-API requests, see RESTful Web and Management API Data Paths.  Listing Containers You can view information about your tenant's containers and related metadata from the dashboard, from the file-system interface, or by using the RESTful Container Management API or Simple-Object Web API. The dashboard also displays performance statistics for each container, which can also be retrieved with the management API [Beta]. Using the Dashboard To view information about the data containers of your tenant, in the dashboard's side navigation menu, select Data. The Data page displays a containers table that includes the name and performance-statistics summary of each container, as demonstrated in the following image:    When you select a container from the table, the Browse tab, which is selected by default, allows you to browse the contents of the container; see Browsing a Container.\nThe Overview tab provides more detailed information about the selected container, as demonstrated for the \u0026quot;bigdata\u0026quot; container in the following image:    The Data-Access Policy tab allows you to define data-access policy rules that restrict access to data in the container based on different criteria. For more information, see the Data-Access Policy Rules.\nUsing the File-System Interface You can list the data containers of your tenant from a command-line shell by running a local file-system ls command on the tenant's data root using the v3io data mount:\nls /v3io Using the Container Management API You can list the data containers of your tenant and see related metadata by using the List Containers operation of the platform's RESTful Containers Management API [Beta]. For example, to send the request from Postman, do the following:\n  Create a new request and set the request method to GET.\n  In the request URL field, enter the following; replace \u0026lt;management-APIs URL\u0026gt; with the HTTPS URL of the platform dashboard:\n\u0026lt;management-APIs URL\u0026gt;/api/containers/ For example:\nhttps://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers/   In the Headers tab, add the following headers:\n  Key Value   Content-Type application/json   Cookie session=\u0026lt;cookie\u0026gt;; replace \u0026lt;cookie\u0026gt; with a session cookie returned in the Set-Cookie response header of a previous Create Session request.     Select Send to send the request, and then check the response Body tab. Following is an example response body for a tenant that has only the predefined containers (\u0026quot;projects\u0026quot;, \u0026quot;bigdata\u0026quot;, and \u0026quot;users\u0026quot;): { \u0026#34;included\u0026#34;: [], \u0026#34;meta\u0026#34;: { \u0026#34;ctx\u0026#34;: \u0026#34;09348042905611315558\u0026#34; }, \u0026#34;data\u0026#34;: [ { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bigdata\u0026#34;, \u0026#34;imports\u0026#34;: [], \u0026#34;created_at\u0026#34;: \u0026#34;2021-01-17T12:41:10.258000+00:00\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;id\u0026#34;: 1026, \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;properties\u0026#34;: [] }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;id\u0026#34;: 1026 }, { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;imports\u0026#34;: [], \u0026#34;created_at\u0026#34;: \u0026#34;2021-01-17T12:41:10.959000+00:00\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;id\u0026#34;: 1027, \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;properties\u0026#34;: [] }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;id\u0026#34;: 1027 }, { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;projects\u0026#34;, \u0026#34;imports\u0026#34;: [], \u0026#34;created_at\u0026#34;: \u0026#34;2021-01-17T12:41:10.258000+00:00\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;id\u0026#34;: 1028, \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;properties\u0026#34;: [] }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;id\u0026#34;: 1028 } ] }   Using the Simple-Object Web API You can list the data containers of your tenant by using the GET Service operation of the platform's Simple-Object Web API, which resembles the Amazon Web Services S3 API.\nFor example, to send the request from Postman, do the following:\n  Create a new request and set the request method to GET.\n  In the request URL field, enter the URL of your tenant's web-APIs service (webapi).\nFor example:\nhttps://default-tenant.app.mycluster.iguazio.com:8443   In the Authorization tab, set the authorization type to \u0026quot;Basic Auth\u0026quot; and enter your username and password in the respective credential fields.   Select Send to send the request, and then check the response Body tab. Following is an example response body (in XML format) for a tenant that has only the two predefined containers (\u0026quot;bigdata\u0026quot; and \u0026quot;users\u0026quot;):\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;ListAllMyBucketsResult\u0026gt; \u0026lt;Owner\u0026gt; \u0026lt;ID\u0026gt;000000000000000000000000000000\u0026lt;/ID\u0026gt; \u0026lt;DisplayName\u0026gt;iguazio\u0026lt;/DisplayName\u0026gt; \u0026lt;/Owner\u0026gt; \u0026lt;Buckets\u0026gt; \u0026lt;Bucket\u0026gt; \u0026lt;Name\u0026gt;bigdata\u0026lt;/Name\u0026gt; \u0026lt;CreationDate\u0026gt;2021-01-17T12:41:10.258000+00:00\u0026lt;/CreationDate\u0026gt; \u0026lt;Id\u0026gt;1026\u0026lt;/Id\u0026gt; \u0026lt;/Bucket\u0026gt; \u0026lt;Bucket\u0026gt; \u0026lt;Name\u0026gt;users\u0026lt;/Name\u0026gt; \u0026lt;CreationDate\u0026gt;2021-01-17T12:41:10.959000+00:00\u0026lt;/CreationDate\u0026gt; \u0026lt;Id\u0026gt;1027\u0026lt;/Id\u0026gt; \u0026lt;/Bucket\u0026gt; \u0026lt;Bucket\u0026gt; \u0026lt;Name\u0026gt;projects\u0026lt;/Name\u0026gt; \u0026lt;CreationDate\u0026gt;2021-01-17T12:41:10.258000+00:00\u0026lt;/CreationDate\u0026gt; \u0026lt;Id\u0026gt;1028\u0026lt;/Id\u0026gt; \u0026lt;/Bucket\u0026gt; \u0026lt;/Buckets\u0026gt; \u0026lt;/ListAllMyBucketsResult\u0026gt;    Creating and Deleting Containers You can create a new data container or delete an existing container from the dashboard or by using the RESTful Container Management API.\nWarningTake extra care when deleting containers, to avoid data loss or other undesirable consequences. It's recommended that you close all open handles to the container before you delete it. For example, deleting a container without first deleting a Nuclio V3IO volume that references the container might result in consumption of extra Kubernetes resources.  Using the Dashboard Follow these steps to create a new container from the dashboard:\n  Navigate to the Data page and select the New Container button:      In the new-container window, enter a name and description for your new container. The following image demonstrates how to create a new container named \u0026quot;mycontainer\u0026quot;:      After you create a container, you can see it in the containers table on the Data page. For example, the following image shows a table with the predefined \u0026quot;bigdata\u0026quot; container and a \u0026quot;mycontainer\u0026quot; container:\n   To delete containers from the dashboard, navigate to the Data page. In the containers table, check the check boxes next to the containers that you want to delete and then select the delete icon () from the action toolbar and verify the delete operation when prompted. The following image demonstrates how to delete a \u0026quot;mycontainer\u0026quot; container:    Using the Container Management API You can create and delete a container by using the Create Container and Delete Container operations of the platform's RESTful Containers Management API [Beta], respectively. For example, to send the requests from Postman, do the following:\nSend a Create Container Request   Create a new request and set the request method to POST.\n  In the request URL field, enter the following; replace \u0026lt;management-APIs URL\u0026gt; with the HTTPS URL of the platform dashboard:\n\u0026lt;management-APIs URL\u0026gt;/api/containers/ For example:\nhttps://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers/   In the Headers tab, add the following headers:\n  Key Value   Content-Type application/json   Cookie session=\u0026lt;cookie\u0026gt;; replace \u0026lt;cookie\u0026gt; with a session cookie returned in the Set-Cookie response header of a previous Create Session request.     In the Body tab, select the raw format and add the following JSON code; you can change the name and descriptions in the example, subject to the container naming restrictions:\n{ \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Test container\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34; } }   Select Send to send the request, and then check the response Body tab. For a successful request, the ID of the new container is returned in the response-data id element. Copy this ID, as it's required by some of the other management operations, such as Delete Container. Following is an example Create Container response body for a new container with ID 1030:\n{ \u0026#34;included\u0026#34;: [], \u0026#34;meta\u0026#34;: { \u0026#34;ctx\u0026#34;: \u0026#34;13053711680930917876\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;relationships\u0026#34;: { \u0026#34;mappings\u0026#34;: { \u0026#34;data\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;container_map\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;2244cd09-5e7e-4957-a38b-c99d97c946a2\u0026#34; } ] }, \u0026#34;created_by\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6e040a9a-9403-44bd-8f90-a61e079c6c45\u0026#34; } }, \u0026#34;storage_class\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;storage_class\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;f8bec94d-9151-475d-98f6-962827ca49ad\u0026#34; } }, \u0026#34;owner\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6e040a9a-9403-44bd-8f90-a61e079c6c45\u0026#34; } }, \u0026#34;tenant\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;tenant\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;b7c663b1-a8ee-49a9-ad62-ceae7e751ec8\u0026#34; } }, \u0026#34;active_mapping\u0026#34;: { \u0026#34;data\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;container_map\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;2244cd09-5e7e-4957-a38b-c99d97c946a2\u0026#34; } } }, \u0026#34;attributes\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Test container\u0026#34;, \u0026#34;imports\u0026#34;: [], \u0026#34;created_at\u0026#34;: \u0026#34;2021-01-18T10:05:54.400000+00:00\u0026#34;, \u0026#34;operational_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;id\u0026#34;: 1030, \u0026#34;admin_status\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;data_lifecycle_layers_order\u0026#34;: [], \u0026#34;data_policy_layers_order\u0026#34;: [], \u0026#34;properties\u0026#34;: [], \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container\u0026#34;, \u0026#34;id\u0026#34;: 1030 } }   If you list your tenant's containers, you can now see your container in the list, and you can browse and modify its contents in the same way as for the predefined containers. For example, you can see and browse your container from the dashboard's Data page.\nSend a Delete Container Request   Create a new request and set the request method to DELETE.\n  In the request URL field, enter the following; replace \u0026lt;management-APIs URL\u0026gt; with the HTTPS URL of the platform dashboard and \u0026lt;container ID\u0026gt; with the ID of the new container that you created in the previous step :\n\u0026lt;management-APIs URL\u0026gt;/api/containers/\u0026lt;container ID\u0026gt; For example, the following URL uses the container ID from the example in the previous step — 1030:\nhttps://dashboard.default-tenant.app.mycluster.iguazio.com/api/containers/1030   In the Headers tab, add a Cookie header (Key) and set its value to session=\u0026lt;cookie\u0026gt;; replace \u0026lt;cookie\u0026gt; with a session cookie returned in the Set-Cookie response header of a previous Create Session request.\n  Select Send to send the request, and then check the response. For a successful request, the response-data type is container_deletion and the container_id attribute shows the ID of the deleted container. Following is an example Delete Container response body for a container with ID 1030: { \u0026#34;included\u0026#34;: [], \u0026#34;meta\u0026#34;: { \u0026#34;ctx\u0026#34;: \u0026#34;11505092891179116359\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;container_id\u0026#34;: 1030, \u0026#34;job_id\u0026#34;: \u0026#34;6e7f9bf5-4a7c-4efb-83b7-31785fa82183\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;container_deletion\u0026#34;, \u0026#34;id\u0026#34;: 0 } }   You can also confirm the container deletion from the dashboard: in the side navigation menu, select Data. The deleted container should no longer appear in the containers table on the Data page.\nCreating and Deleting Container Directories You can create and delete container directories from the dashboard or by using the file-system interface. You can use the same procedure to create a directory (folder) either as a direct child of the parent container or as a nested child of another directory.\nNote that the dashboard allows you to delete only empty directories. Therefore, to delete a directory with content, it's typically better to use the file-system interface to run a delete recursive command.\nTable and Stream DirectoriesNoSQL tables and streams that you create in the platform are stored in container directories; for more information, see Working with NoSQL Data and Working with Data Streams (\u0026quot;Streaming\u0026quot;). Note:\n  When you create a new table or stream, all directories within the target container in the specified path are created automatically if they don't already exist.\n  To delete a table or stream, simply delete the respective directory, as outlined in this tutorial.\n    WarningTake extra care when performing a recursive delete operation, to avoid losing valuable data.  Using the Dashboard Follow these steps to create a new container directory from the dashboard:\n  Navigate to the Data page and select a container from the containers table.\n  In the Browse tab (selected by default), select the new-folder icon () from the action toolbar and then enter the name of the new directory (folder) when prompted.      To delete a container directory from the dashboard, you must first delete all files and subdirectories in the directory. You cannot delete a directory with content from the dashboard. To delete an empty container directory, select the directory from the browse table in the Browse container data tab. Check all items in the directory and then select the delete icon () from the action toolbar and confirm the delete operation when prompted. After you delete all items in the directory, return to the browse table and repeat the procedure for any non-empty directory that you want to delete. When you're done, check the check boxes next to the directories that you want to delete in the container browse table, select the delete icon from the action toolbar, and confirm the delete operation. The following image demonstrates how to delete an empty container directory:    Using the File-System Interface You can create and delete (remove) directories by running a [mkdir], [rm -r], or [rmdir] file-system command, respectively, from a command-line shell. For example:\n  The following commands create a mydata directory in the predefined \u0026quot;bigdata\u0026quot; container and a directory by the same name in the running-user directory of the \u0026quot;users\u0026quot; container:\nLocal file-system —\nmkdir -p /v3io/bigdata/mydata mkdir -p /User/mydata Hadoop FS —\nhadoop fs -mkdir v3io://bigdata/mydata hadoop fs -mkdir $V3IO_HOME_URL/mydata   The following commands remove (delete) the container directories created in the previous examples:\nLocal file-system —\nrm -r /v3io/bigdata/mydata rm -r /User/mydata Hadoop FS —\nhadoop fs -rm -r v3io://bigdata/mydata hadoop fs -rm -r $V3IO_HOME_URL/mydata   NoteYou can replace /User in the local-file system examples with /v3io/$V3IO_HOME, /v3io/users/$V3IO_USERNAME, or /v3io/users/\u0026lt;username\u0026gt; (for example, /v3io/users/iguazio for running-user \u0026quot;iguazio\u0026quot;).  You can replace $V3IO_HOME_URL in the Hadoop FS examples with v3io://$V3IO_HOME, v3io://users/$V3IO_USERNAME, or v3io://users/\u0026lt;username\u0026gt; (for example, v3io://users/iguazio for running-user \u0026quot;iguazio\u0026quot;).  See File-System Data Paths.  Browsing a Container You can browse the contents of a container and its directories from the dashboard, by using the file-system interface, or by using the Simple-Object Web API.\nUsing the Dashboard To browse the contents of a container from the dashboard, navigate to the Data page and select a container from the containers table. The Browse tab is selected by default and displays the contents of the container. See, for example, the containers-table image in the new-container creation instructions.\nTo view a directory's metadata, select the directory (folder) in the container's browse table by single-clicking the directory name or checking the adjacent check box. You can then also perform directory actions (such as delete) by selecting the relevant icon from the action toolbar. The following image demonstrates a selected mydata directory in the predefined \u0026quot;bigdata\u0026quot; container:    To view a directory's contents, double-click the directory in the browse table or select the directory from the container's navigation tree, as demonstrated in the following image:    Using the File-System Interface You can run a file-system list-directory command (ls) to browse the contents of a container or specific container directory from a command-line shell. For example:\n  List the contents of the root directory of the predefined \u0026quot;bigdata\u0026quot; container:\nLocal file-system —\nls -lF /v3io/bigdata/ Hadoop FS —\nhadoop fs -ls v3io://bigdata/   List the contents of a users/iguazio/mydata container directory where \u0026quot;iguazio\u0026quot; is the running user of the command-line shell service ($V3IO_USERNAME). All of the following syntax variations execute the same copy command:\nLocal file-system —\nls -lFA /User/mydata ls -lFA /v3io/$V3IO_HOME/mydata ls -lFA /v3io/users/$V3IO_USERNAME/mydata ls -lFA /v3io/users/iguazio/mydata/ Hadoop FS —\nhadoop fs -ls $V3IO_HOME_URL/mydata hadoop fs -ls v3io://$V3IO_HOME/mydata/ hadoop fs -ls v3io://users/$V3IO_USERNAME/mydata/ hadoop fs -ls v3io://users/iguazio/mydata/   Using the Simple-Object Web API You can list the objects in a container by using the GET Container operation of the platform's Simple-Object Web API, which resembles the Amazon Web Services S3 API.\nFor example, to send the request from Postman, do the following:\n  Create a new request and set the request method to GET.\n  In the request URL field, enter the following; replace \u0026lt;web-APIs URL\u0026gt; with the URL of the parent tenant's web-APIs service (webapi), and replace \u0026lt;container name\u0026gt; with the name of the container that you want to browse:\n\u0026lt;web-APIs URL\u0026gt;/\u0026lt;container name\u0026gt; For example, the following URL sends a request to web-API service URL https://default-tenant.app.mycluster.iguazio.com:8443 to browse the \u0026quot;bigdata\u0026quot; container:\nhttps://default-tenant.app.mycluster.iguazio.com:8443/bigdata   In the Authorization tab, set the authorization type to \u0026quot;Basic Auth\u0026quot; and enter your username and password in the respective credential fields.   Select Send to send the request, and then check the response Body tab. Following is an example response body (in XML format):\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;ListBucketResult\u0026gt; \u0026lt;Name\u0026gt;bigdata\u0026lt;/Name\u0026gt; \u0026lt;Prefix/\u0026gt; \u0026lt;Marker/\u0026gt; \u0026lt;Delimiter\u0026gt;/\u0026lt;/Delimiter\u0026gt; \u0026lt;NextMarker\u0026gt;tmp\u0026lt;/NextMarker\u0026gt; \u0026lt;MaxKeys\u0026gt;1000\u0026lt;/MaxKeys\u0026gt; \u0026lt;IsTruncated\u0026gt;false\u0026lt;/IsTruncated\u0026gt; \u0026lt;CommonPrefixes\u0026gt; \u0026lt;Prefix\u0026gt;mydata/\u0026lt;/Prefix\u0026gt; \u0026lt;/CommonPrefixes\u0026gt; \u0026lt;CommonPrefixes\u0026gt; \u0026lt;Prefix\u0026gt;naipi_files/\u0026lt;/Prefix\u0026gt; \u0026lt;/CommonPrefixes\u0026gt; \u0026lt;CommonPrefixes\u0026gt; \u0026lt;Prefix\u0026gt;tmp/\u0026lt;/Prefix\u0026gt; \u0026lt;/CommonPrefixes\u0026gt; \u0026lt;/ListBucketResult\u0026gt;    See Also  Data Containers The Data-Layer APIs Ingesting and Preparing Data Ingesting and Consuming Files  ","keywords":["containers,","getting","started,","create","container,","delete","container,","browser","container,","directories,","folders,","create","directory,","delete","directory,","create","folder,","delete","folder,","web","apis,","cluster-management","apis,","management","apis,","Create","Container,","Delete","Container,","List","Containers,","file","system,","dfs,","hadoop,","hdfs,","v3io,","v3io","paths,","table","paths"],"path":"https://github.com/jasonnIguazio/data-layer/containers/working-with-containers/","title":"Working with Data Containers"},{"content":" Overview Streams are used for storing sequenced information that is rapidly and continuously produced and consumed. Stream data is stored as a continuous series of data records, which can include any type of data. The platform treats the data as a binary large object (blob).\nFor optimal performance and scalability, the platform allows dividing a stream into multiple shards, so that records are ingested and consumed sequentially per shard. See the detailed explanation in the Stream Sharding and Partitioning section in this document.\nAfter a data container is created and configured, you can create one or more streams in the container. The following image shows a container with Web Clicks and Server Logs streams:\nA producer adds records to a stream. For example, an IoT device that sends sensor data to the platform is a producer. A consumer retrieves and processes stream data sequentially.\nStreams are identified by their user-assigned names, which must be unique within the parent container's relative stream path.\nWhen creating a stream, you also set its retention period. All stream records are guaranteed to be available for consumption during the stream's retention period. After this period elapses, existing records might be deleted to make room for new records, starting from the earliest ingested records.\n You can manage and access stream data in the platform by using the Spark or web APIs.\nTerminology Comparison The following table compares the MLOps Platform's streaming terminology with that of similar third-party tools and the platform's file interface:\n   Stream Sharding and Partitioning For optimal performance and to enable scalability of consumers, records are best distributed among shards, which are uniquely identified data sets within a stream. When a record is added to a stream, it is assigned to a specific shard from which it can subsequently be retrieved. Sequencing of records by their ingestion time (the time the data arrived at the platform) is guaranteed within each shard but not across shards.\nBy default, the platform handles assignment of records to shards internally, using the Randomized Round Robin (RRR) algorithm. However, the producer can select to assign new stream records to specific shards by providing the shard ID for each record. Alternatively, the producer can select to use custom partition keys to influence the designation of records to shards: as explained in the following Record Metadata section, the producer can optionally provide partition-key metadata for new records. The platform uses a hash function to map a record's partition key (if provided) to a specific shard, so that in the context of a fixed shard structure all records with the same key are mapped to the same shard (subject to the documented exceptions). This mechanism allows you to group similar records together and ensure a specific sequence of record consumption. If both a shard ID and a partition key are provided, the record is assigned based on the shard ID and the partition key does not affect the shard assignment.\nWhen creating a stream, you must configure its shard count (the number of shards in the stream). You can increase the shard count at any time, but you cannot reduce it. From the platform's perspective, there is virtually no cost to creating even thousands of shards. Note that after increasing a stream's shard count, new records with a previously used partition key are assigned either to the same shard that was used for this partition key or to a new shard. All records with the same partition key that are added after the shard-count change will be assigned to the same shard (be it the previously used shard or a new shard).\nThe following diagram depicts a platform data stream with sequenced shard records:\n   Record Metadata The platform returns to the consumer, together with the records, metadata for each record. The metadata includes the record's sequence number and ingestion time, as well as the following optional user metadata if provided by the producer during the record submission:\n Client information Custom client information, which is opaque to the platform (provided as a blob). You can use this metadata, for example, to save the data format of a record, or the time at which a sensor or application event was triggered.  Partition key A partition key to associate with the record. Records with the same partition-key values are typically assigned to the same shard (unless you override the partition key with a shard ID) — see Stream Sharding and Partitioning.  Stream Record Consumption  Multiple consumer instances can consume data from the same stream. A consumer retrieves records from a specific shard. It is recommended that you distribute the data consumption among several consumer instances (\u0026quot;workers\u0026quot;), assigning each instance one or more shards.\nFor each shard, the consumer should determine the location within the shard from which to begin consuming records. This can be the earliest ingested record, the end of the shard, the first ingested record starting at a specific time, or a specific record identified by its sequence number (a unique record identifier that is assigned by the platform based on the record's ingestion time). The consumer first uses a seek operation to obtain the desired consumption location, and then provides this location as the starting point for its record consumption. The consumption operation returns the location of the next record to consume within the shard, and this location should be used as the location for a subsequent consumption operation, allowing for sequential record consumption.\nThe following diagram demonstrates records ingestion into multiple stream shards (for example, using the Streaming Web API PutRecords operation), and records consumption from a specific shard (for example, using the Streaming Web API GetRecords operation):\n   Deleting Streams Currently, most platform APIs don't have a dedicated method for deleting a stream. However, you can use the file-system interface to delete a stream directory from the relevant data container:\nrm -r \u0026lt;path to stream\u0026gt; The following examples delete a \u0026quot;mystream\u0026quot; stream from a \u0026quot;mycontainer\u0026quot; container:\n  Local file-system command —\nrm -r /v3io/mycontainer/mystream   Hadoop FS command —\nhadoop fs -rm -r v3io://mycontainer/mystream   See Also  Data-Layer References  Streaming Web API Reference Spark-Streaming Integration API Reference    ","keywords":["streaming,","streams,","stream","shards,","shards,","sharding,","stream","partititons,","partitioning,","stream","records,","record","metadata,","record","sequence","number,","PutRecords,","GetRecords,","scalability,","stream","consumption,","parallel","consumption,","record","ingestion,","stream","delete,","stream","producer,","stream","consumer,","stream","seek,","seek","records,","get","records,","stream","retention,","retention","period,","terminology"],"path":"https://github.com/jasonnIguazio/data-layer/stream/","title":"Working with Data Streams (\"Streaming\")"},{"content":" Overview The platform provides a NoSQL database service, which supports storage and consumption of data in a tabular format. A table is a collection of data objects known as items (rows), and their attributes (columns). For example, items can represent people with attribute names such as Name, Age, and PhoneNumber.\nYou can manage and access NoSQL data in the platform by using the NoSQL Frames, Spark DataFrame, or web APIs.\nTerminology Comparison The following table compares the MLOps Platform's NoSQL DB terminology with that of similar third-party tools and the platform's file interface:\n   Creating Tables NoSQL tables in the platform don't need to be created prior to ingestion. When writing data to a NoSQL table, if the table doesn't exit, it's automatically created in the specified path as part of the write operation.\nDeleting Tables Currently, most platform APIs don't have a dedicated method for deleting a table. An exception to this is the delete method of the V3IO Frames client's NoSQL backend. However, you can use the file-system interface to delete a table directory from the relevant data container:\nrm -r \u0026lt;path to table\u0026gt; The following examples delete a \u0026quot;mytable\u0026quot; table from a \u0026quot;mycontainer\u0026quot; container:\n  Local file-system command —\nrm -r /v3io/mycontainer/mytable   Hadoop FS command —\nhadoop fs -rm -r v3io://mycontainer/mytable   Partitioned Tables Table partitioning is a common technique for optimizing physical data layout and related queries. In a partitioned table, some item attributes (columns) are used to create partition directories within the root table directory using the format \u0026lt;table path\u0026gt;/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;[/\u0026lt;attribute\u0026gt;=\u0026lt;value\u0026gt;/...], and each item is then saved to the respective partition directory based on its attribute values. For example, for a \u0026quot;mytable\u0026quot; table with year and month attribute partitions, an item with attributes year = 2018 and month = 1 will be saved to a mytable/year=2018/month=1/ directory. This allows for more efficient data queries that search for the data only in the relevant partition directories instead of scanning the entire table. This technique is used, for example, by Hive, and is supported for all the built-in Spark Dataset file-based data sources (such as Parquet, CSV, and JSON). The platform includes specific support for partitioning NoSQL tables with Spark DataFrames [Tech Preview], and for querying partitioned tables with Frames, Spark DataFrames [Tech Preview], or Presto. For more information, see the Frames, Spark, and Presto references.\nPartitioning Best PracticesWhen defining table partitions, follow these guidelines:\n Partition the table according to the queries that you expect to be run most often. For example, if you expect most queries to be by day, define a single day partition. However, if you expect most queries to be based on other attributes, partition your table accordingly; for example, if most queries will be by country, state, and city, create country/state/city partition directories. Bear in mind that the number of partitions and their sizes affect the performance of the queries: to optimize performance, avoid partitions that are too small (less than 10Ks of items) by consolidating partitions, especially if you have many partitions. For example, partitioning a table by seconds or minutes will result in a huge amount of very small partitions and therefore querying the table is likely to be inefficient, especially if you expect to have occasional scans of a larger period of time (such as a year).    Read Optimization By default, read requests (queries) on a NoSQL table are processed by scanning all items in the table, which affects performance. However, the platform's NoSQL APIs support two types of read optimizations that can be used to improve performance:\n Item-specific queries Range Scans  Both optimizations involve queries on the table's primary key or its components and rely on the way that data objects (such as table items) are stored in the platform: the name of the object is the value of its primary-key attribute; the object is mapped to a specific data slice according to the value of its sharding key (which is also the primary key for simple object names); and objects with a compound primary key that have the same sharding-key value are sorted on the data slice according to the value of their sorting key. See Object Names and Primary Keys.\nIt's recommended that you consider these optimizations when you select your table's primary key and plan your queries. For more information and best-practice guidelines, see Best Practices for Defining Primary Keys and Distributing Data Workloads.\nFaster Item-Specific Queries The fastest table queries when using the platform's NoSQL Web API or the Iguazio Presto connector are those that uniquely identify a specific item by its primary-key value. Such queries are processed more efficiently because the platform searches the names of the object files only on the relevant data slice and stops the search when the requested item is located. See the web-API GetItem operation and the Presto CLI references.\nRange Scans A range scan is a query for specific sharding-key values and optionally also for a range of sorting-key values, which is processed by searching the sorted object names on the data slice that is associated with the specified sharding-key value(s). Such processing is more efficient than the standard full table scan, especially for large tables.\nWhen using Frames, NoSQL Spark DataFrames, or Presto, the platform executes range scans automatically for compatible queries. When using the NoSQL Web API, you can select whether to perform a range scan, a parallel scan, or a standard full table scan. Note that to support and use range scans effectively, you need to have a table with a compound \u0026lt;sharding key\u0026gt;.\u0026lt;sorting key\u0026gt; primary key that is optimized for your data set and expected data-access patterns. When using a NoSQL Spark DataFrame or Presto, the table must also contain sharding- and sorting-key user attributes. Note that range scans are more efficient when using a string sorting-key attribute. For more information and best-practice guidelines, see the Best Practices for Defining Primary Keys and Distributing Data Workloads guide, and especially the Using a Compound Primary Key for Faster NoSQL Table Queries and Using a String Sorting Key for Faster Range-Scan Queries guidelines.\nSee Also  Accessing Platform NoSQL and TSDB Data Using the Frames Library Data-Layer References.  Frames NoSQL-Backend API Reference NoSQL Web API Reference  Spark Datasets API Reference   Using Presto Data Objects  Best Practices for Defining Primary Keys and Distributing Data Workloads    ","keywords":["working","with","nosql","databases,","working","with","nosql","data,","working","with","key-value","data,","working","with","kv","data,","nosql","databases,","nosql","dbs,","nosql","data,","key-value","data,","kv","data,","nosql","tables,","kv","tables,","nosql,","key-value,","kv,","datbases,","tables,","table","items,","rows,","attributes,","columns,","deleting","tables,","partitioning,","partitions,","range","scan,","items","names,","object","names,","primary","key,","sharding","key,","sorting","key,","terminology"],"path":"https://github.com/jasonnIguazio/data-layer/nosql/","title":"Working with NoSQL (Key-Value) Data"},{"content":" Overview The Services page of the platform dashboard displays information about running application services — which include both default services that are created as part of the platform's deployment (such as the web-APIs or Presto services) and user-defined services (such as Spark or Jupyter Notebook). A service administrator can create, delete, enable, disable, restart, and configure application services, and view service logs. A user-defined service is typically assigned to a specific running user and can also optionally be shared with all other users in the same tenant. Except for service administrators, who can view all services, users can see and use only the services that they're running and the shared services (including the default services). For more information about the platform's application services, see The Platform's Application Services. For information about the required permissions for viewing, running, and managing application services in the platform, see Security.\nNote   When running an application service from another service, the service is executed using the permissions of the running user of the parent service. For example, if user \u0026quot;c\u0026quot; logs into a shared web-shell service for running user \u0026quot;a\u0026quot;, which is configured to work with a Spark service for running user \u0026quot;b\u0026quot;, and runs Spark from the web shell—Spark is executed with the permissions of user \u0026quot;a\u0026quot; (the running user of the parent web-shell service), and not as user \u0026quot;c\u0026quot; (the running user of the Spark service) or user \u0026quot;b\u0026quot; (the logged-in user of the shell service).\n  Check the product software specifications and restrictions and release notes for additional restrictions and known issues related to application services.\n    DNS-Configuration Prerequisite As a prerequisite to using the platform's application services, you need to configure conditional forwarding for your cluster's DNS server. For more information and step-by-step instructions, see Configuring the DNS Server.\nCreating a New Service Follow these steps to create a new application service from the dashboard. You must have the Service Admin management policy to create a service:\n  In the side navigation menu, select Services.\n   On the Services page, select the New Service option from the top action toolbar.\n  Select the desired service type, configure the required parameters, and optionally configure additional parameters. You can also select to share the service with all users of the parent tenant. Note  Common parameters tab—You can configure the memory and CPU resources for the service from the Resources section of the Common Parameters dashboard tab. For empty parameter fields, the platform uses the default system values. The platform doesn't perform logical validation of your configuration. It's up to you to balance the resource needs of your application services, taking into account the available resources of your platform environment. When setting the resource limits, consider that an insufficient limit might result in the termination of the service. If you're using the Iguazio trial, remember that your environment has limited resources (see the trial evaluation overview). For some services, you must define the running user.\n  Custom parameters tab:—Individual services may have additional custom parameters. For example, you can configure the number of replicas (workers) for the Spark and Presto services. The customizable parameters are described in each service topic.\n  Dependent services—Some services allow you to configure related services to be used by the new service, from among the services that are accessible to the selected running user. For example, you can configure a Spark service to be used for running Spark jobs from a web-based shell, or Jupyter Notebook. (The web-shell services and Jupyter Notebook use an internal Spark service by default.)\n  Configuration changes—You can change the service's configuration at a later stage, after its initial deployment.\n      Optionally repeat the previous steps to create as many services as you wish.\n  When you're done defining services, remember to select Apply Changes from the top action toolbar, and wait for confirmation of a successful deployment. The deployment might take a while to complete, depending on the number and type of services that you create.\n  See Also  The Platform's Application Services The Platform Dashboard (UI) Security  Services Management Policies The Service Admin management policy   Monitoring Platform Services Data Layer—learn how to use the application services to access and work with data in the platform Introducing the Platform  ","keywords":["working","with","services,","working","with","application","services,","application","services,","services,","creating","services,","service","create,","service","fundamentals,","fundamentals,","dashboard,","ui"],"path":"https://github.com/jasonnIguazio/services/fundamentals/","title":"Working with Services"},{"content":" The platform comes pre-deployed with the Iguazio V3IO TSDB library (\u0026quot;V3IO TSDB\u0026quot;) for working with data in time-series databases (TSDBs). This library exposes a high-performance API for creating, updating, querying, and deleting time series databases. You can access the API with:\n The V3IO TSDB CLI tool (tsdbctl) that is pre-deployed as part of the default platform installation, for seamless integration with the platform. The TSDB APIs included in the pre-deployed platform application services V3IO Frames, which expose a Python API.  V3IO TSDB was developed by Iguazio as an open-source project. The full V3IO TSDB source code is in the public V3IO TSDB GitHub repository. NoteFor restrictions related to the platform's TSDB support, refer to the Software Specifications and Restrictions.  See Also  The TSDB CLI Accessing Platform NoSQL and TSDB Data Using the Frames Library Frames TSDB-Backend API Reference Data Layer  ","keywords":["working","with","time-series","databases,","working","with","tsdbs,","time-series","databaes,","tsdb,","time","series,","v3io","tsdb,","tsdb","cli,","tsdbctl,","v3io-python","sdk","tsdb","api,","v3io-python","sdk,","v3io-py","tsdb","api,","v3io-py,","v3io","frames","tsdb","backend,","v3io","frames","tsdb","api,","v3io","frames,","frames","tsdb","api,","frames","tsdb,","v3io","tsdb","nuclio","functions,","tsdb","nuclio","functions,","tsdb","nuclio","serverless","function,","tsdb","serverless","functions"],"path":"https://github.com/jasonnIguazio/data-layer/tsdb/working-with-tsdbs/","title":"Working with Time-Series Databases (TSDBs)"},{"content":" Description Writes (ingests) data from pandas DataFrames to a NoSQL table.\nNote NoSQL tables in the platform don't need to be created prior to ingestion. When writing (ingesting) data into a table that doesn't exist, the table and all directories in the specified path are automatically created. When updating a table item (i.e., when writing an item with the same primary-key attribute value as an existing table item), the existing item is overwritten and replaced with the written item. All items that are written to a given table must conform to the same schema. For more information, see the Table Schema overview.    Syntax write(backend, table, dfs, [condition=\u0026#34;\u0026#34;, max_rows_in_msg=0, index_cols=None])  NoteThe method has additional parameters that aren't currently supported for the NoSQL backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after dfs.\n  Parameters backend | condition | dfs | index_cols | max_rows_in_msg | table\n backend The backend type — \u0026quot;nosql\u0026quot; or \u0026quot;kv\u0026quot; for the NoSQL backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a NoSQL table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/nosql/my_table\u0026quot;.\n Type: str   Requirement: Required   dfs One or more DataFrames containing the data to write.\nNote The written DataFrames must include a single index column whose value is the value of the item's primary-key attribute (name); see Item Name and Primary Key. You can either include the index column as part of the DataFrame definition (as typically done with pandas DataFrames) or by using the index_cols parameter of the write method, which overrides any index-column definitions in the DataFrame. If you don't define an index column using either of these methods, Frames assigns the name idx to the auto-generated pandas range-index column (pandas.RangeIndex) and uses it as the item's primary-key attribute. You should therefore refrain from using the name idx for non-index columns (regular user attributes) in NoSQL items. See the maximum write DataFrame size restriction. If you need to write a larger amount of data, use multiple DataFrames.     Type: A single DataFrame, a list of DataFrames, or a DataFrames iterator   Requirement: Required   index_cols A list of column (attribute) names to be used as index columns for the write operation, for all ingested DataFrames (as set in the dfs parameter) regardless of any index-column definitions in the DataFrames. By default, the DataFrames' index columns are used. The NoSQL backend supports a single mandatory index column, which represents the item's primary-key attribute. See details in the index-column note for the dfs parameter.\n Type: []str   Requirement: Optional   Default Value: None   condition A Boolean condition expression that defines a conditional logic for executing the write operation. See Condition Expression for syntax details and examples. The condition is applied to each written DataFrame row (item).\nTo reference an item attribute in the target table from within the expression, use the attribute name — \u0026lt;attribute name\u0026gt;; for example, \u0026quot;is_init==true\u0026quot; references an is_init attribute in the table.  To reference an attribute (column) in the written DataFrame, embed the attribute name within curly braces — {\u0026lt;column name\u0026gt;}; for example, \u0026quot;{age}\u0026gt;18\u0026quot; references an age attribute in the DataFrame.  For example, an \u0026quot;{is_stable} == true AND \u0026quot;{version} \u0026gt; version)\u0026quot; condition indicates that each DataFrame item (row) should be written to the table only if it has an is_stable attribute whose value is true and the target table has a matching item (with the same primary-key attribute value) whose current version attribute value is lower than the value of the version attribute (column) in the DataFrame.\n Requirement: Optional   Default Value: None   max_rows_in_msg The maximum number of DataFrame rows to write in each message (i.e., the size of the write chunks). When the value of this parameter is 0 (default), each DataFrame is written in a single message.\n Type: int   Requirement: Optional   Default Value: 0    Examples Following are some usage examples for the write method of the Frames NoSQL backend:\n  Write a single DataFrame with several rows (items) to a mytable table in the client's data container (table). Use the pandas set_index fuction to define username as the DataFrame's index column, which identifies the table's primary-key attribute.\ntable = \u0026#34;mytable\u0026#34; data = [ [\u0026#34;lisaa\u0026#34;, \u0026#34;Lisa\u0026#34;, \u0026#34;Andrews\u0026#34;, 11, \u0026#34;US\u0026#34;, \u0026#34;DC\u0026#34;], [\u0026#34;toms\u0026#34;, \u0026#34;Tom\u0026#34;, \u0026#34;Stein\u0026#34;, 10, \u0026#34;Israel\u0026#34;, \u0026#34;TLV\u0026#34;], [\u0026#34;nickw\u0026#34;, \u0026#34;Nickolas\u0026#34;, \u0026#34;Weber\u0026#34;, 15, \u0026#34;Germany\u0026#34;, \u0026#34;Berlin\u0026#34;], [\u0026#34;georgec\u0026#34;, \u0026#34;George\u0026#34;, \u0026#34;Costas\u0026#34;, 13, \u0026#34;Greece\u0026#34;, \u0026#34;Athens\u0026#34;], [\u0026#34;christ\u0026#34;, \u0026#34;Chris\u0026#34;, \u0026#34;Thompson\u0026#34;, 10, \u0026#34;UK\u0026#34;, \u0026#34;London\u0026#34;], [\u0026#34;julyj\u0026#34;, \u0026#34;July\u0026#34;, \u0026#34;Johnes\u0026#34;, 14, \u0026#34;US\u0026#34;, \u0026#34;NY\u0026#34;] ] attrs = [\u0026#34;username\u0026#34;, \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;] df = pd.DataFrame(data, columns=attrs) df.set_index(\u0026#34;username\u0026#34;, inplace=True) client.write(backend=\u0026#34;nosql\u0026#34;, table=table, dfs=df)   Write a DataFrames iterator to a my_tables/students_11-15 table in the client's data container (table). Use the read method's index_cols parameter to set the index column (primary-key attribute) for all of the iterator DataFrames to the username attribute. Use a similar data set to that used in Example 1, but write only the DataFrames rows (items) for which the value of the age column (attribute) is larger or equal to 11 and smaller than 15 (condition).\ntable = \u0026#34;/my_tables/students_11-15\u0026#34; attrs = [\u0026#34;username\u0026#34;, \u0026#34;first_name\u0026#34;, \u0026#34;last_name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;] dfs = [ pd.DataFrame([[\u0026#34;lisaa\u0026#34;, \u0026#34;Lisa\u0026#34;, \u0026#34;Andrews\u0026#34;, 11, \u0026#34;US\u0026#34;, \u0026#34;DC\u0026#34;]], columns=attrs), pd.DataFrame([[\u0026#34;toms\u0026#34;, \u0026#34;Tom\u0026#34;, \u0026#34;Stein\u0026#34;, 10, \u0026#34;Israel\u0026#34;, \u0026#34;TLV\u0026#34;]], columns=attrs), pd.DataFrame([[\u0026#34;nickw\u0026#34;, \u0026#34;Nickolas\u0026#34;, \u0026#34;Weber\u0026#34;, 15, \u0026#34;Germany\u0026#34;, \u0026#34;Berlin\u0026#34;]], columns=attrs), pd.DataFrame([[\u0026#34;georgec\u0026#34;, \u0026#34;George\u0026#34;, \u0026#34;Costas\u0026#34;, 13, \u0026#34;Greece\u0026#34;, \u0026#34;Athens\u0026#34;]], columns=attrs), pd.DataFrame([[\u0026#34;christ\u0026#34;, \u0026#34;Chris\u0026#34;, \u0026#34;Thompson\u0026#34;, 10, \u0026#34;UK\u0026#34;, \u0026#34;London\u0026#34;]], columns=attrs), pd.DataFrame([[\u0026#34;julyj\u0026#34;, \u0026#34;July\u0026#34;, \u0026#34;Johnes\u0026#34;, 14, \u0026#34;US\u0026#34;, \u0026#34;NY\u0026#34;]], columns=attrs) ] client.write(\u0026#34;nosql\u0026#34;, table=table, dfs=dfs, condition=\u0026#34;{age}\u0026gt;=11 AND {age}\u0026lt;15\u0026#34;, index_cols=[\u0026#34;username\u0026#34;])   See Also  Creating NoSQL Tables Frames NoSQL-Backend Overview  Item Name and Primary Key   Frames Client Constructor  ","keywords":["write","method,","frames","nosql","write","method,","frames","kv","write","method,","frames","write,","frames","nosql","write,","frames","kv","write,","frames","client","write,","frames","client","nosql","write,","frames","client","kv","write,","frames","write","reference,","frames","nosql","write","reference,","frames","kv","write","reference,","nosql","ingestion,","conditional","write,","pandas","DataFrames,","DataFrames","index","columns,","data","ingestion,","backend,","condition,","dfs,","index_cols,","max_rows_in_msg,","table"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/nosql/write/","title":"write Method"},{"content":" Description Writes (ingests) data from pandas DataFrames to a TSDB table.\nSyntax write(backend, table, dfs, [labels=None, max_rows_in_msg=0, index_cols=None])  NoteThe method has additional parameters that aren't currently supported for the TSDB backend. Therefore, when calling the method, be sure to explicitly specify the names of all parameters after dfs.\n  Parameters backend | dfs | index_cols | labels | max_rows_in_msg | table\n backend The backend type — \u0026quot;tsdb\u0026quot; for the TSDB backend. See Backend Types.\n Type: str   Requirement: Required   table The relative path to the backend data — a directory in the target data container (as configured for the client object) that represents a TSDB table. For example, \u0026quot;mytable\u0026quot; or \u0026quot;examples/tsdb/my_metrics\u0026quot;.\n Type: str   Requirement: Required   dfs One or more DataFrames containing the data to write.\nNote DataFrame index columns —  You must define one or more non-index DataFrame columns that represent the sample metrics; the name of the column is the metric name and its values is the sample data (i.e., the ingested metric). See also TSDB metric-samples sofware specifications and restrictions. You must define a single index column whose value is the value the sample time of the data. Note that a TSDB DataFrame cannot have more than one index column of a time data type. You can optionally define string index columns that represent metric labels for the current DataFrame row. See also TSDB-labels sofware specifications and restrictions. Note that you can also define labels for all DataFrame rows by using the labels parameter (in addition or instead of using column indexes to apply labels to a specific row). You can either include the index columns as part of the DataFrame definition (as typically done with pandas DataFrames) or by using the index_cols parameter of the write method, which overrides any index-column definitions in the DataFrame.   See the maximum write DataFrame size restriction. If you need to write a larger amount of data, use multiple DataFrames.     Type: A single DataFrame, a list of DataFrames, or a DataFrames iterator   Requirement: Required   labels A dictionary of metric labels of the format {\u0026lt;label\u0026gt;: \u0026lt;value\u0026gt;[, \u0026lt;label\u0026gt;: \u0026lt;value\u0026gt;, ...]}, which will be applied to all the DataFrame rows (i.e., to all the ingested metric samples). For example, {\u0026quot;os\u0026quot;: \u0026quot;linux\u0026quot;, \u0026quot;arch\u0026quot;: \u0026quot;x86\u0026quot;}. See also TSDB-labels sofware specifications and restrictions. Note that you can also define labels for a specific DataFrame row by adding a string index column to the row (in addition or instead of using the labels parameter to define labels for all rows), as explained in the description of the dfs parameter.\n Type: dict with str keys   Requirement: Optional   Default Value: None   index_cols A list of column (attribute) names to be used as index columns for the write operation, for all ingested DataFrames (as set in the dfs parameter) regardless of any index-column definitions in the DataFrames. By default, the DataFrames' index columns are used. As explained for the dfs parameter, the TSDB backend supports a single mandatory time index column that represents the sample time of the data and multiple optional string index columns that represent metric labels.\n Type: []str   Requirement: Optional   Default Value: None   max_rows_in_msg The maximum number of DataFrame rows to write in each message (i.e., the size of the write chunks). When the value of this parameter is 0 (default), each DataFrame is written in a single message.\n Type: int   Requirement: Optional   Default Value: 0    Examples Following are some usage examples for the write method of the Frames TSDB backend. See the DataFrame index-columns note in the description of the dfs parameter for information regarding the alternative methods for defining the metric sample-time and label attributes for the write operation, which are demonstrates in the examples.\nBoth examples use the following functions to generate random metrics data:\nimport numpy as np # Generate a matrix of random floating-point numbers between 0 and `max` def gen_floats(d0=1, d1=1, max=100): return np.random.rand(d0, d1) * max   Write a DataFrame with time-series sample metrics to a mytsdb TSDB table in the client's data container (table); define the metric sample-time and label attributes as DataFrame index columns:\nfrom datetime import datetime metrics = [\u0026#34;cpu\u0026#34;, \u0026#34;temperature\u0026#34;] times = pd.date_range(freq=\u0026#34;1S\u0026#34;, start=datetime(2019, 1, 1, 0, 0, 0, 0), end = datetime(2019, 1, 1, 23, 59, 59, 0)) df = pd.DataFrame(data=gen_floats(num_items, len(metrics)), index=[times, [\u0026#34;1\u0026#34;] * len(times)], columns=metrics) df.index.names = [\u0026#34;time\u0026#34;, \u0026#34;node\u0026#34;] tsdb_table = \u0026#34;mytsdb\u0026#34; client.write(backend=\u0026#34;tsdb\u0026#34;, table=tsdb_table, dfs=df)   Write time-series metric samples to a tsdb/my_metrics table in the client's data container (table), using one of two alternative variations; both variations use the following code for generating the data set:\nfrom datetime import datetime, timedelta metrics = [\u0026#34;cpu\u0026#34;, \u0026#34;memory\u0026#34;, \u0026#34;disk\u0026#34;] num_metrics = len(metrics) label_sets = [ {\u0026#34;site\u0026#34;: \u0026#34;DC\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;}, {\u0026#34;site\u0026#34;: \u0026#34;DC\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;windows\u0026#34;}, {\u0026#34;site\u0026#34;: \u0026#34;NY\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;windows\u0026#34;}, {\u0026#34;site\u0026#34;: \u0026#34;SC\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;}, {\u0026#34;site\u0026#34;: \u0026#34;SC\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;windows\u0026#34;}, {\u0026#34;site\u0026#34;: \u0026#34;NY\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;} ] end_t = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) start_t = end_t - timedelta(days=2) times = pd.date_range(freq=\u0026#34;5min\u0026#34;, start=start_t, end=end_t) num_items = len(times) Variation 1 — set the sample-time attribute as a DataFrame index column and use the write method's labels parameter to define the metric-label attributes; use multiple single-DataFrame (dfs) write calls:\ntsdb_table = \u0026#34;/tsdb/my_metrics\u0026#34; for label_set in label_sets: df = pd.DataFrame(data=gen_floats(num_items, num_metrics), index=times, columns=metrics) df.index.name = \u0026#34;time\u0026#34; client.write(\u0026#34;tsdb\u0026#34;, table=tsdb_table, dfs=df, labels=label_set) Variation 2 — define the metric sample-time and label attributes by using the write method's index_cols parameter; use a single write call with multiple DataFrames (dfs):\ndfs = [] for label_set in label_sets: df = pd.DataFrame(data=gen_floats(num_items, num_metrics), index=[times, [label_set[\u0026#34;site\u0026#34;]] * num_items, [label_set[\u0026#34;host\u0026#34;]] * num_items, [label_set[\u0026#34;os\u0026#34;]] * num_items], columns=metrics) df.index.names = index_cols df.reset_index(inplace=True) dfs.append(df) tsdb_table = \u0026#34;/tsdb/my_metrics\u0026#34; index_cols = [\u0026#34;time\u0026#34;, \u0026#34;site\u0026#34;, \u0026#34;host\u0026#34;, \u0026#34;os\u0026#34;] client.write(\u0026#34;tsdb\u0026#34;, table=tsdb_table, dfs=df, index_cols=index_cols)   See Also  Adding Samples to a TSDB (The TSDB CLI) Frames TSDB-Backend Overview Frames Client Constructor  ","keywords":["write","method,","frames","tsdb","write","method,","frames","write,","frames","tsdb","write,","frames","client","write,","frames","client","tsdb","write,","frames","write","reference,","frames","tsdb","write","reference,","tsdb","ingestion,","data","ingestion,","TSDB","metrics,","TSDB","labels,","pandas","DataFrames,","DataFrame","index","columns,","backend,","dfs,","index_cols,","labels,","max_rows_in_msg,","table"],"path":"https://github.com/jasonnIguazio/data-layer/reference/frames/tsdb/write/","title":"write Method"}]