<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark Datasets Python and Scala API References on Iguazio MLOps Platform Documentation</title><link>/data-layer/reference/spark-apis/spark-datasets/</link><description>Recent content in Spark Datasets Python and Scala API References on Iguazio MLOps Platform Documentation</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/data-layer/reference/spark-apis/spark-datasets/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark DataFrame Data Types</title><link>/data-layer/reference/spark-apis/spark-datasets/spark-df-data-types/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/data-layer/reference/spark-apis/spark-datasets/spark-df-data-types/</guid><description>The platform currently supports the following Spark DataFrame data types. For a description of the Spark types, see the Spark SQL data types documentation For a general reference of the attribute data types that are supported in the platform, see the Attribute Data Types Reference.
NoteThe platform implicitly converts between Spark DataFrame column data types and platform table-schema attribute data types, and converts integer (IntegerType) and short (ShortType) values to long values (LongType / &amp;quot;long&amp;quot;) and floating-point values (FloatType) to double-precision values (DoubleType / &amp;quot;double&amp;quot;).</description></item><item><title>Overview of Spark Datasets</title><link>/data-layer/reference/spark-apis/spark-datasets/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/data-layer/reference/spark-apis/spark-datasets/overview/</guid><description>Using Spark DataFrames A Spark Dataset is an abstraction of a distributed data collection that provides a common way to access a variety of data sources. A DataFrame is a Dataset that is organized into named columns (&amp;quot;attributes&amp;quot; in the platform's unified data model). See the Spark SQL, DataFrames and Datasets Guide. You can use the Spark SQL Datasets/DataFrames API to access data that is stored in the platform. In addition, the platform's Iguazio Spark connector defines a custom data source that enables reading and writing data in the platform's NoSQL store using Spark DataFrames â€” including support for table partitioning, data pruning and filtering (predicate pushdown), performing &amp;quot;replace&amp;quot; mode and conditional updates, defining and updating counter table attributes (columns), and performing optimized range scans.</description></item><item><title>The NoSQL Spark DataFrame</title><link>/data-layer/reference/spark-apis/spark-datasets/nosql-dataframe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/data-layer/reference/spark-apis/spark-datasets/nosql-dataframe/</guid><description>Introduction The platform includes the Iguazio Spark connector, which defines a custom Spark data source for reading and writing NoSQL data in the platform's NoSQL store using Spark DataFrames. A Spark DataFrame of this data-source format is referred to in the documentation as a NoSQL DataFrame. This data source supports data pruning and filtering (predicate pushdown), which allows Spark queries to operate on a smaller amount of data; only the data that is required by the active job is loaded.</description></item></channel></rss>