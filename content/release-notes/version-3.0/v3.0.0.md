---
title:    "Version 3.0.0 Release Notes"
subtitle: "Release Date: 9 Feb 2021"
keywords: "release notes"
menu:
  main:
    name:       "V3.0.0 Release Notes"
    parent:     "release-notes-3.0"
    identifier: "release-notes-3.0.0"
    weight:     1000
---
{{< comment >}}<!--
- [TODO-NEXT-RNS] Add `{{< in-progress >}}` while working on the release notes.
  => DONE for v3.0.0.
- TODO: Update the front-matter metadata and the version number in the opening
  paragraph and at the start of the #highlights section.
  => DONE for v3.0.0.
- TODO: Check for internal "[TODO-NEXT-RNS]" comments for changes to be made in
  the next release notes (and add/keep comments for the new release).
  => DONE for v3.0.0. (3.3.21) All current next-RN comments are for post 3.0.0.
- TODO: Add Highlights. => (3.3.21) Done for v3.0.0 with open questions.
- TODO: At the end, update the subsection links in each h2 RN section and check
  all section cross-references/links to ensure they're relevant to the final
  release notes for this version.
  Also, check for the applicability of the Tech Preview and Beta notes.
  => DONE for v3.0.0.
- [TODO-POST-REVIEW] Replace the internal-release-note "reviewer" param for the
  reviewed notes with the "reviewed" param.
  => (8.3.21) DONE for v3.0.0.
- TODO: Handle any [TODO-BEFORE-PUBLICATION] comments for open RN issues.
  => DONE for v3.0.0 (7.3.21) No remaining open issues.
-->
<!-- [IntInfo] Permanent KIs are not linked to a specific Jira ticket in the
  internal info and are marked internally as `**[PERMANENT-KI]**`. -->
{{< /comment >}}

This document outlines the main additions and changes to the Iguazio MLOps Platform ("the platform") in version 3.0.0, and known issues in this version.

{{< techpreview note="1" >}}

{{< comment >}}<!-- [IntInfo] (sharonl) The RNs don't include beta features. -->
{{< beta note="1" >}}
{{< /comment >}}

{{< internal-release-note id="internal-release-jira-n-confluence-links" >}}
**V3.0.0 Release DOC Task:** {{< jira ig="17510" >}} ; **V3.0.0 Release-Notes Sub-task:** {{< jira ig="17511" >}}
<br/>
**[Release Index](https://confluence.iguazeng.com/display/RM/Release+Index) (Confluence):** &mdash; [3.0.0](https://confluence.iguazeng.com/display/RM/Release+Index#ReleaseIndex-3.0.0 )
<br/>
**Build:** `3.0_b27_20210205135416` (9.2.21) {{< comment >}}<!-- [TODO-NEXT-RNS] Edit. # DONE for v3.0.0 -->
{{< /comment >}}
<br/>
**[V3.0.0 Status](https://jira.iguazeng.com/secure/Dashboard.jspa?selectPageId=11606)** ;
**[V3.0.0 Program](https://jira.iguazeng.com/secure/Dashboard.jspa?selectPageId=11401)**
<br/>
**V3.0.0 Requirements** &mdash; [all &mdash; Requirements, Sub-Requirements, Improvements](https://jira.iguazeng.com/issues/?jql=project%20%3D%20ig%20AND%20(%22Target%20Version%22%20in%20(3.0.0)%20OR%20fixVersion%20in%20(3.0.0)%20OR%20affectedVersion%20in%20(3.0.0))%20AND%20issuetype%20in%20(Requirement%2C%20Sub-Requirement%2C%20Improvement)); [Requirements](https://jira.iguazeng.com/issues/?jql=project%20%3D%20ig%20AND%20(%22Target%20Version%22%20in%20(3.0.0)%20OR%20fixVersion%20in%20(3.0.0)%20OR%20affectedVersion%20in%20(3.0.0))%20AND%20issuetype%20%3D%20Requirement)
<br/>
**V3.0.0 release-notes tickets:** [Jira query](https://jira.iguazeng.com/issues/?jql=labels%20in%20(RN-3.0.0%2C%20RN-3.0.0-fixes%2C%20RN-3.0.0-known-issues%2C%20RN-3.0.0-TechPreview)){{< comment >}}<!-- `v3.0.0_RN_done` filter (#13603)  -->
  <!-- [ci-comment-shcd-extra-space] Moving the opening `comment` tag to the
    next line results in extra space in the output, even if the <br/> tag is
    removed. -->
{{< /comment >}}
<br/>
**Issues intentionally excluded from the v3.0.0 RNs:** [Jira query](https://jira.iguazeng.com/issues/?jql=labels%20in%20(RN-3.0.0%2C%20RN-3.0.0-fixes%2C%20RN-3.0.0-known-issues%2C%20RN-3.0.0-TechPreview))
{{< comment >}}<!-- `v3.0_RN_excluded` filter (#13129) -->
{{< /comment >}}
{{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Highlights-->
{{< rn-heading t="highlights" >}}

Version 3.0.0 introduces many new powerful features and enhancements, as well as bug fixes, as detailed in the release notes.
Following are some of the main highlights of this release:

- [Official support for MLRun](#new-mlrun-official-support-and-version-upgrade) and [integration of the MLRun UI into the platform UI](#new-ui-projects-redeisgn-w-mlrun-integration)
- Extensive project improvements, including UI and API changes for working in the context of a project.
    Among the improvements &mdash;

    - [Dashboard (UI) <gui-title>Projects</gui-title> redesign](#new-ui-projects) with new views and features
    - A new [predefined "projects" container](#new-predefined-projects-container)
- Support for [Spark Operator](#new-spark-operator-service)
- Support for [Amazon EKS](#new-amazon-eks-support) deployment, including usage-based pricing
- Added [more Grafana dashboards for application-cluster monitoring](#new-grafana-app-cluster-monitoring-dashboards)
- Support for [SSH connection to the web-shell service](#new-web-shell-ssh), which enables secured connections from remote IDEs
- A new [OAuth2 (OIDC) Authenticator service](#new-authenticator-service) (`authenticator`), which enables secured access of external (non-platform) users to Nuclio API gateways and shared Grafana dashboards in the platform
- Support for [small AWS data nodes](#new-aws-cloud-i32xlarge-data-nodes) (i3.2xlarge EC2 instance types)
- Enhanced the support for [asynchronous file upload from the dashboard (UI)](#new-ui-async-file-upload)
- [Upgraded versions](#new-managed-app-services) of pre-deployed and certified software, including [Spark 2.4.5](#new-spark-version-upgrade) and [Presto 332](#new-presto-version-upgrade)

<!-- //////////////////////////////////////// -->
<!-- New Features and Enhancements -->
{{< rn-heading t="new-and-enhance" >}}

[Application Services](#new-managed-app-services) | [Authenticator](#new-authenticator-service) | [AWS specifications](#new-cloud-deployment-specs) | [Azure specifications](#new-cloud-deployment-specs) | [Dashboard (UI](#new-dashboard) [Deployment Specifications](#new-deployment-specs) | [Frames](#new-frames) | [General](#new-general) | [Grafana](#new-logging-n-monitoring-services) | [Hive](#new-presto-n-hive) | [Jupyter](#new-jupyter) | [Logging and Monitoring Services](#new-logging-n-monitoring-services) | [MLRun](#new-mlrun) | [Nuclio](#new-nuclio) | [Presto](#new-presto-n-hive) | [Prometheus](#new-tsdb-prometheus) | [Python SDK](#new-v3io-py) | [Security](#new-security-n-user-management) | [Spark](#new-spark) | [TSDB](#new-tsdb) | [TSDB Nuclio Functions](#new-tsdb-nuclio-funcs) | [User Management](#new-security-n-user-management) | [Web Shell](#new-web-shell)

<!-- ---------------------------------------- -->
{{< rn-heading id="new-general" text="General" >}}

- <a id="new-predefined-projects-container"></a>Added a new predefined data container named **"projects"** for storing shared project artifacts.

    {{< internal-release-note reviewed="odedm" rnid="new-predefined-projects-container" ig="15834" type="req" owner="Hedi I. (hedii)" docig="17802" >}}
<br/>
(sharonl) (1.3.21) In consultation with Orit and Adi, it was decided not to mention in the release notes that the "projects" containers replaces "bigdata" as the default platform container.
We don't want users to rely on any default-container functionality, and we've also removed default-container references from the non-RN doc.
It was also ultimately decided, in consultation with Adi, not to mention in the release notes the intention of no longer predefining the "bigdata" container in future releases.
We mention this in the v3.0.0 non-RN documentation.
<br/>
See also the MLRun [#new-mlrun-default-projects-artifacts-path-change](#new-mlrun-default-projects-artifacts-path-change) release-notes enhancement for changing the default project artifacts path to <path>projects/&lt;project name&gt;/artifacts</path>.
{{< comment >}}<!-- [c-projects-default-container] [c-bigdata-container-rm]
  [IntInfo] See info in data/vars/product.toml. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-expressions-if_else-func"></a>Added a new <func>if_else</func> expression function for applying if-else conditional logic in both update and condition expressions:
    ```
    if_else(<Boolean condition expression>, <"then" expression>, <"else" expression>)
    ```

    {{< internal-release-note reviewed="assafb" rnid="new-expressions-if_else-func" ig="16196" type="req" owner="Alex T. (alext)" docig="17600" >}}
<br/>
(sharonl) (12.3.21) The <func>if_else</func> function was actually added in v1.7 or v1.5 (see Requirement {{< jira ig="5023" >}}, canceled Tech Preview DOC {{< jira ig="9719" >}}), but it was never documented because it wasn't tested by QA and wasn't really used internally either.
In v3.0.0, QA tested and verified the function and it was decided to document it.
    {{< /internal-release-note >}}

- <a id="new-table-scan-performance-imprv"></a>Improved table-scan performance for large data containers.

    {{< internal-release-note reviewed="assafb" rnid="new-table-scan-performance-imprv" ig="15089" type="req" owner="Alex T. (alext)" >}}
    {{< /internal-release-note >}}

- <a id="new-ghpages-doc-site-restruct"></a>Restructured the platform documentation site to improve the user experience.
    As part of these changes, the MLRun documentation is now available also as part of the platform documentation (see {{< xref f="ds-and-mlops/" >}}).

    {{< internal-release-note reviewed="gilads" rnid="new-ghpages-doc-site-restruct" owner="Sharon L. (sharonl)" docig="17510" >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-managed-app-services" text="Managed Application Services" >}}

{{< note id="new-services-other-secs-ref-note" >}}
- For information about the new default OAuth2 (OIDC) Authenticator service (`authenticator`), see the [Security and User Management](#new-authenticator-service) new features and enhancements.
    {{< comment >}}<!-- [c-authenticator-service] -->
    {{< /comment >}}
- For information about UI improvements related to managing application services, see the [Dashboard (UI)](#new-dashboard) new features and enhancements.
{{< /note >}}

- <a id="new-k8s-version-upgrade"></a>Upgraded to Kubernetes version 1.17.14.

    {{< internal-release-note rnid="new-k8s-version-upgrade" ig="16414" type="req" owner="Vladimir S. (vladimirs)" docig="17513" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-jupyter" text="Jupyter Notebook" >}}
{{< internal-release-note reviewed="urih,gilads" >}}
{{< /internal-release-note >}}

{{< note id="new-jupyter-mlrun-sec-ref-note" >}}
See also the [MLRun](#new-mlrun) new features and enhancements for improvements related to using MLRun from Jupyter Notebook.
{{< /note >}}
{{< comment >}}<!-- [TODO-NEXT-RNS] Check #new-mlrun for relevance. -->
{{< /comment >}}

- <a id="new-jupyter-versions-upgrade"></a>Upgraded versions of pre-deployed and certified software &mdash;

    {{< internal-release-note rnid="new-jupyter-version-upgrade" docig="17513" >}}
    {{< /internal-release-note >}}

    - <a id="new-jupyterlab-version-upgrade"></a>JupyterLab version 2.2.0
        {{< internal-release-note rnid="new-jupyterlab-version-upgrade" ig="16377" type="req" owner="Uri H. (urih)" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-v3io-tutorials-version-upgrade"></a>Platform tutorial Jupyter notebooks &mdash; version 3.0
    - <a id="new-jupyter-v3io-py-sdk-version-upgrade"></a>V3IO Python SDK &mdash; version 0.5
        {{< internal-release-note rnid="new-jupyter-v3io-py-sdk-version-upgrade" ig="16765" type="req" owner="Uri H. (urih)" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-frames-client-version-upgrade"></a>V3IO Frames client &mdash; version 0.8
        {{< internal-release-note rnid="new-jupyter-frames-client-version-upgrade" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-nuclio-version-upgrade"></a>Nuclio Jupyter library (`nuclio-jupyter`) version 0.8
        {{< internal-release-note rnid="new-jupyter-nuclio-version-upgrade" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-conda-version-upgrade"></a>Conda version 4.8.3
        {{< internal-release-note rnid="new-jupyter-conda-version-upgrade" ig="16376" type="req" owner="Urih H. (urih)" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-nvidia-cuda-version-upgrade"></a>NVIDIA CUDA version 11.0
        {{< internal-release-note rnid="new-jupyter-nvidia-cuda-version-upgrade" ig="16888" type="req" owner="Urih H. (urih)">}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-nvidia-rapids-upgrade"></a>NVIDIA RAPIDS version 0.17
        {{< internal-release-note rnid="new-jupyter-nvidia-rapids-upgrade" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-python-pkgs-upgrade"></a>Additional pre-deployed Python packages upgrade
        {{< internal-release-note rnid="new-jupyter-python-pkgs-upgrade" ig="16888" type="req" owner="Uri H. (urih)" >}}
        {{< /internal-release-note >}}

- <a id="new-jupyter-rmv-presto-dependency"></a>Removed the dependency of the Jupyter Notebook service on the Presto service.

    {{< internal-release-note rnid="new-jupyter-rmv-presto-dependency" >}}
**R&D Tickets:** Requirement {{< jira ig="16847" >}}; Bug {{< jira ig="17332" >}}<br/>
**Owner:** Uri H. (urih)
    {{< /internal-release-note >}}

- <a id="new-jupyter-gpu-flavor-wo-reserving-gpus"></a>In environments with GPUs, enabled selecting the <gui-label>Jupyter Full Stack with GPU</gui-label> flavor in the dashboard service configuration without reserving any GPU resources (i.e., without setting <gui-label>Resources | GPU | Limit</gui-label>).

    {{< internal-release-note reviewed="erann" rnid="new-jupyter-gpu-flavor-wo-reserving-gpus" ig="16158" type="req" owner="Eran N. (erann)" docig="17507" >}}
    {{< /internal-release-note >}}

- <a id="new-tutorials-n-demos-infra-imprv"></a>Platform Jupyter tutorials and demos infrastructure improvements &mdash;

    - <a id="new-tutorials-demos-n-gs-tutorial-mv-to-mlrun-demos"></a>Consolidated the sources for the platform's end-to-end use-case application demos and moved all demo sources and the getting-started tutorial to the MLRun demos GitHub repository ([mlrun_demos](https://github.com/mlrun/demos)).
        See also the [MLRun-demos enhancements note](#new-mlrun-demos-imprv).
        {{< internal-release-note rnid="new-tutorials-demos-n-gs-tutorial-mv-to-mlrun-demos" ig="16920" type="req" owner="Uri H. (urih)" docig="17429" >}}
        {{< /internal-release-note >}}
    - <a id="new-tutorials-update-demos-script-imprv-n-rename"></a>Improved the script for retrieving and updating local demo copies, including new options for downloading a specific version, and renamed the script from <file>get-additional-demos.sh</file> to <file>update-demos.sh</file>.
        By default, the script now attempts to download the latest version of the demos that matches the version of the installed MLRun Python package (`mlrun`).
        {{< internal-release-note rnid="new-tutorials-update-demos-script-imprv-n-rename" ig="17544" type="bug" owner="Uri H. (urih)" docig="17551" >}}
<br/>
(sharonl) (1.3.21) {{< jira ig="17544" >}} is for the renaming of the demos-update script.
There's no dedicated ticket for the script's rewrite, but it's related to the demo-sources consolidation and demos pre-deployment tasks (see the previous and next bullets).
        {{< /internal-release-note >}}
    - <a id="new-tutorials-update-script-imprv"></a>Improved the <file>igz-tutorials-get.sh</file> script for updating the local tutorial copies from the tutorials GitHub repository ([v3io/tutorials](https://github.com/v3io/tutorials/)) to support downloading a specific version and to download the latest compatible version for the current platform version by default.
        {{< internal-release-note rndis="new-tutorials-update-script-imprv" >}}
(sharonl) (4.3.21) There's no dedicated Jira ticket for this, but I confirmed with Gilad and Uri and in the code itself.
        {{< /internal-release-note >}}
    - <a id="new-mlrun-demos-predeploy"></a>Pre-deployed the MLRun demos in the platform.
        Upon the creation of the first Jupyter Notebook service for a given user, the platform now automatically downloads the demos version that matches the version of the installed MLRun service and copies them to a <dirname>demos</dirname> directory in the running user's home directory (<path>/User/demos</path>).
        If the download fails, such as in offline environments without an internet connection, the demo files are copied from a backup in the platform image (in which case you might not get the latest compatible version).
        {{< internal-release-note rnid="new-mlrun-demos-predeploy" ig="16963" type="req" owner="Uri H. (urih)" docig="16967" >}}
<br/>
(sharonl) (1.3.21) We use the new <file>update-demos.sh</file> script (see the previous bullet) to deploy the demos to the <path>/User/demos</path> for the user's first new Jupyter NB service.
Because the `mlrun` Python package isn't installed at this time, we use the <opt>--mlrun-ver</opt> flag to get the demos version that matches the version of the installed MLRun service.
        {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-v3io-py" text="V3IO Python SDK" >}}

- <a id="new-v3io-py-sdk-version-upgrade"></a>Upgraded to V3IO Python SDK (`v3io-py`) version 0.5.

    {{< internal-release-note reviewed="urih" rnid="new-v3io-py-sdk-version-upgrade" ig="1676" type="req" owner="Uri H. (urih)" docig="17513" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-frames" text="V3IO Frames" >}}
{{< internal-release-note reviewed="dinan" >}}
{{< /internal-release-note >}}

- <a id="new-frames-version-upgrade"></a>Upgraded to Frames server and client versions 0.8.

    {{< internal-release-note rnid="new-frames-version-upgrade" owner="Dina N. (dinan)" docig="17513" >}}
    {{< /internal-release-note >}}

- <a id="new-frames-pandas-1-0-data-types"></a>Added write support for new pandas 1.0 data types &mdash; <api>BooleanDType</api>, <api>Integer32DType</api>, <api>Integer64DType</api>, and <api>StringDtype</api>.
    Note that as with the data types used in previous pandas releases, values of these types are implicitly converted to the corresponding platform table-schema data types &mdash; <api>"boolean"</api>, <api>"long"</api>, and <api>"string"</api>.

    {{< internal-release-note rnid="new-frames-pandas-1-0-data-types" ig="16302" type="req" owner="dinan" docig="17505" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-mlrun" text="MLRun" >}}
{{< internal-release-note reviewed="odedm,gilads" >}}
{{< /internal-release-note >}}

{{< note id="new-mlrun-ui-sec-ref-note" >}}
See also the [dashboard (UI) <gui-title>Projects</gui-title> new features and enhancements](#new-ui-projects), including the [MLRun UI integration into the platform dashboard](#new-ui-projects-redeisgn-w-mlrun-integration).
{{< /note >}}
{{< comment >}}<!-- [TODO-NEXT-RNS] Remove or edit. -->
{{< /comment >}}

- <a id="new-mlrun-official-support-and-version-upgrade"></a>Added official support for MLRun (previously supported as {{< techpreview fmt="0" >}}) and upgraded to version 0.6.

    {{< internal-release-note rnid="new-mlrun-official-support-and-version-upgrade" >}}
**DOC Issues:** {{< jira ig="17296" >}} (MLRun GA support); {{< jira ig="17513" >}} (v3.0.0 support-matrix update)<br/>
    {{< /internal-release-note >}}

- <a id="new-mlrun-align-script"></a>Added an <file>align_mlrun.sh</file> script for installing the MLRun Python package (`mlrun`) and updating the package version to match the version of the platform's MLRun service.
    The script is available in the running-user directory (<path>/User</path>) upon creation of a Jupyter Notebook service for the relevant user.
    You can find information about this script and related execution code in the platform's <file>welcome.ipynb</file> tutorial Jupyter notebook, which is also available in the running-user directory.

    {{< internal-release-note reviewed="urih" rnid="new-mlrun-align-script" ig="17350" type="req" owner="Urih H. (urih)" docig="17473" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-default-projects-artifacts-path-change"></a>Changed the default artifacts path for new projects to a <path>&lt;project name&gt;/artifacts</path> directory in the new [predefined "projects" data container](#new-predefined-projects-container).
    The artifacts directory is created when the first artifact is logged for the project.

    {{< internal-release-note rnid="new-mlrun-default-projects-artifacts-path-change" ig="15834" type="req" owner="Hedi I. (hedii)" docig="17802" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-job-scheduling"></a>Added support for scheduling jobs and viewing scheduled jobs from the code (using the MLRun API) and [from the dashboard (UI)](#new-ui-project-jobs-schedule).

    {{< internal-release-note rnid="new-mlrun-job-scheduling" ig="16530" type="subreq" req="16332" owner="Oded M. (odedm)" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-spark-operator-api"></a>Added an API for using the platform's new [Spark Operator service](#new-spark-operator-service) to simplify submission and scheduling of Spark jobs.

    {{< internal-release-note reviewed="urih" rnid="new-mlrun-spark-operator-api" ig="16356" type="req" owner="Uri H. (urih)" docig="17932" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-graph-serving"></a>Added support for serving graphs that are composed of predefined graph blocks (such as model servers, routers, ensembles, and data-engineering tasks) or based on native Python classes or functions.
    See the [MLRun documentation](https://docs.mlrun.org/en/release-v0.6.x-latest/serving/serving-graph.html).

    {{< internal-release-note rnid="new-mlrun-graph-serving" >}}
(sharonl) (7.3.21) Gilad asked that I add this release note without a related Jira ticket.
    {{< /internal-release-note >}}

- <a id="new-mlrun-remote-env"></a>Added support for remote MLRun environments: develop your code locally and run it on a remote cluster.
    See the [MLRun documentation](https://docs.mlrun.org/en/release-v0.6.x-latest/howto/remote.html).
    {{< internal-release-note rnid="new-mlrun-remote-env" >}}
(sharonl) (7.3.21) Gilad asked that I add this release note without a related Jira ticket.
    {{< /internal-release-note >}}

- <a id="new-mlrun-dist-runtimes-doc-imprv"></a>Improved the documentation for distributed runtimes &mdash; [Dask](https://docs.mlrun.org/en/release-v0.6.x-latest/runtimes/dask-overview.html), [MPI (Horovod)](https://docs.mlrun.org/en/release-v0.6.x-latest/runtimes/horovod.html), and [Spark](https://docs.mlrun.org/en/release-v0.6.x-latest/runtimes/spark-operator.html).

    {{< internal-release-note rnid="new-mlrun-dist-runtimes-doc-imprv" >}}
(sharonl) (7.3.21) Gilad asked that I add this release note without a related Jira ticket.
    {{< /internal-release-note >}}

- <a id="new-mlrun-add-frames-to-image"></a>Added V3IO Frames to MLRun images.

    {{< internal-release-note rnid="new-mlrun-add-frames-to-image" ig="17037" type="req" owner="Hedi I. (hedii)" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-enforce-project-name-restrictions"></a>Enforced MLRun project-name restrictions &mdash; [RFC 1123](https://tools.ietf.org/html/rfc1123) DNS label-name requirements (DNS-1123 label) &mdash; both from the MLRun API and from the dashboard (UI).
    See the related [known issue](#ki-mlrun-misnamed-old-projects) for old projects that don't conform to these restrictions.

    {{< internal-release-note rnid="new-mlrun-enforce-project-name-restrictions" ig="16531" type="subreq" req="14392" owner="Hedi I. (hedii)" docig="17599" >}}
<br/>
(sharonl) (2.3.21) See also the referenced MLRun known issue [#ki-mlrun-misnamed-old-projects](#ki-mlrun-misnamed-old-projects) and the {{< xref f="cluster-mgmt/deployment/sw-specifications.md" a="mlrun-project-names" text="MLRun project-name software restrictions" >}}.
{{< comment >}}<!-- [c-project-name-restrictions] -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-demos-imprv"></a> Restructured and improved the MLRun demos, including directories and files renaming; getting-started tutorial rewrite; and new how-to demos that demonstrate how to convert existing ML code to an MLRun project and how to work with Spark.
    (See also the [Jupyter Notebook tutorials and demos infrastructure-improvements note](#new-tutorials-n-demos-infra-imprv) &mdash; including moving all platform demos sources and the getting-started tutorial to the [mlrun_demos](https://github.com/mlrun/demos) GitHub repository, pre-deploying the demos in the platform, and improving the demos-update script.)

    {{< internal-release-note rnid="new-mlrun-demos-imprv" >}}
<br/>
(sharonl) (1.3.21) There's no dedicated Jira ticket for these changes, but they're related also to the move of all demos and the getting-started tutorial to the _mlrun_demos_ GitHub repo (Requirement {{< jira ig="16920" >}} / DOC {{< jira ig="17429" >}}) &mdash; see the [#new-tutorials-demos-n-gs-tutorial-mv-to-mlrun-demos](#new-tutorials-demos-n-gs-tutorial-mv-to-mlrun-demos) Jupyter Notebook tutorials note.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-pipelines" text="Pipelines" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="new-pipelines-version-upgrade"></a>Upgraded to Kubeflow Pipelines version 1.0.1.

    {{< internal-release-note rnid="new-upgrade-pipelines" ig="16374" type="req" owner="Adam M. (adamm)" docig="17513" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-nuclio" text="Nuclio Serverless Functions" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="new-nuclio-version-upgrade"></a>Upgraded to Nuclio version 1.5.

    {{< internal-release-note rnid="new-nuclio-version-upgrade" owner="Oded M. (odedm)" docig="17513" >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-clusterip-default-service-type"></a>Changed the default Kubernetes service type for Nuclio functions from NodePort to ClusterIP to avoid exposing the function outside of the platform cluster.

    {{< internal-release-note rnid="new-nuclio-clusterip-default-service-type" ig="16407" type="req" owner="Adam M." docig="17501" >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-api-gateways-external-user-auth"></a>Added support for authenticating external (non-platform) user access to Nuclio API gateways by selecting the <gui-label>OAuth2</gui-label> authentication method, which uses the new [OAuth2 authenticator service](#new-authenticator-service) (`authenticator`).

    {{< internal-release-note rnid="new-nuclio-api-gateways-external-user-auth" ig="14707" type="req" owner="Hedi I. (hedii)" docig="17121" >}}
{{< comment >}}<!-- [c-authenticator-service] -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-func-run-as-non-root-user"></a>Added support for running Nuclio functions as any user, not just as a root user, to enable access to external volumes (such a NetApp storage) using relevant user permissions.

    {{< internal-release-note rnid="new-nuclio-func-run-as-non-root-user" ig="15870" type="req" owner="Adam M." >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-ui-import-override-existing-func"></a>Enabled importing a Nuclio function with the same name as an existing function from the dashboard to override the existing function.

    {{< internal-release-note reviewed="erann" rnid="new-nuclio-ui-import-override-existing-func" ig="14502" type="req" owner="Eran N. (erann)" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="new-tsdb-nuclio-funcs" text="TSDB Nuclio Functions" >}}
{{< comment >}}<!-- [c-tsdb-nuclio-funcs-classif] -->
{{< /comment >}}
{{< internal-release-note reviewed="dinan" >}}
{{< /internal-release-note >}}

- <a id="new-tsdb-nuclio-funcs-version-upgrade"></a>Upgraded to V3IO TSDB Nuclio Functions version 0.6.

    {{< internal-release-note rnid="new-tsdb-nuclio-funcs-version-upgrade" owner="Dina N. (dinan)" docig="16762" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-tsdb" text="Time-Series Databases (TSDBs)" >}}
{{< internal-release-note reviewed="dinan" >}}
{{< /internal-release-note >}}

- <a id="new-tsdb-version-upgrade"></a>Upgraded to V3IO TSDB version 0.11.

    {{< internal-release-note rnid="new-tsdb-version-upgrade" owner="Dina N. (dinan)" docig="17513" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="new-tsdb-prometheus" text="Prometheus" >}}

- <a id="new-tsdb-prometheus-version-upgrade"></a>Upgraded to V3IO Prometheus version 3.5 and Prometheus version 2.15.2.

    {{< internal-release-note rnid="new-tsdb-prometheus-version-upgrade" ig="16873" type="req" owner="Dina N. (dinan)" docig="17513" >}}
    {{< /internal-release-note >}}

- <a id="new-prometheus-ui-access"></a>Provided easy access to the Prometheus UI by linking to it from [the monitoring service](#new-monitoring-service-ui-link-to-prometheus-ui) on the <gui-title>Services</gui-title> dashboard page.

    {{< internal-release-note rnid="new-prometheus-ui-access" ig="16832" type="req" owner="Adam N. (adamm)" docig="17430" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-presto-n-hive" text="Presto and Hive" >}}
{{< internal-release-note reviewed="dinan" >}}
{{< /internal-release-note >}}

- <a id="new-presto-version-upgrade"></a>Upgraded to Presto version 332.

    {{< internal-release-note rnid="new-presto-version-upgrade" ig="16064" type="req" owner="Dina N. (dinan)" docig="17513" >}}
    {{< /internal-release-note >}}

- <a id="new-presto-create-default-hive-db-path"></a>Added automatic creation of the default Hive database path (<path>/user/hive/warehouse</path> in the "bigdata" container), if it doesn't already exist, when enabling Hive for the Presto service.

    {{< internal-release-note rnid="new-presto-create-default-hive-db-path" ig="16798" type="req" owner="Dina N. (dinan)" docig="17595" >}}
<br/>
(sharonl) (2.3.21) Adi wanted to document this as an enhancement and not as a bug fix.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-spark" text="Spark" >}}
{{< internal-release-note reviewed="dinan" >}}
{{< /internal-release-note >}}

- <a id="new-spark-version-upgrade"></a>Upgraded to Spark version 2.4.5.

    {{< internal-release-note rnid="new-spark-version-upgrade" ig="16946" type="req" owner="Dina N. (dinan)" docig="17513" >}}
    {{< /internal-release-note >}}

- <a id="new-spark-operator-service"></a>Added a new Spark Operator default (pre-deployed) shared single-instance tenant-wide service named `spark-operator` (version v2.4.5).
    The service uses the [spark-on-k8s-operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator) Kubernetes Operator for Spark ("Spark Operator") to simplify submission and scheduling of Spark jobs.
    Use the service via the new [MLRun Spark Operator API](#new-mlrun-spark-operator-api).
    <br/>
    {{< techpreview mark="1" >}} Note that the use of this service over GPUs is supported only as {{< techpreview fmt="0" >}}.

    {{< internal-release-note reviewed="urih" rnid="new-spark-operator-service" ig="16356" type="req" owner="Uri H. (urih)" docig="17932" >}}
<br/>
(sharonl) (23.2.21 For the {{< techpreview fmt="0" >}} GPU support, see Sub-Requirement {{< jira ig="18001" >}}.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-web-shell" text="Web Shell" >}}

- <a id="new-web-shell-ssh"></a>Added support for secure connectivity to the platform cluster using SSH, which enables debugging from remote IDEs such as PyCharm and VSCode.
    You can enable SSH and configure the port from the web-shell's custom service configuration on the dashboard's <gui-title>Services</gui-title> page.
    When SSH is configured, you can get the authentication key from the new <gui-label>SSH</gui-label> menu option for this service.
    <br/>
    Note that the SSH port must be in the range of 30000&ndash;32767, and the SSH connection must be done with user "iguazio" regardless of the identity of the running user of the web-shell service.

    {{< internal-release-note reviewed="odedm" reviewed="erann" rnid="new-web-shell-ssh" >}}
**R&D Tickets:** Requirement {{< jira ig="16018" >}} (Platform), Sub-Requirement {{< jira ig="16695" >}} (UI); Improvement {{< jira ig="18138" >}} (UI: note the valid port range); Bug {{< jira ig="18137" >}} (UI: note that the SSH user must be "iguazio") <br/>
**Owners:** Sahar E. (sahare) (IG-16018); Eran N. (erann) (IG-16695; IG-18138; IG-18137)<br/>
**DOC Issues:** {{< jira ig="18176" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-logging-n-monitoring-services" text="Logging and Monitoring Services" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="new-log-forwarder-elasticsearch-version-upgrade"></a>Upgraded the Elasticsearch version supported by the log-forwarder service to version 7.10.

    {{< internal-release-note rnid="new-log-forwarder-elasticsearch-version-upgrade" ig="15744" type="subreq" req="7750" owner="Moshe H. (mosheh)" docig="17513" >}}
    {{< /internal-release-note >}}

- <a id="new-grafana-version-upgrade"></a>Upgraded to Grafana version 7.2.

    {{< internal-release-note rnid="new-grafana-version-upgrade" ig="15744" type="req" owner="Adam M. (adamm)" docig="17513" >}}
<br/>
(sharonl) (22.2.1) We upgraded to Grafana version 7.2.0, but it was agreed (in consultation with Adi, Gilad, Orit, Oded, and Liat)to document the support using an X.Y version resolution.
    {{< /internal-release-note >}}

- <a id="new-mlrun-job-monitoring"></a>Added a new predefined <gui-title>MLRun Jobs Monitoring</gui-title> Grafana service dashboard for monitoring MLRun jobs' resource consumption (such as memory and CPU).

    {{< internal-release-note rnid="new-mlrun-job-monitoring" ig="15929" type="req" owner="Oded M. (odedm)" >}}
    {{< /internal-release-note >}}

- <a id="new-grafana-app-cluster-monitoring-dashboards"></a>Added more Grafana dashboards for application-cluster monitoring.
    The dashboards are available in the <dirname>private</dirname> dashboards folder of the application-cluster Grafana service, which is accessible to users with the IT Admin management policy via a <gui-label>Status dashboard</gui-label> link on the <gui-title>Clusters | Application</gui-title> platform-dashboard tab:

    -  Application-services and Nuclio-functions monitoring dashboards (similar to the existing predefined dashboards of the same names for the Grafana service that's accessible from the <gui-title>Services</gui-title> dashboard page):
        - <gui-title>Application Services Monitoring</gui-title>
        - <gui-title>Nuclio Functions Monitoring - Overview</gui-title>
        - <gui-title>Nuclio Functions Monitoring - Per Function</gui-title>
    - <gui-title>Kubernetes Resource Usage Analysis</gui-title> &mdash; a new dashboard that displays resource-consumption information for the cluster, such as CPU and memory utilization.

    {{< internal-release-note rnid="new-grafana-app-cluster-monitoring-dashboards" owner="Oded M. (odedm)" >}}
<br/>
(sharonl) (4.3.21) It seems that there's no Jira ticket for this.
I received the information from Orit and Adi.
    {{< /internal-release-note >}}

- <a id="new-grafana-external-user-auth"></a>Added support for authenticating external (non-platform) user access to the dashboards of shared platform Grafana services by using the new [OAuth2 (OIDC) Authenticator service](#new-authenticator-service) (`authenticator`).

    {{< internal-release-note rnid="new-grafana-external-user-auth" ig="14707" type="req" owner="Hedi I. (hedii)" docig="17121" >}}
{{< comment >}}<!-- [c-authenticator-service] -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-grafana-private-n-public-folders"></a>Moved the platform's predefined Grafana dashboards to a <dirname>private</dirname> folder that isn't visible to viewers with the Viewer role &mdash; namely, external (non-platform) users who are given access to the Grafana UI via the [authenticator service](#new-grafana-external-user-auth).
    In addition, predefined an empty <dirname>public</dirname> Grafana dashboards folder with default permissions for storing custom dashboards that you wish to share also with external users.

    {{< internal-release-note rnid="new-grafana-private-n-public-folders" ig="17353" type="req" owner="Hedi H. (hedii)" docig="17424" >}}
{{< comment >}}<!-- [IntInfo] (sharonl) (1.3.21) 
- The new folders are visible in the Dashboards | Manage tab in the Grafana UI.
- I found that Grafana uses "folder" and not "directory" terminology, and this
  was also the terminology in the Requirement IG-17353. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-monitoring-service-ui-link-to-prometheus-ui"></a>Changed the display of the monitoring service's name (`monitoring`) on the <gui-title>Services</gui-title> dashboard page to link to the UI of the pre-deployed [Prometheus service](#new-prometheus-ui-access).

    {{< internal-release-note rnid="new-monitoring-service-ui-link-to-prometheus-ui" ig="16832" type="req" owner="Adam N. (adamm)" docig="17430" >}}
    {{< /internal-release-note >}}

- <a id="new-app-node-offline-event-rename"></a>Renamed the <api>AppNode.Offline</api> event to <api>AppNode.NotReady</api>.

    {{< internal-release-note rnid="new-app-node-offline-event-rename" ig="17703" type="bug" owner="Oded M. (odedm)" >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-security-n-user-management" text="Security and User Management" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="new-authenticator-service"></a>Added a new OAuth2 (OIDC) Authenticator default (pre-deployed) shared single-instance tenant-wide service named `authenticator` (version 2.23.0).
    This service is used for OAuth2 authentication of user access to [Nuclio API gateways](#new-nuclio-api-gateways-external-user-auth) and [shared Grafana dashboards](#new-grafana-external-user-auth), including access by external (non-{{< product lc >}}) users (for example, using GitHub).
    {{< internal-release-note rnid="new-authenticator-service" ig="14707" type="req" owner="Hedi I. (hedii)" docig="17121" >}}
{{< comment >}}<!-- [c-authenticator-service] -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-dashboard" text="Dashboard (UI)" >}}
{{< internal-release-note reviewed="erann" >}}
{{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-ui-projects" text="Projects" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="new-ui-projects-redeisgn-w-mlrun-integration"></a>Redesigned the <gui-title>Projects</gui-title> dashboard area to integrate the MLRun dashboard (UI), synchronize MLRun and Nuclio projects, and extend project information and functionality.
    As part of these changes &mdash;
    {{< internal-release-note rnid="new-ui-projects-redeisgn-w-mlrun-integration" >}}
**R&D Tickets:** Requirement {{< jira ig="16347" >}} (UI: integrate the MLRun UI into the platform UI); Requirement {{< jira ig="14392" >}} (UI: new project-overview page), Sub-Requirement {{< jira ig="16531" >}} (Platform: support new project overview and sync MLRun and Nuclio projects)<br/>
**Owners:** Eran N. (erann) (IG-16347; IG-14392); Hedi I. (hedii) (IG-16531)<br/>
**DOC Issue:** {{< jira ig="17599" >}}<br/>
    {{< /internal-release-note >}}

    - <a id="new-ui-project-home-redesign"></a>Redesigned the <gui-title>Projects</gui-title> home page to display new project summaries and action menus.
        {{< internal-release-note >}}[#new-ui-project-home-redesign](#new-ui-project-home-redesign)
        {{< /internal-release-note >}}
    - <a id="new-ui-project-overview"></a>Added a new <gui-title>Projects | &lt;project name&gt;</gui-title> project-overview page, which is available when selecting a project from the <gui-title>Projects</gui-title> home page.
        The project overview includes a high-level overview of the project's assets, status of running jobs and functions, ability to add new project components such as jobs, and links to additional project information.
        {{< internal-release-note >}}[#new-ui-project-overview](#new-ui-project-overview)
        {{< /internal-release-note >}}
    - <a id="new-ui-projects-functions-page-access-change"></a>Changed access to the <gui-title>Functions</gui-title> project page, which displays the project's Nuclio serverless functions and API gateways.
        This information is now available via <gui-label>Real-time functions (Nuclio)</gui-label> and <gui-label>API gateways (Nuclio)</gui-label> links in the project overview's <gui-label>Quick Links</gui-label> sidebar section.
        {{< internal-release-note >}}[#new-ui-projects-functions-page-access-change](#new-ui-projects-functions-page-access-change)
        {{< /internal-release-note >}}

- <a id="new-ui-project-jobs-schedule"></a>Added a new <gui-title>Jobs | Schedule</gui-title> project tab for scheduling MLRun jobs and viewing scheduled jobs (part of the new [MLRun job-scheduling support](#new-mlrun-job-scheduling)).
    As part of this change, the job-monitoring information moved to a new <gui-title>Jobs | Monitor</gui-title> tab.

    {{< internal-release-note rnid="new-ui-project-jobs-schedule" ig="16332" type="req" owner="Eran M. (erann)" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-project-job-create-imprv"></a>Added usability improvements to the <gui-title>Create Job</gui-title> wizard for creating new MLRun jobs.

    {{< internal-release-note rnid="new-ui-project-job-create-imprv" ig="16361" type="req" owner="Eran N. (erann)" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-project-archive"></a>Added support for archiving projects from the dashboard (new <gui-label>Archive</gui-label> project-menu option) and for filtering out archived projects from the projects view (see the <gui-label>All Projects</gui-label> filter options).

    {{< internal-release-note rnid="new-ui-project-archive" ig="16757" type="subreq" req="15889" owner="Eran N. (erann)" >}}
**Requirement Owner:** Hedi I. (hedii)
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-ui-data" text="Data" >}}

- <a id="new-ui-async-file-upload"></a>Enhanced the support for asynchronous file upload.
    You can now issue new upload requests while previous requests are in progress, and see the upload status in the new <gui-title>File Uploads</gui-title> pop-up window.
    Note that when uploading multiple files in the same upload request, some files might not be visible in the file-uploads window until the upload of other files in the request has completed.

    {{< internal-release-note reviewed="erann" rnid="new-ui-async-file-upload" ig="16763" type="req" owner="Eran N. (erann)" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-table-update-display-during-load"></a>Enhanced table browsing by displaying the data as it is loaded instead of waiting for the entire table to load.

    {{< internal-release-note rnid="new-ui-table-update-display-during-load" ig="13584" type="req" owner="Eran N. (erann)" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-ui-misc" text="Miscellaneous" >}}

- <a id="new-ui-maintenance-mode-warnings"></a>Added warning messages when the cluster is in maintenance mode.

    {{< internal-release-note rnid="new-ui-maintenance-mode-warnings" ig="16251" type="req" owner="Eran N. (erann)" docig="17506" >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-deployment-specs" text="Deployment Specifications" >}}
{{< internal-release-note reviewed="danyk,maora" >}}
{{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-cloud-deployment-specs" text="Cloud Deployments" >}}

- <a id="new-cloud-data-nodes-increase-min-os-boot-disk"></a>Increased the minimal required OS boot-disk size for all cloud data-node types to 400 GB.

    {{< internal-release-note rnid="new-cloud-data-nodes-increase-min-os-boot-disk" type="req" ig="14898" owner="odedm" docig="17111" >}}
<br/>
(sharonl) (2.3.21) I got the information about the increase of the minimal cloud **data-node** OS boot-disk size to 400 GB from Orit and Dany and it matches the internal BOM.
(For the VM data nodes we already documented a minimal data-node boot-disk requirement of 400 GB.)
<br/>
(12.3.21) I learned that this change is connected to Requirement {{< jira ig="14808" >}} "Docker Registry: Long-term solution for K8s GC problem"; (Maor said that the feature itself doesn't require documentation, only the related spec implications).
I also learned that in addition to the increase in the minimal cloud _data-node_ OS boot-disk size (to 400 GB), we _reduced_ the minimal cloud _app-node_ OS-boot disk size from 400 GB to 250 GB; (for VM it remains 400 GB).
I updated the AWS & Azure deployment specs accordingly, but in consultation with Maor and Adi it was decided not to mention this in the release notes (as we prefer users to use larger disks).
See info in DOC {{< jira ig="17111" >}}.
    {{< /internal-release-note >}}

- <a id="new-amazon-eks-support"></a>AWS &mdash; added support for deploying the platform on the Amazon Elastic Kubernetes Service ([Amazon EKS](https://aws.amazon.com/eks/)), including usage-based pricing.

    {{< internal-release-note rnid="new-amazon-eks-support" ig="12450" type="req" owner="Ilan G. (ilang)" docig="18316" >}}
{{< comment >}}<!-- [IntInfo] (sharonl) (24.3.21) I retroactively added the new
  DOC IG-18316 internal info.
  The deployment is actually of the platform's _application cluster_. I learned
  this now and decided not to edit the release notes retroactively to
  explicitly reflect this. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-aws-cloud-i32xlarge-data-nodes"></a>AWS &mdash; added support for using smaller data nodes of EC2 instance type i3.2xlarge.

    {{< internal-release-note rnid="new-aws-cloud-i32xlarge-data-nodes" ig="16114" type="req" owner="Dany K. (danyk)" docig="17766" >}}
    {{< /internal-release-note >}}

- <a id="new-azure-cloud-ncv3-series-gpu-app-nodes"></a>Azure &mdash; added support for using the GPU-optimized NCv3-series VM instance sizes for the application-nodes.

    {{< internal-release-note rnid="new-azure-cloud-ncv3-series-gpu-app-nodes" ig="16059" type="req" owner="Gall Z. (galz)" docig="17762" >}}
    {{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Fixes -->
{{< rn-heading t="fixes" >}}

[Application Services](#fixes-managed-app-services) | [Dashboard (UI)](#fixes-dashboard) | [Jupyter](#fixes-jupyter) | [Logging and Monitoring Services](#fixes-logging-n-monitoring-services) | [MLRun](#fixes-mlrun) | [Nuclio](#fixes-nuclio) | [TSDB](#fixes-tsdb)

<!-- ---------------------------------------- -->
{{< rn-heading id="fixes-managed-app-services" text="Managed Application Services" >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-jupyter" text="Jupyter Notebook" >}}
{{< internal-release-note reviewed="urih" >}}
{{< /internal-release-note >}}

- <a id="fix-jupyter-dir-delete"></a>Added support for deleting directories (folders) from the Jupyter UI.
    Note that this feature required disabling the Jupyter trash mechanism for both files and directories, so deleted items are no longer moved to the trash and cannot be restored.

    {{< internal-release-note rnid="fix-jupyter-dir-delete" ig="16587" type="bug" ki_start_ver="2.8.0" owner="Uri H. (urih)" >}}
<br/>
(sharonl) (13.10.20) The known issue was added in the v2.10.0 RNs and also retroactively in the v2.8.0 RNs.
The bug existed also to earlier releases.
<br/>
(23.2.21) The bug was fixed for v3.0.0 and we replaced the release-notes known issue with a fix note.
I confirmed with Uri that the trade off for the fix is that we disabled the Jupyter trash mechanism for both files and folders (`delete_to_trash=False`), which means that deleted items aren't moved to the trash folder and cannot be restored from the trash in case of accidental deletion (no delete undo).
I decided to document this both in the v3.0.0 RNs fix item and permanently in the {{< xref f="cluster-mgmt/deployment/sw-specifications.md" a="jupyter-ui-trash-disable" text="Jupyter software specifications" >}}.
{{< comment >}}<!-- [c-jupyter-ui-trash-disable] -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="fix-jupyter-wrong-ver-in-services-ui"></a>Fixed the version of the Jupyter Notebook service on the <gui-title>Services</gui-title> dashboard page.

    {{< internal-release-note rnid="fix-jupyter-wrong-ver-in-services-ui" >}}
**Bags:** {{< jira ig="17040" >}} (v3.0.0); {{< jira ig="16755" >}} (v2.10)<br/>
**Owner:** Uri H. (urih)<br/>
**DOC Issues:** {{< jira ig="17511" >}} (v3.0.0 RNs); {{< jira ig="16761" >}} (V2.10.0 RNs); {{< jira ig="15178" >}} (v2.8.0 RNs)<br/>
Known issue since RN v2.8.0<br/>
<br/>
(sharonl) (22.9.20) This was also a known issue in the v2.8.0 RNs, and at the time it wasn't connected to a specific Jira ticket (see the **"2.8.0 Support matrix"** email thread).
When it was discovered that the bug persists in v2.10.0, bug {{< jira ig="16755" >}} was opened (see the **"2.10.0 Support Matrix"** email thread, copied in DOC {{< jira ig="16762" >}}).
(25.2.21) Bug IG-16755 was fixed in v2.10.x (post the v2.10.0 release) and a separate Bug {{< jira ig="17040" >}} was opened for v3.0.0 and fixed and closed for this version, so in the v3.0.0 release notes the previous known issue (`#ki-jupyter-wrong-ver-in-services-ui`) was replaced with this fix note.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-mlrun" text="MLRun" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

  - <a id="fix-mlrun-show-mpijob-logs-in-ui"></a>Added MPIJob logs to the dashboard (UI).

    {{< internal-release-note rnid="fix-mlrun-show-mpijob-logs-in-ui" ig="17880" type="bug" owner="Hedi I. (hedii)" >}}
<br/>
(sharonl) (3.3.21) This bug was found and fixed for v3.0.0.
We decided to document the fix in the v3.0.0 release notes, even though this wasn't a known issue in previous release notes, because this is a field issue.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-nuclio" text="Nuclio Serverless Functions" >}}

- <a id="fix-nuclio-funcs-n-api-gateways-reserved-names-check"></a>Enforced reserved-names restrictions for creation of new Nuclio functions and API gateways for the following names &mdash; <api>controller</api>, <api>dashboard</api>, <api>dlx</api>, <api>scaler</api>.
    <a id="fix-nuclio-function-name-dashboard"></a>This also fixes the previous known issue of no access to the <gui-title>Projects</gui-title> dashboard page when deploying a Nuclio function named <api>dashboard</api>.

    {{< internal-release-note rnid="fix-nuclio-funcs-n-api-gateways-reserved-names-check" ig="16818" type="bug" ki_start_ver="2.8.0" owner="Oded M. (odedm)" >}}
<br/>
(sharonl) (25.9.20) The known issue was added in the new v2.10.0 release notes and also retroactively in the v2.8.0 release notes, and it was also documented in the {{< xref f="data-layer/reference/reserved-names.md" >}} reference.
<br/>
(23.2.21) The previous known issue (`#ki-nuclio-function-name-dashboard`) referred specifically to a Nuclio function named <func>dashboard</func>.
The fix was more broad &mdash; enforce reserved-named restrictions for the four names detailed in the fix note, for both Nuclio functions and API gateways.
See info in Bug {{< jira ig="16818" >}}.
<br/>
In v3.0.0, in which we restructured the documentation site and moved the reserved-names references to the new data-layer section, I replaced the Nuclio section in this reference with a reserved-names entry in the {{< xref f="cluster-mgmt/deployment/sw-specifications.md" a="nuclio-reserved-names" text="Nuclio sofware specifications and restructions" >}}, because Nuclio isn't a data-layer service/API.
{{< comment >}}<!-- [c-nuclio-reserved-names]
  [c-nuclio-function-name-dashboard] -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="fix-nuclio-ui-continuous-func-status-update"></a>Fixed the Nuclio function status on the dashboard (UI) to always reflect the current deployment state.

    {{< internal-release-note rnid="fix-nuclio-ui-continuous-func-status-update" ig="16894" type="bug" owner="Liran B. (liranb)" >}}
<br/>
(sharonl) (3.3.21) This bug was found and fixed for v3.0.0.
We decided to document the fix in the v3.0.0 release notes, even though this wasn't a known issue in previous release notes, because this is a field issue (reported by Payoneer).
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-tsdb" text="Time-Series Databases (TSDBs)" >}}
{{< internal-release-note reviewed="dinan" >}}
{{< /internal-release-note >}}

- <a id="fix-tsdb-invalid-query-error-mssg"></a>Improved error messages for invalid TSDB queries, which are displayed when working with TSDB tables in Grafana dashboards.

    {{< internal-release-note rnid="fix-tsdb-invalid-query-error-mssg" ig="16911" type="bug" owner="Dina N. (dinan)" >}}
<br/>
(sharonl) (3.3.21) This bug was found and fixed for v3.0.0.
We decided to document the fix in the v3.0.0 release notes, even though this wasn't a known issue in previous release notes, because this is a field issue (reported by Samsung).
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-logging-n-monitoring-services" text="Logging and Monitoring Services" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="fix-monitoring-service-memory-issue"></a>Fixed a memory issue in the monitoring service.

    {{< internal-release-note reviewed="dinan" rnid="fix-monitoring-service-memory-issue" ig="16842" type="bug" owner="Gal T. (galt)" >}}
<br/>
(sharonl) (3.3.21) This bug was found and fixed for v3.0.0.
We decided to document the fix in the v3.0.0 release notes, even though this wasn't a known issue in previous release notes, because this is a field issue (reported by Ecolab).
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="fixes-dashboard" text="Dashboard (UI)" >}}
{{< internal-release-note reviewed="erann" >}}
{{< /internal-release-note >}}

{{< note id="fixes-dashboard-other-fix-secs-ref-note" >}}
See also the [MLRun](#fixes-mlrun) and [Nuclio](#fixes-nuclio) fixes for dashboard changes that are specific to these services.
{{< /note >}}

- <a id="fix-ui-nosql-table-distortion-w-attr-name-closed"></a>Fixed the <gui-label>Table</gui-label> view distortion when browsing a NoSQL table with an attribute named <attr>closed</attr>.

    {{< internal-release-note rnid="fix-ui-nosql-table-distortion-w-attr-name-closed" ig="16747" type="bug" ki_start_ver="2.8.0" owner="Eran N. (erann)" >}}
<br/>
(sharonl) (24.9.20) The known issue was added to the v2.10.0 RNs and also retroactively to the v2.8.0 RNs, in consultation with Orit and Adi (see Bug {{< jira ig="16747" >}}).
    {{< /internal-release-note >}}

- <a id="fix-ui-smtp-email-notif-tenant-admin-req"></a>Fixed <gui-title>SMTP</gui-title> settings-page issues:
      users with the IT Admin management policy but without the Tenant Admin policy no longer see an "`Error: Failed to fetch user list (you can still update the rest)`" error message on this page, and can now successfully use the <gui-label>Users to notify</gui-label> field to define IT Admin users who'll receive cluster-alert email notifications.

    {{< internal-release-note rnid="fix-ui-smtp-email-notif-tenant-admin-req" ig="16892" type="bug" ki_start_ver="2.10.0" owner="Oded M. (odedm)" docig="15568" >}}
<br/>
(sharonl) (12.10.20) See the related v2.10.0 RNs UI enhancement note {{< xref f="release-notes/version-2.10/v2.10.0.md" a="new-ui-smtp-alert-emails" text="#new-ui-smtp-alert-emails" >}}, and the information in Bug {{< jira ig="16892" >}} and related Requirement {{< jira ig="14371" >}} and DOC {{< jira ig="15568" >}}.
<br/>
(24.2.21) With the resolution of this bug in v3.0.0, in addition to replacing the previous RNs known issue ({{< xref f="release-notes/version-2.10/v2.10.0.md" a="ki-ui-smtp-email-notif-tenant-admin-req" text="#ki-ui-smtp-email-notif-tenant-admin-req" >}}) with this fix note, I also updated the {{< xref f="cluster-mgmt/deployment/post-deployment-howtos/smtp.md" >}} deployment how-to accordingly.
{{< comment >}}<!-- [c-smtp-email-notif-tenant-admin-req] -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Known Issues -->
{{< rn-heading t="known-issues" >}}

[Application Services](#ki-managed-app-services)  | [Backup and Recovery](#ki-backup-recovery-n-ha) | [Dashboard (UI](#ki-dashboard) | [File System](#ki-file-system) | [Frames](#ki-frames) | [Hadoop](#ki-hadoop) | [High Availability (HA)](#ki-backup-recovery-n-ha) | [Hive](#ki-presto-n-hive) | [Jupyter](#ki-jupyter) | [MLRun](#ki-mlrun)  | [Logging and Monitoring Services](#ki-logging-n-monitoring-services) | [Nuclio](#ki-nuclio) | [Presto](#ki-presto-n-hive) | [Prometheus](#ki-tsdb-prometheus) | [Security](#ki-security-n-user-management) | [Spark](#ki-spark) | [TSDB](#ki-tsdb) | [User Management](#ki-security-n-user-management) | [Web APIs](#ki-web-apis)

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-managed-app-services" text="Managed Application Services" >}}
{{< comment >}}<!-- [IntInfo] (sharonl) (12.5.19) At Adi's request, I removed
  the note that I previously added here to refer to the dashboard known issues
  for UI issues related to services. -->
{{< /comment >}}

- <a id="ki-services-failure-for-reused-usernames"></a>A user whose username was previously in use in the platform cannot run application services.

    {{< internal-release-note rnid="ki-services-failure-for-reused-usernames" ig="14150" type="bug" ki_start_ver="2.0.0" owner="Felix G. (felixg)" >}}
<br/>
(sharonl) (21.3.19) In consultation with Adi, I documented this only as a known issue in the release notes, but not in other doc locations.
<br/>
(25.5.20) In the v2.0.0&ndash;v2.5.0 RNs, this issue was associated with Bug {{< jira ig="11202" >}}.
On 6.5.20, Orit closed this bug as _Not a bug_ and opened Requirement {{< jira ig="14150" >}} "deletion of user workspace when deleting a user" (currently planned for v3.0.0), which according to Orit should also resolve this issue.
=> I've now linked the KI in Jira to requirement IG-14150 instead of the closed IG-11202 bug (including duplicating the existing RN labels and adding a KI RN label for v2.8.0), and I've marked IG-11202 as _External Release Note_ = _DONE_ so that it doesn't appear in related queries.
{{< comment >}}<!-- [IntInfo] (sharonl) (25.5.20) In the v2.8.0 RNs I also
  removed the connection to DOC IG-10843, as this was the general v2.0.0/v2.1.0
  doc task for managed application services. -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-jupyter" text="Jupyter Notebook" >}}

- <a id="ki-jupyter-notebooks-scala-not-supported"></a>Running Scala code from a Jupyter notebook isn't supported in the current release.
    Instead, run Scala code from a Zeppelin notebook or by using <file>spark-submit</file>, or use Python.

    {{< internal-release-note rnid="ki-jupyter-notebooks-scala-not-supported" ig="11174" type="bug" ki_start_ver="2.0.0" owner="Gal T. (galt)" docig="10216" id="11174" >}}
<br/>
(sharonl) This is documented also in the **Software Specifications and Restrictions** and in other locations in the doc.
{{< comment >}}<!-- [c-jupyter-notebooks-scala-not-supported] -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-frames" text="V3IO Frames" >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-frames-nosql" text="Frames NoSQL Backend (&quot;nosql&quot;/&quot;kv&quot;)" >}}

- <a id="ki-frames-nosql-write-partitioned-table-overwriteTable-not-supported"></a>The <func>write</func> client method's <api>"overwriteTable"</api> save mode isn't supported for partitioned tables {{< techpreview mark="1" >}}.

    {{< internal-release-note rnid="ki-frames-nosql-write-partitioned-table-overwriteTable-not-supported" ig="16167" type="bug" ki_start_ver="2.10.0" owner="Dina N. (dinan)" docig="15553" >}}
<br/>
(sharon) (27.9.20) I added the {{< techpreview mark="1" >}} mark because the support for writing partitioned tables is {{< techpreview fmt="0" >}} in v2.10.0 (see Requirement {{< jira ig="13538" >}}, DOC {{< jira ig="15553" >}}) and the related {{< xref f="release-notes/version-2.10/v2.10.0.md" a="new-frames-nosql-write-partitioned-tables-tp" text="v2.10.0 new-feature release note" >}}.

    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-mlrun" text="MLRun" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="ki-mlrun-default-projet-dir-permissions"></a>The default-project directory (<dirname>default</dirname>) is shared for read-only and execution.

    {{< internal-release-note rnid="ki-mlrun-default-projet-dir-permissions" ml="211" type="bug" ki_start_ver="3.0.0" owner="Hedi I. (hedii)" >}}
    {{< /internal-release-note >}}

- <a id="ki-mlrun-misnamed-old-projects"></a>Beginning with platform version 3.0.0 / MLRun version 0.6.0, project-name restrictions are enforced for MLRun projects both from the dashboard (UI) and the MLRun API.
    Project names must conform to the [RFC 1123](https://tools.ietf.org/html/rfc1123) DNS label-name requirements (DNS-1123 label) &mdash; 1&ndash;63 characters, contain only lowercase alphanumeric characters (a&ndash;z, 0&ndash;9) or hyphens (-), and begin and end with a lowercase alphanumeric character.
    Older MLRun projects whose names don't conform to these restrictions will be in a degraded mode and won't have Nuclio functionality (no serverless functions or API gateways).
    To fix this you need to replace the old projects (you cannot simply rename them).
    For assistance in upgrading old projects, contact Iguazio's support team.

    {{< internal-release-note reviewed="erann" rnid="ki-mlrun-misnamed-old-projects" ig="16531" type="subreq" req="14392" ki_start_ver="3.0.0" owner="Hedi I. (hedii)" docig="17599" >}}
<br/>
(sharonl) (2.3.21) See also the related v3.0.0 MLRun {{< xref f="release-notes/version-3.0/v3.0.0.md" a="new-mlrun-enforce-project-name-restrictions" text="#new-mlrun-enforce-project-name-restrictions" >}} enhancement release notes and the {{< xref f="cluster-mgmt/deployment/sw-specifications.md" a="mlrun-project-names" text="MLRun project-name software restrictions" >}}.
<br/>
I confirmed with R&D (Hedi, Liran, and Eran N.) that For Nuclio we already enforced project-name restrictions from the UI in previous releases (Eran said that we actually enforced stricter DNS-1035 label restrictions, as they don't allow names to begin with a digit, which is permitted in DNS-1123 label &mdash; so the v3.0.0 UI restrictions are more lenient).
Liran said that we added the project-name restrictions enforcement to the Nuclio API only in Nuclio v1.5.x / platform v3.0.0, but I decided not to mention this in the release notes (I wasn't asked to do this and I think most Nuclio projects in the platform are likely to have been created from the UI, which enforced the restrictions.)
{{< comment >}}<!-- [c-project-name-restrictions] -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-nuclio" text="Nuclio Serverless Functions" >}}

- <a id="ki-nuclio-unsupported-dark-site-langs"></a>Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).

    {{< internal-release-note rnid="ki-nuclio-unsupported-dark-site-langs" ig="13134" type="bug" ki_start_ver="2.0.0" owner="Oded M. (odedm)" id="13134" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (16.10.19) In the v2.5.0 release notes, I moved the Java reference that was in the similar v2.3 release-notes known issue for Bug {{< jira ig="11148" >}} to a {{< xref f="release-notes/version-2.5/v2.5.0.md" a="fix-nuclio-unsupported-dark-site-java" text="fix note" >}} for this bug, and I added .NET Core to the remaining known issue and associated it with the new bug {{< jira ig="13134" >}}.
<br/>
(18.11.20) Oded resolved Bug 13134 as _Won't Fix_; see info in the bug [comment](https://jira.iguazeng.com/browse/IG-13134?focusedCommentId=69937&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-69937):
> I am closing this as we're dragging this bug for a lot of versions now and this is not really beneficial.
> It is a decision to not support those runtimes in dark site for now, and due to lack of customer interest there is currently no plan to make them dark-site compatible in the foreseeable future

(25.2.12) I marked this internally in the RNs as a permanent known issue and disconnected the known issue from Bug 13134 (marked as _External Release Note_ = _Done_ to avoid checking the bug for future release notes).
{{< comment >}}<!-- [c-nuclio-unsupported-dark-site-langs] [IntInfo] (sharonl)
  (7.10.20) In v2.10.0, I added related info also in specs/sw-specifications.md
  - #nuclio-offline-deploy > #nuclio-offline-deploy-unsupported-langs. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-nuclio-ui-error-for-successful-deployment-after-prev-timeout-failure"></a>In case of a successful automatic deployment of a Nuclio function for a request that previously timed out &mdash; for example, if a requested resource, such as a GPU, becomes available after the time-out failure &mdash; the function status in the dashboard is still <gui-label>Error</gui-label>.

    {{< internal-release-note rnid="ki-nuclio-ui-error-for-successful-deployment-after-prev-timeout-failure" ig="12487" type="bug" ki_start_ver="2.3.0" owner="Adi H. (adih)" id="12487" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-tsdb" text="Time-Series Databases (TSDBs)" >}}

{{< comment >}}<!-- [TODO-NEXT-RNS] Uncomment if we add a ki-tsdb-nuclio-funcs
  section (and add a link to the section at the start of the KIs section). -->
{{< note id="ki-tsdb-other-ki-secs-ref-note" >}}
See also the [TSDB Nuclio functions](#ki-tsdb-nuclio-funcs) section under the Nuclio known issues.
{{< /note >}}
{{< comment >}}<!-- [c-tsdb-nuclio-funcs-classif] -->
{{< /comment >}}
{{< /comment >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-tsdb-prometheus" text="Prometheus" >}}

- <a id="ki-tsdb-prometheus-service-change-to-invalid-path"></a>Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.

    {{< internal-release-note rnid="ki-tsdb-prometheus-service-change-to-invalid-path" ig="12187" type="bug" owner="Golan S. (golans)" docig="10843" id="12187" >}}
Known issue since RN v2.2.0 / TSDB v0.9.2<br/>
<br/>
(sharonl) (6.6.19) The previous known issue of succeeding to create a new Prometheus table with an invalid TSDB table path was {{< xref s="release-notes" f="version-2.2/v2.2.0.md" text="v2.2.0 release notes" a="fix-tsdb-prometheus-service-create-for-non-existent-tsdb-table" text="fixed" >}} in v2.2.0.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-presto-n-hive" text="Presto and Hive" >}}

- <a id="ki-presto-hive-username-cant-be-changed-for-existing-db"></a>You can't change the MySQL DB user for an existing Hive MySQL DB.
    To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.

    {{< internal-release-note rnid="ki-presto-hive-username-cant-be-changed-for-existing-db" ig="11757" type="bug" ki_start_ver="2.2.0" owner="Golan S. (golans)" docig="9650" id="11757" >}}
    {{< /internal-release-note >}}
    {{< comment >}}<!-- [ci-presto-hive-username-cant-change-for-existing-db]
      (sharonl) (5.7.20) In consultation with Dina and Orit, I documented this
      restriction also in the Hive notes in data-layer/presto/overview.md
      (added retroactively to the current active doc sites when this was in a
      reference/section - v2.8.0 & v2.5.4). -->
    {{< /comment >}}

- <a id="ki-presto-invalid-cfg-service-n-dependent-services-failure"></a>Invalid Presto configuration files or connector definitions might cause the Presto service and all dependent services to fail.
    This is the expected behavior.
    To recover, delete the problematic configuration or manually revert relevant changes to the default Presto configuration files, save the changes to the Presto service, and select <gui-label>Apply Changes</gui-label> on the dashboard <gui-title>Services</gui-title> page.
    {{< internal-release-note rnid="ki-presto-invalid-cfg-service-n-dependent-services-failure" ig="15849" type="bug" ki_start_ver="2.10.0" owner="Dina N. (dinan)" >}}
<br/>
(sharonl) (23.2.12) In consultation with Orit, in the v3.0.0 release notes I removed "(including Jupyter Notebook)" after "dependent services" in the first sentence because in v3.0.0 we removed the Jupyter dependency on Presto &mdash; see Requirement {{< jira ig="16847" >}} and Bug {{< jira ig="17332" >}}, which was fixed and closed in v3.0.0, and the related v3.0.0 RNs enhancement note (`#new-jupyter-rmv-presto-dependency`).
<br/>
(7.3.21) Following input from Customer Success (Nir), I edited the note for the new v3.0.0 release notes and retroactively in the v2.10.0 release notes to expand it also to invalid editing of Presto configuration files and not only of an invalid Presto connector configuration.
(I changed the note anchor ID, accordingly, from `#ki-presto-invalid-connector-cfg-service-n-dependent-services-failure` to `#ki-presto-invalid-cfg-service-n-dependent-services-failure`, but in the v2.10.0 release notes I also kept the old anchor so as not to break the old link.)
    {{< /internal-release-note >}}

- <a id="ki-presto-wrong-default-cfg-edit-no-revert"></a>In rare cases, after failing to deploy incorrect edits to a default configuration file, it might not be possible to revert the default Presto configuration from the dashboard.

    {{< internal-release-note reviewed="dinan" rnid="ki-presto-wrong-default-cfg-edit-no-revert" ig="18147" type="bug" ki_start_ver="3.0.0" owner="Dina N. (dinan)" >}}
    {{< /internal-release-note >}}

- <a id="ki-hive-cli-no-web-shell-multi-user-exec-permiss"></a>Running the Hive CLI from a Jupyter Notebook service when there's no web-shell service in the cluster might fail if another user had previously run the CLI from another instance of Jupyter.
    To resolve this, ensure that there's at least one web-shell service in the cluster.

    {{< internal-release-note reviewed="dinan" rnid="ki-hive-cli-no-web-shell-multi-user-exec-permiss" ig="18009" type="bug" ki_start_ver="3.0.0" owner="Dina N. (dinan)" >}}
<br/>
(sharonl (4.3.21) I added the bug in the new v3.0.0 release notes and in consultation with Orit, I added it also, retroactively, to the v2.10.0 RNs; (the affected version in IG-18009 is 2.2.0, which according to Orit is when we added Hive).
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-spark" text="Spark" >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-spark-ui" text="Spark UI" >}}

- <a id="ki-spark-ui-broken-links-after-app-kill"></a>The Spark UI might display broken links after killing a running application from the UI.
    To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.

    {{< internal-release-note rnid="ki-spark-ui-broken-links-after-app-kill" ig="12143" type="bug" ki_start_ver="2.3.0" owner="Uri H. (urih)" id="12143" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (24.9.20) The bug was closed as _Won't Fix_.
Because this is expected to be a permanent KI, I set the bug's <gui-label>External Release Note</gui-label> field to _DONE_ and not to _DONE-KNOWN-ISSUES_, as there's no point in updating the ticket for each new RN with this KI.
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-spark-streaming" text="Spark Streaming" >}}

- <a id="ki-spark-streaming-consume-after-shard-count-increase-ig-6471"></a>To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.

    {{< internal-release-note rnid="ki-spark-streaming-consume-after-shard-count-increase-ig-6471" ig="6471" type="bug" owner="Golan (golans)" docig="7157" ki_start_ver="1.5.3" id="6471" >}}
<br/>
(sharonl) The shard-count increase can currently be done only using the <api>UpdateStream</api> Streaming Web API operation.
<br/>
This bug exists also in earlier versions but was discovered only before the v1.5.3 release and the v1.5.0 documentation publication, and therefore added to the RNs only in v1.5.3 + added to the **Software Specifications and Restrictions** and <api>UpdateStream</api> Streaming Web API reference documentation in v1.5.0 and retroactively also in the v1.0.3 reference &mdash; see DOC Task {{< jira ig="7157" >}}.
    {{< /internal-release-note >}}
    {{< comment >}}<!-- [c-spark-streaming-consume-after-shard-increase] -->
    {{< /comment >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-spark-operator" text="Spark Operator" >}}
{{< internal-release-note reviewed="urih" >}}
{{< /internal-release-note >}}

- <a id="ki-spark-opeartor-job-name-size"></a>Spark jobs whose names exceed 63 characters might fail because of a Spark bug when running on Kubernetes &mdash; see Spark Bug [SPARK-24894](https://issues.apache.org/jira/browse/SPARK-24894), which was resolved in Spark version 3.0.0.

    {{< internal-release-note rnid="ki-spark-opeartor-job-name-size" ig="17068" type="bug" ki_start_ver="3.0.0" owner="Urih H. (urih)" docig="17932" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-web-apis" text="Web APIs" >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-simple-object-web-api" text="Simple-Object Web API" >}}

- <a id="ki-s3-web-api-get-n-head-object-wrong-ret-data"></a><api>GET Object</api> and <api>HEAD Object</api> don't return correct `ETag` and `Content-Type` metadata.

    {{< internal-release-note rnid="ki-s3-web-api-get-n-head-object-wrong-ret-data" ig="3453" type="bug" ki_start_ver="1.0.0" owner="Assaf B. (assafb)" >}}
<br/>
(sharonl)
(25.5.20) In the v1.5.0&ndash;v2.5.0 RNs we had a joint KI for Bugs {{< jira ig="3451" >}} and {{< jira ig="3453" >}}.
In v2.8.0 QA verified that the part of the existing RNs KI related to Bug IG-3451 &mdash; <api>GET Object</api> doesn't return the correct <api>Last-Modified</api> metadata &mdash; no longer exists in v2.8.0.
According to Assaf and Orit, it was probably fixed before v2.8.0.
=> In consultation with Orit, I edited the existing KI to restrict it to the remaining issues for Bug IG-3453, associated the KI only with IG-3453, and marked IG-3451 as _External Release Note_ = _Done_, without a v2.8.0 RN label and without adding a v2.8.0 fix note.
    {{< /internal-release-note >}}

- <a id="ki-s3-web-api-ofb-range-req-wrong-ret-code"></a>A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.

    {{< internal-release-note rnid="ki-s3-web-api-ofb-range-req-wrong-ret-code" ig="13449" type="bug" ki_start_ver="2.5.0" owner="Roy B. (royb))" id="13449" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-nosql-web-api" text="NoSQL Web API" >}}

- <a id="ki-nosql-web-api-range-scan-after-container-or-dir-recreate-migh-fail"></a>Range-scan requests following recreation of a data container or a container directory might return a 513 error; in such cases, reissue the request.

    {{< internal-release-note reviewed="assafb" rnid="ki-nosql-web-api-range-scan-after-container-or-dir-recreate-migh-fail" ig="17963" type="bug" ki_start_ver="3.0.0" owner="Roy B. (royb)" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-streaming-web-api" text="Streaming Web API" >}}

-	<a id="ki-putrecords-for-non-stream-dir-error-ig-4310"></a><api>PutRecords</api> with a stream path that points to an existing non-stream directory returns error <api>ResourceNotFoundException</api> instead of <api>ResourceIsnotStream</api>.

    {{< internal-release-note rnid="ki-putrecords-for-non-stream-dir-error-ig-4310" ig="4310" owner="Assaf B. (assafb)" ki_start_ver="1.0.0" >}}
    {{< /internal-release-note >}}

-	<a id="ki-web-invalid-updatestream-req-body-resp-error"></a><api>UpdateStream</api> with an invalid request-body JSON format may not return the expected <api>InvalidArgumentException</api> error.

    {{< internal-release-note rnid="ki-web-invalid-updatestream-req-body-resp-error" ig="4310" owner="Assaf B. (assafb)" ki_start_ver="1.0.0" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-logging-n-monitoring-services" text="Logging and Monitoring Services" >}}
{{< internal-release-note reviewed="odedm" >}}
{{< /internal-release-note >}}

- <a id="ki-data-cluster-statistics-not-available-offline"></a>Data-cluster statistics are not available when the platform is in offline mode.

    {{< internal-release-note reviewed="odedm" rnid="ki-data-cluster-statistics-not-available-offline" ig="9029" type="req" owner="Adam M. (adamm)" ki_start_ver="3.0.0" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (3.3.21) In v3.0.0 we moved to gathering data-cluster statistics using Prometheus instead of Graphite (on the application cluster), and a byproduct of this change is that data-cluster statistics (storage and performance statistics, such as those displayed in the container overview in the UI) are no longer available when the cluster is in offline mode.
Therefore, we added this permanent known issue in v3.0.0 and newer release notes.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-hadoop" text="Hadoop" >}}

-	<a id="ki-hadoop-unsupported-hdfs-commands"></a>The following Hadoop (`hdfs`) commands aren't supported:
    <cmd>createSnapshot</cmd>, <cmd>deleteSnapshot</cmd>, <cmd>getfattr</cmd>, <cmd>setfattr</cmd>, <cmd>setfacl</cmd>, and <cmd>setrep</cmd>.

    {{< internal-release-note rnid="ki-hadoop-unsupported-hdfs-commands" ki_start_ver="1.0.0" id="hdfs-unsupported-cmds" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (26.6.18) Initially, this known issue included the <cmd>truncate</cmd> command and was associated with Bug {{< jira ig="3106" >}} to implement this command.
Bug 3106 and duplicate Bug {{< jira ig="5239" >}} were resolved in v1.7.0 as {{< techpreview fmt="0" >}} (not tested by QA), at which point I documented the support for <cmd>truncate</cmd> as an {{< xref s="release-notes" f="version-1.7/v1.7.1.md" a="3106" text="enhancement" >}} in the v1.7.1 (v1.7 GA) release notes, removed the reference to the command from the known-issue note, and disassociated the known-issue note from Bug 3106.
The changes were done after the initial publication of the v1.7.0 Equinix Early Edition release notes.
<br/>
Golan said there's no Jira ticket to support the other commands.
<br/>
(2.5.19) In the v2.1.0 release notes, the reference to the supported <cmd>truncate</cmd> command was marked as  {{< techpreview fmt="0" >}}.
(27.10.19) In the v2.5.0 RNs, the <cmd>truncate</cmd> reference was removed from this note, after QA have verified the support for this command.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-file-system" text="File System" >}}

{{< note id="ki-file-system-hadoop-fs-note" >}}
For Hadoop FS known issues, see the [Hadoop known issues](#ki-hadoop).
{{< /note >}}

-	<a id="ki-fs-ops-no-stream-shard-sup"></a>File-system operations are not supported for stream shards.

    {{< internal-release-note rnid="ki-fs-ops-no-stream-shard-sup" ig="3605" type="task" owner="Ortal L. (ortall))" ki_start_ver="1.0.0" id="3604" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(17.10.19) I decided to stop making RN-updates in Task {{< jira ig="3604" >}} because the task has now been canceled => permanent known issue.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-security-n-user-management" text="Security and User Management" >}}

- <a id="ki-app-cluster-status-dashboard-access-requires-direct-it-admin-policy"></a>Users who are members of a user group with the IT Admin management policy but are not assigned this policy directly cannot access the application-cluster status dashboard.
    To bypass this issue, assign the IT Admin management policy directly to any user who require such access..

    {{< internal-release-note reviewed="odedm" rnid="ki-app-cluster-status-dashboard-access-requires-direct-it-admin-policy" ig="17514" type="bug" ki_start_ver="3.0.0" owner="Oded M. (odedm)" >}}
<br/>
(sharonl) (3.3.21) This is a field issue (reported by Ambyint) that was discovered in v2.10.0 but added only in the v3.0.0 release notes.
    {{< /internal-release-note >}}

- <a id="ki-authenticator-token-expiration-change-no-affect-on-existing-tokens"></a>Changing the token-expiration period for the `authenticator` service (`id_tokens_expiration` configuration) doesn't affect existing tokens.
    For further assistance, contact Iguazio's support team.

    {{< internal-release-note reviewed="odedm" rnid="ki-authenticator-token-expiration-change-no-affect-on-existing-tokens" ig="17295" type="bug" ki_start_ver="3.0.0" owner="Hedi I. (hedii)" >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-backup-recovery-n-ha" text="Backup, Recovery, and High Availability (HA)" >}}

- <a id="ki-stream-data-no-backup"></a>Stream data is not backed up and restored as part of the platform's backup and upgrade operations.

    {{< internal-release-note rnid="ki-stream-data-no-backup" ki_start_ver="1.5.1" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (13.2.18) Adi requested that we document this as a known issue in the RNs even though this behavior is by design and currently there's no plan to change it => permanent known issue.
<br/>
This is also documented in the **Software Specifications and Restrictions** document (DOC Task {{< jira ig="3582" >}}).
    {{< /internal-release-note >}}

- <a id="ki-ha-main-k8s-master-app-node-failure-cloud-azure"></a>In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services.
  Previously run application services will continue to run but cannot be stopped, and new services cannot be created.

    {{< internal-release-note rnid="ki-ha-main-k8s-master-app-node-failure-cloud-azure" ig="13840" type="req" owner="Alze P. (alexp)" >}}
Known issue since RN v2.1.0 for all environments; restricted in RN v2.3.0 to the cloud, and restricted in RN v2.8.0 to Azure (see the v2.8.0 {{< xref s="release-notes" f="version-2.8/v2.8.0.md" text="v2.8.0 release notes" a="fix-cloud-vip-aws" text="AWS fix note" >}})<br/>
<br/>
(sharonl) (10.7.19) In the **v2.1.0 &amp; v2.2.0** release notes we had a known issue for this HA bug that wasn't restricted to cloud environments.
The known issue was for Bug {{< jira ig="11774" >}}.
In **v2.3.0**, this bug was fixed only for non-cloud environments and Bug {{< jira ig="12573" >}} and Requirement {{< jira ig="12636" >}} were opened for the remaining cloud issues.
=> In the v2.3.0 release notes, I restricted the known issue to cloud environments and linked it to the new cloud tickets, and I added a {{< xref s="release-notes" f="version-2.3/v2.3.0.md" a="fix-ha-main-k8s-master-app-node-failure-non-cloud" text="fix note" >}} for non-cloud environments.
(24.5.20) In **v2.8.0**, the KI was fixed for AWS as part of Requirement {{< jira ig="12636" >}}, which was then modified not to cover Azure and a separate v3.0.0 Requirement {{< jira ig="13840" >}} was opened for Azure cloud deployment changes that should also fix this KI, and the RN KI was modified accordingly and tagged `#ki-ha-main-k8s-master-app-node-failure-cloud-azure`; see also the v2.8.0 RNs `#fix-cloud-vip-aws` {{< xref s="release-notes" f="version-2.8/v2.8.0.md" text="v2.8.0 release notes" a="fix-cloud-vip-aws" text="AWS fix note" >}}.
    {{< /internal-release-note >}}

- <a id="ki-ha-duplicated-data-on-recovery"></a>The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.

    {{< internal-release-note rnid="ki-ha-duplicated-data-on-recovery" id="4887-7790" >}}
**Bugs:** {{< jira ig="4887" >}} (streaming &amp; S3 data append)<br/>
**Owner:** Orit N. (oritn)<br/>
Known issue since RN v1.5.0 for Bug {{< jira ig="4887" >}} and since RN v1.7.0 also for Bug {{< jira ig="7790" >}} (update-expression addition).<br/>
<br/>
(sharonl) This issue is also documented in the **"Software Specifications and Restrictions"** doc (`#duplicated-sys-failure-recovery-data`).
<br/>
(27.2.19) Bug {{< jira ig="7790" >}} has now been closed as _Won't Fix_.
Ori wrote in the comments "This is a known arch limitation of the system, at this point one can use the conditional update with the mtime returned on the command".
(2.6.19) Orit confirmed that the issue persists in v2.2.0 and she and Adi said to keep the known issue in the release notes as-is (see also this IG-7790 [comment](https://jira.iguazeng.com/browse/IG-7790?focusedCommentId=44092&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-44092) in Bug {{< jira ig="7790" >}}).
Orit wrote that this issue isn't related to the app nodes but rather the data path.
<br/>
(17.10.19) I decided to stop making RN-updates in Bug {{< jira ig="7790" >}} because the bug has been closed as _Won't Fix_ for a while and the issue isn't likely to be resolved in the foreseeable future => permanent known issue.
Bug {{< jira ig="4887" >}}, which is also related to this known issue, is still open, so I still update the RN information in that bug.
{{< comment >}}<!-- (25.2.21) I removed Bug IG-7790 from the "Bugs" in the
  internal RN info at the start so it's not checked for future releases. -->
<!-- [PERMANENT-KI] in relation to Bug IG-7790 (not IG-4887). -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-ha-getitems-failure-ig-6475"></a>The <api>GetItems</api> NoSQL Web API operation might fail during failover.

    {{< internal-release-note rnid="ki-ha-getitems-failure-ig-6475" ki_start_ver="1.7.0" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (26.2.19) In previous release notes we had a combined HA known issue for bugs {{< jira ig="6475" >}} and {{< jira ig="6246" >}}, phrased generically as "I/O requests might fail during failover.", per Ori's guideline (see, for example, the {{< xref s="release-notes" f="version-1.9/v1.9.5.md" a="ki-ha-io-requests-failure-ig-6475-6246" text="v1.9.5 RNs" >}}) and the details in the bugs' _Release Note Information_).
In v2.0.0, Bug IG-6246 was fixed and closed, but Bug IG-6475 remained open and was deferred to v2.4.0.
Roy B. and Ortal confirmed that the remaining known issue for Bug IG-6475 is only for the <api>GetItems</api> NoSQL Web API operation.
Therefore, I rephrased the known issue and created a {{< xref s="release-notes" f="version-2.0/v2.0.0.md" text="v2.0.0 release notes" a="fix-ha-io-stream-requests-failure-ig-6246" text="fix RN" >}} for Bug IG-6246.
<br/>
(23.2.12) Bug {{< jira ig="6475" >}} was fixed and closed in v3.0.0.
Orit explained that the fix is that in case of failover, <api>GetItems</api> will no longer return duplicate/missing items, it will stop and return status code 513.
But the known issue, as phrased in the release notes (possible failure of <api>GetItems</api> during failover) persists and should remain documented.
=> I unlinked the RN from Bug IG-6475 and removed the release-notes requirement tag to avoid following up on the bug resolution.
This should remain a known issue in future release notes as well, until further notice.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-dashboard" text="Dashboard (UI)" >}}

{{< note id="ki-dashboard-other-ki-secs-ref-note" >}}
See the [MLRun](#ki-mlrun) and [Nuclio](#ki-nuclio) known issues for dashboard issues related specifically to these frameworks.
{{< /note >}}
{{< comment >}}<!-- [TODO-NEXT-RNS] Check #ki-mlrun and #ki-nuclio for
  relevance. -->
{{< /comment >}}

<!-- //////////////////////////////////////// -->
<!-- Notes -->
{{< rn-heading t="notes" >}}

- <a id="note-coordinated-platform-actions"></a>Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.

    {{< internal-release-note rnid="note-coordinated-platform-actions" docig="5829" >}}
<br/>
(sharonl) (2.4.18) Adi requested that we add this in the release notes (starting with the v1.5.3 RNs, even though it's also true for earlier versions).
<br/>
(20.3.19) At Maor's request, I rephrased the previous note to replace "restart, and shut-down" with "start-up and upgrade" (+ I added "actions"); Maor said that customers don't need to coordinate platform shutdowns.
<br/>
(25.5.20)At Maor's request, I returned the reference to "shut-down". Maor wrote (in his v2.8.0 RNs review) &mdash; "I know I (probably) said a user can shutdown the platform by himself (cause he can, thru the UI), but let's add "shutdown" there as well.
The reason is that customer can't really verify a successful shutdown, and might end up with a full data loss if they physically shutdown the servers without verifying the data from the cache was dumped into the disks."
    {{< /internal-release-note >}}

