---
title:    "Version 2.10.0 Release Notes"
subtitle: "Release Date: 6 Sep 2020"
keywords: "release notes"
menu:
  main:
    name:       "V2.10.0 Release Notes"
    parent:     "release-notes-2.10"
    identifier: "release-notes-2.10.0"
    weight:     900
---
{{< comment >}}<!--
- TODO: Update the front-matter metadata and the version number in the opening
  paragraph and at the start of the #highlights section.
  => (16.9.20) DONE for v2.10.0. 
- TODO: Check for internal "[TODO-NEXT-RNS]" comments for changes to be made in
  the next release notes (and add/keep comments for the new release).
  => (13.10.20) Done for v2.10.0.
- [TODO-POST-REVIEW] Remove the internal-release-note "reviewer" param for each
  reviewed note. => (5.10.20) DONE for v2.10.0.
- TODO: Handle any [TODO-BEFORE-PUBLICATION] comments for open RN issues.
  => (16.10.20) DONE for v2.10.0.
- TODO: At the end, update the subsection links in each h2 RN section and check
  all section cross-references/links to ensure they're relevant to the final
  release notes for this version. => (16.10.20) Done for v2.10.0.
  Also, check for the applicability of the Tech Preview and Beta notes.
  => (16.10.20) DONE for v2.10.0.
-->
{{< /comment >}}

This document outlines the main additions and changes to the Iguazio MLOps Platform ("the platform") in version 2.10.0, and known issues in this version.

{{< techpreview note="1" >}}

{{< comment >}}<!-- [IntInfo] (sharonl) The RNs don't include beta features. -->
{{< beta note="1" >}}
{{< /comment >}}

{{< internal-release-note id="internal-release-jira-n-confluence-links" >}}
**V2.10.0 RNs DOC Task:** {{< jira ig="16761" >}}
<br/>
**[Release Index](https://confluence.iguazeng.com/display/RM/Release+Index) (Confluence):** &mdash; [2.10.0](https://confluence.iguazeng.com/display/RM/Release+Index#ReleaseIndex-2.10.0)
<br/>
**Build:** (6.9.20) `2.10_b79_20200901170103`; (7.9.20) `2.10_b81_20200903195924` (final v2.10.0 GA build)
<br/>
**[V2.10.0 Status](https://jira.iguazeng.com/secure/Dashboard.jspa?selectPageId=11213)** ;
**[V2.10.0 Program](https://jira.iguazeng.com/secure/Dashboard.jspa?selectPageId=11300)**
<br/>
**V2.10.0 Requirements** &mdash; [all &mdash; Requirements, Sub-Requirements, Improvements](https://jira.iguazeng.com/issues/?jql=project%20=%20ig%20AND%20(%22Target%20Version%22%20in%20(2.10.0)%20OR%20fixVersion%20in%20(2.10.0)%20OR%20affectedVersion%20in%20(2.10.0))%20AND%20issuetype%20in%20(Requirement,%20Sub-Requirement,%20Improvement)); [Requirements](https://jira.iguazeng.com/issues/?jql=project%20%3D%20ig%20AND%20(%22Target%20Version%22%20in%20(2.10.0)%20OR%20fixVersion%20in%20(2.10.0)%20OR%20affectedVersion%20in%20(2.10.0))%20AND%20issuetype%20%3D%20Requirement)
<br/>
**V2.10.0 release-notes tickets:** [Jira query](https://jira.iguazeng.com/issues/?jql=labels in (RN-2.10.0, RN-2.10.0-fixes, RN-2.10.0-known-issues, RN-2.10.0-TechPreview))){{< comment >}}<!-- `v2.10.0_RN_done` filter (#13429)  -->
  <!-- [ci-comment-shcd-extra-space] Moving the opening `comment` tag to the
    next line results in extra space in the output, even if the <br/> tag is
    removed. -->
{{< /comment >}}
<br/>
**Issues intentionally excluded from the v2.10.0 RNs:** [Jira query](https://jira.iguazeng.com/issues/?jql=labels%20in%20(RN-2.10-excluded-fixes%2C%20RN-2.10-excluded-known-issues))
{{< comment >}}<!-- `v2.10_RN_excluded` filter (#13129) -->
{{< /comment >}}
{{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Highlights-->
{{< rn-heading t="highlights" >}}

Version 2.10.0 introduces many new powerful features and enhancements, as well as bug fixes, as detailed in the release notes.
Following are some of the main highlights of this release:

- Upgraded the [MLRun](#new-mlrun) version, including these enhancements {{< techpreview mark="1" >}} &mdash;

    - [New job-creation dashboard interface](#new-mlrun-ui-job-creation), including marketplace functions selection and support for advanced configuration
    - [Enhanced the hyperparameters support](#new-mlrun-hyperparams-enh), including a new <paramname>tuning_strategy</paramname> run parameter for determining the tuning algorithm &mdash; list, grid search, or random search
    - [New method for loading a function as an inline Python module](#new-mlrun-function_to_module) (<func>function_to_module</func>)
- Upgraded the [Frames](#new-frames) version, including these enhancements &mdash;
    - [Automatic command-history logging and retrieval method](#new-frames-history) (<func>history</func>) for troubleshooting
    - Support for [writing partitioned NoSQL tables](#new-frames-nosql-write-partitioned-tables-tp) {{< techpreview mark="1" >}}
- Support for [filtering performance reports by interface](#new-ui-data-container-performance-by-interface) in the dashboard's data-container overview
- Support for the [Spark <api>cluster</api> deploy mode](#new-spark-deploy-cluster-mode-scala-java)
- Support for the [V3IO Python SDK (**v3io-py**) NoSQL and streaming client APIS](#new-v3io-py-sdk-nosql-n-stream)
- Enhanced security with [stricter password restrictions](#new-stricter-pwd) and support for [API-gateway access-key authentication](#new-ui-project-api-gatweways-user-auth) 
- Support for [adding an Oracle Presto connector](#new-presto-oracle-conenctor-support-tp) {{< techpreview mark="1" >}} 

<!-- //////////////////////////////////////// -->
<!-- New Features and Enhancements -->
{{< rn-heading t="new-and-enhance" >}}

[Application Services](#new-managed-app-services) | [Dashboard (UI](#new-dashboard) | [Frames](#new-frames) | [General](#new-general) | [Grafana](#new-logging-n-monitoring-services) | [Horovod](#new-horovod) | [Jupyter](#new-jupyter) | [Logging and Monitoring Services](#new-logging-n-monitoring-services) | [MLRun](#new-mlrun) | [MPI Operator](#new-horovod) | [Nuclio](#new-nuclio) | [Presto](#new-presto) | [Prometheus](#new-tsdb-prometheus) | [Python SDK](#new-v3io-py) | [Security](#new-security-n-user-management) | [Spark](#new-spark) | [TSDB](#new-tsdb) | [TSDB Nuclio Functions](#new-tsdb-nuclio-funcs) | [User Management](#new-security-n-user-management) | [Web Shell](#new-web-shell)

<!-- ---------------------------------------- -->
{{< rn-heading id="new-general" text="General" >}}

- <a id="new-sys-attr-__last_sequence_num-sys"></a>Added a <attr>__last_sequence_num</attr> system attribute that stores the sequence number of the last record in a stream shard; applicable only to stream-shard objects.

    {{< internal-release-note rnid="new-sys-attr-__last_sequence_num-sys" ig="15449" type="req" owner="Alex T. (alext)" docig="15551" >}}
<br/>
(sharonl) (11.10.20) See also the related UI [#new-ui-last-sequence-data-browse-metadata](#new-ui-last-sequence-data-browse-metadata) enhancement note for the UI Sub-Requirement {{< jira ig="15550" >}} &mdash; display the value of the new attribute in the dashboard data-browse object metadata.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-managed-app-services" text="Managed Application Services" >}}

{{< note id="new-services-other-secs-ref-note" >}}
See also the [dashboard](#new-dashboard) new features and enhancements for UI improvements related to managing application services.
{{< /note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-jupyter" text="Jupyter Notebook" >}}

- <a id="new-jupyter-v3io-py-sdk-pre-deploy"></a>Pre-deployed version 0.3.20 of the platform's Python SDK &mdash; the Iguazio V3IO Python SDK (**v3io-py**) &mdash; as part of the Jupyter Notebook service.

    {{< internal-release-note rnid="new-jupyter-v3io-py-sdk-pre-deploy" ig="15681" type="req" owner="Uri H. (urih)" docig="16392" >}}
    {{< /internal-release-note >}}

- <a id="new-jupyter-versions-upgrade"></a>Upgraded versions of pre-deployed and certified software &mdash;

    {{< internal-release-note rnid="new-jupyter-version-upgrade" docig="16762" >}}
    {{< /internal-release-note >}}

    - <a id="new-jupyter-python-version-upgrade"></a>Python version 3.7
        {{< internal-release-note rnid="new-jupyter-python-version-upgrade" ig="15378" type="req" owner="Uri H. (urih)" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-nuclio-version-upgrade"></a>Nuclio Jupyter library (**nuclio-jupyter**) version 0.8.5
        {{< internal-release-note rnid="new-jupyter-nuclio-version-upgrade" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-v3io-tutorials-version-upgrade"></a>Platform tutorial Jupyter notebooks &mdash; version 2.10.5
        {{< internal-release-note rnid="new-jupyter-v3io-tutorials-version-upgrade" >}}
        {{< /internal-release-note >}}
    - <a id="new-jupyter-frames-client-version-upgrade"></a>V3IO Frames client &mdash; version 0.7.36
        {{< internal-release-note rnid="new-jupyter-frames-client-version-upgrade" >}}
        {{< /internal-release-note >}}

- <a id="new-jupyter-disable-scale-to-zero-by-default"></a>Disabled the <gui-label>Scale to zero</gui-label> option by default for new Jupyter Notebook services.
    You can still select to enable this option from the <gui-title>Services</gui-title> dashboard, at any stage.

    {{< internal-release-note rnid="new-jupyter-disable-scale-to-zero-by-default" ig="16287" type="req" owner="Oded M. (odedm)" docig="16307" >}}
    {{< /internal-release-note >}}

- <a id="new-jupyter-ui-dir-rename"></a>Blocked moving (renaming) of directories with nested directories from the Jupyter UI, to prevent possible data loss as a result of the recursive copy that's used to implement the move.
    You can still move such directories, when necessary, by running recursive file-system copy and delete commands from a command line.
    For more information, see the {{< xref f="cluster-mgmt/deployment/sw-specifications.md" a="jupyter-dir-cp-mv-rename" text="Jupyter Notebook directory copy and move software restrictions" >}}.

    {{< internal-release-note rnid="new-jupyter-ui-dir-rename" ig="15813" type="bug" owner="Uri H. (urih)" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-v3io-py" text="V3IO Python SDK" >}}

- <a id="new-v3io-py-sdk-nosql-n-stream"></a>Added support for the NoSQL (key-value) and streaming client APIs of the platform's Python SDK &mdash; the Iguazio V3IO Python SDK (**v3io-py**), version 0.3.20.
    The SDK is also pre-deployed in the platform's Jupyter Notebook service (see the [Jupyter Notebook enhancements](#new-jupyter-v3io-py-sdk-pre-deploy)).

    {{< internal-release-note rnid="new-v3io-py-sdk-nosql-n-stream" ig="14462" type="req" owner="Eran D. (erand)" docig="15596" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-frames" text="V3IO Frames" >}}

- <a id="new-frames-version-upgrade"></a>Upgraded to Frames server version 0.7.37 and client version 0.7.36.

    {{< internal-release-note rnid="new-frames-version-upgrade" owner="Dina N. (dinan)" docig="16762" >}}
    {{< /internal-release-note >}}

- <a id="new-frames-history"></a>Added automatic logging of Frames operations (commands history), and a <func>history</func> method for retrieving information from the history logs.

    {{< internal-release-note rnid="new-frames-history" ig="15354" type="req" owner="Tal N. (taln)" docig="15554" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="new-frames-nosql" text="Frames NoSQL Backend (&quot;nosql&quot;/&quot;kv&quot;)" >}}

- <a id="new-frames-nosql-write-partitioned-tables-tp"></a>{{< techpreview mark="1" >}} Added support for writing partitioned tables.

    {{< internal-release-note rnid="new-frames-nosql-write-partitioned-tables-tp" ig="13538" type="req" owner="Tal N. (taln)" docig="15553" >}}
<br/>
(sharonl) (27.9.20) See the related [<api>"overwriteTable"</api> save-mode known issue](#ki-frames-nosql-write-partitioned-table-overwriteTable-not-supported)
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-mlrun" text="MLRun" techpreview="1" >}}

- <a id="new-mlrun-version-upgrade-tp"></a>Upgraded to MLRun version 0.5.1.
    For future upgrades to newer MLRun releases, consult {{< company >}}'s {{< email id="support" link="1" text="support team" >}}.

    {{< internal-release-note rnid="new-mlrun-version-upgrade-tp" docig="16762" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-default-service-on-prem-predeploy-tp"></a>Added a default (pre-deployed) shared single-instance tenant-wide MLRun service (`mlrun`) to on-prem deployments (VM and bare-metal).
    (Previously, this service was pre-deployed only in the cloud.)

    {{< internal-release-note rnid="new-mlrun-default-service-on-prem-predeploy-tp" ig="14393" type="req" owner="Hedi I. (hedii)" docig="15882" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-ui-job-creation"></a>Added a new job-creation interface to the MLRun dashboard, including marketplace functions selection and support for advanced configuration.

    {{< internal-release-note rnid="new-mlrun-ui-job-creation" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-git-n-slack-notif"></a>Added support for Git and Slack notifications.

    {{< internal-release-note rnid="new-mlrun-git-n-slack-notif" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-hyperparams-enh"></a>Enhanced the hyperparameters support: added a new <paramname>tuning_strategy</paramname> run parameter for determining the tuning algorithm &mdash; list, grid search, or random search. You can also provide the input as a file.

    {{< internal-release-note rnid="new-mlrun-hyperparams-enh" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-function_to_module"></a>Added a <func>function_to_module</func> method for loading a function as an inline Python module.

    {{< internal-release-note rnid="new-mlrun-function_to_module" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-remote-access-n-jobs-n-workflows-submit-impr"></a>Simplified and improved remote access and submission of jobs and workflows.

    {{< internal-release-note rnid="new-mlrun-remote-access-n-jobs-n-workflows-submit-impr" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-kubeflow-v1-mpi-operator"></a>Added support for Kubeflow v1 MPI Operator for distributed machine learning using Horovod.

    {{< internal-release-note rnid="new-mlrun-kubeflow-v1-mpi-operator" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
<br/>
(sharonl) (6.10.20) Gilad referred to https://github.com/kubeflow/mpi-operator/blob/master/deploy/v1/mpi-operator.yaml.
`v1` seems to refer to the [Kubeflow](https://www.kubeflow.org/) version (the latest is currently v1.1).
[Kubeflow MPI Operator](https://github.com/kubeflow/mpi-operator) has its own versioning; the latest is v0.2.3, which is the version of the `mpi-operator` service in platform v2.10.0.
The service type is `Horovod` and it's used to support d for distributed ML using Horovod.
    {{< /internal-release-note >}}

- <a id="new-mlrun-log-level-per-run"></a>Added support for setting the log level for each run.

    {{< internal-release-note rnid="new-mlrun-log-level-per-run" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

- <a id="new-mlrun-scheduling-impr"></a>Improved the scheduling mechanism.

    {{< internal-release-note rnid="new-mlrun-scheduling-impr" ig="15569" type="req" owner="Hedi I. (hedii)" docig="16508" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-nuclio" text="Nuclio Serverless Functions" >}}

- <a id="new-nuclio-version-upgrade"></a>Upgraded to Nuclio version 1.4.17.

    {{< internal-release-note rnid="new-nuclio-version-upgrade" owner="Oded M. (odedm)" docig="16762" >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-cors"></a>Added support for cross-origin resource sharing (CORS) for Nuclio functions, allowing access to selected web-page resources from a different origin (domain) than that on which the resources are being served.

    {{< internal-release-note rnid="new-nuclio-cors" ig="14734" type="req" owner="Liran B. (lirang)" docig="16397" >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-go-modules"></a>Upgraded to Go version 1.4.3, including the use of [Go modules](https://github.com/golang/go/wiki/Modules) to manage module dependencies.
    {{< note id="nuclio-go-offline-deploy-note" >}}Go modules require an internet connection for downloading module files.
      Therefore, building Go functions in an air-gapped environment without an internet connection (a.k.a. "a dark site") requires special treatment.
      See additional information in the {{< xref f="cluster-mgmt/deployment/sw-specifications.md" a="nuclio-offline-deploy-go" text="Nuclio software specifications and restrictions" >}}.
{{< comment >}}<!-- [IntInfo] (sharonl) (13.10.20) Oded said that only the
  build portion needs to be done with an internet connection, and that after
  the function is built, the deployment itself is still "dark-site friendly".
  (He said that when you hit deploy - I think he meant when you select this in
  the UI - it actually does build + deploy. -->
{{< /comment >}}
    {{< /note >}}

    {{< internal-release-note rnid="new-nuclio-go-modules" ig="16556" type="bug" owner="Oded M. (odedm)" >}}
<br/>
(sharonl) (7.10.20) In [Nuclio v1.4.14](https://github.com/nuclio/nuclio/releases/tag/1.4.14) we upgraded to [Go v1.14.3](https://github.com/golang/go/releases/tag/go1.14.3), which is also used in the v2.10.0 platform version of Nuclio &mdash; [Nuclio v1.4.17](https://github.com/nuclio/nuclio/releases/tag/1.4.17).
<br/>
Platform v2.8.0 uses Nuclio v1.3.25, which uses an earlier version of Go without Go modules, and therefore Nuclio Go functions can be deployed offline in this release without any special handling on the user's part.
<br/>
Note that as explained by Oded, other code languages also require an internet connection for deployment, including Python (unless you use only the built-in packages).
In [Nuclio v1.4.2](https://github.com/nuclio/nuclio/releases/tag/1.4.2) we moved to Go version 1.4 and the use of [Go modules](https://github.com/golang/go/wiki/Modules), which rely on an internet connection to get the required module files for resolving build dependencies.
<br/>
Bug {{< jira ig="16556" >}} was closed as _Won't Fix_.
It was decided that we'll give the users general guidelines for offline Go deployment in the documentation.
<br/>
For additional information and doc considerations, see the comments in Bug {{< jira ig="16556" >}}.
{{< comment >}}<!-- [c-nuclio-go-offline-deploy] [IntInfo] (sharonl) (7.10.20)
  Orit and Oded said that the source of the issue with the offline deployment
  of Go Nuclio functions is different than that of Ruby, Node.js, and .NET Core
  functions, for which we have a separate known issue
  (#ki-nuclio-unsupported-dark-site-langs) for Bug 13134, which is still open
  (even though there's a share issue of a required internet connection). -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="new-nuclio-kafka-ret-headers-n-timestamps"></a>Kafka trigger (<api>kafka-cluster</api>) &mdash; added Kafka headers and timestamps to the event information.

    {{< internal-release-note rnid="new-nuclio-kafka-ret-headers-n-timestamps" ig="15558" type="req" owner="Sahar E. (sahare)" >}}
<br/>
(sharonl) (2.10.20) Nir confirmed that we can document this for v2.10.0 even though the ticket is currently marked as _DONE_ (not _TESTED_) and has QA labels.
Nir wrote that the new functionality was verified with Quadient.
{{< comment >}}<!-- [IntInfo] (sharonl) Adi and Orit referred to Nir. See the
  "URGENT] V2.10.0 Untested Requirements"  email thread. -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="new-tsdb-nuclio-funcs" text="TSDB Nuclio Functions" >}}
{{< comment >}}<!-- [c-tsdb-nuclio-funcs-classif] -->
{{< /comment >}}

- <a id="new-tsdb-nuclio-funcs-version-upgrade"></a>Upgraded to V3IO TSDB Nuclio Functions version 0.5.11.

    {{< internal-release-note rnid="new-tsdb-nuclio-funcs-version-upgrade" owner="Dina N. (dinan)" docig="16762" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-tsdb" text="Time-Series Databases (TSDBs)" >}}

- <a id="new-tsdb-version-upgrade"></a>Upgraded to V3IO TSDB version version 0.10.12.

    {{< internal-release-note rnid="new-tsdb-version-upgrade" owner="Dina N. (dinan)" docig="16762" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="new-tsdb-prometheus" text="Prometheus" >}}

- <a id="new-tsdb-prometheus-version-upgrade"></a>Upgraded to V3IO Prometheus version 3.3.6.

    {{< internal-release-note rnid="new-tsdb-prometheus-version-upgrade" owner="Dina N. (dinan)" docig="16762" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-presto" text="Presto" >}}

- <a id="new-presto-add-execution-plan-sql-log-info"></a>Added execution-plan information for SQL queries on platform NoSQL tables to the Presto coordinator logs.

    {{< internal-release-note rnid="new-presto-add-execution-plan-sql-log-info" ig="15647" type="req" owner="Tal N. (taln)" docig="16393" >}}
    {{< /internal-release-note >}}

- <a id="new-presto-oracle-conenctor-support-tp"></a> {{< techpreview mark="1" >}} Added support for adding an Oracle connector.
    For details, contact {{< company >}}'s {{< email id="support" link="1" text="support team" >}}.

    {{< internal-release-note rnid="new-presto-oracle-conenctor-support-tp" ig="15448" type="req" owner="Dina N. (dinan)" docig="15555" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-spark" text="Spark" >}}

- <a id="new-spark-deploy-cluster-mode-scala-java"></a>Added support for the Spark <api>cluster</api> deploy mode for submitting Spark jobs by adding <opt>--deploy-mode=cluster</opt> to the <cmd>spark-submit</cmd> call.
    This mode is supported for Scala and Java; Spark doesn't currently support Python in standalone clusters.
    In the <api>cluster</api> mode, the driver is launched from a worker process in the cluster, while in the default <api>client</api> deploy mode the driver is launched in the same worker process as the client that submits the application (such as Jupyter Notebook, a web shell, or Zeppelin).
    Cluster deployment provides advantages such as the ability to automate jobs execution and run Spark jobs remotely on the cluster &mdash; which is useful, for example, for running ongoing Spark jobs, such as streaming.

    {{< internal-release-note rnid="new-spark-deploy-cluster-mode-scala-java" ig="14199" type="req" owner="Dina N. (dinan)" docig="16398" >}}
<br/>
(sharonl) (8.10.20) See the Spark **Spark Standalone Mode > [Launching Spark Applications](https://spark.apache.org/docs/latest/spark-standalone.html#launching-spark-applications)** documentation and our internal **[Enabling cluster deploy-mode for the Spark service](https://confluence.iguazeng.com/pages/viewpage.action?spaceKey=FC&title=Enabling+cluster+deploy-mode+for+the+Spark+service)** Confluence page.
    {{< /internal-release-note >}}

- <a id="new-spark-enable-logs-cleanup-by-workers"></a>Enabled spark workers to clean up old logs after a week.

    {{< internal-release-note rnid="new-spark-enable-logs-cleanup-by-workers" ig="15565" type="req" owner="Dina N. (dinan)" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-web-shell" text="Web Shell" >}}

- <a id="new-web-shell-python-version"></a>Upgraded to Python version 3.7.

    {{< internal-release-note rnid="new-web-shell-python-version" ig="15378" type="req" owner="Uri H. (urih)" docig="16762" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-horovod" text="Horovod / MPI Operator" >}}

- <a id="new-horovod-mpi-opeartor-version-upgrade"></a>Upgraded the **mpi-operator** Horovod service to Kubeflow MPI Operator version 0.2.3.

    {{< internal-release-note rnid="new-horovod-mpi-opeartor-version-upgrade" owner="Oded M. (odedm)" docig="16762" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="new-logging-n-monitoring-services" text="Logging and Monitoring Services" >}}

- <a id="new-grafana-smtp-alerts-tp"></a>{{< techpreview mark="1" >}} Added support for sending Grafana email alerts via SMTP.

    {{< internal-release-note rnid="new-grafana-smtp-alerts-tp" ig="15741" type="req" owner="Sahar E. (sahare)" >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-file-system" text="File System" >}}

- <a id="new-local-fs-for-new-container-no-need-to-restart-cli-service"></a>Newly created data containers can now be accessed using local file-system commands without restarting the command-line service (such as a web shell or Jupyter Notebook).

    {{< internal-release-note rnid="new-local-fs-for-new-container-no-need-to-restart-cli-service" ig="2259" type="req" owner="Roy B. (royb)" docig="15546" >}}
<br/>
(sharonl) (11.10.20) In v2.8.0, we documented the related issue in the SW specs and restrictions and working with containers getting-started tutorial, but not as a release-note known issue; the previous doc restrictions were removed in the v2.10.0 doc (see DOC {{< jira ig="15546" >}}).
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-security-n-user-management" text="Security and User Management" >}}

- <a id="new-stricter-pwd"></a>Enforced stricter password restrictions.
    User passwords in v2.10.0 must conform to the following restrictions:

    - Contain at least one uppercase letter (A&ndash;Z)
    - Contain at least one lowercase letter (a&ndash;-z)
    - Contain at least one special character (!, @, #, $, %, ^, &, *)
    - Contain at least one digit (0&ndash;9)
    - Length of at least 8 characters

    {{< internal-release-note rnid="new-stricter-pwd" ig="14100" type="req" owner="Adam M. (adamm)" >}}
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="new-dashboard" text="Dashboard (UI)" >}}

- <a id="new-ui-data-container-performance-by-interface"></a>In the <gui-title>Data</gui-title> container overview, added support for filtering the container performance reports by API interface.

    {{< internal-release-note rnid="new-ui-data-container-performance-by-interface" ig="10101" type="req" owner="Eran N. (erann)" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-help"></a>Added a <gui-title>Help</gui-title> page with getting-started and additional information to facilitate the user experience.

    {{< internal-release-note rnid="new-ui-help" ig="15370" type="req" owner="Ern N. (erann)" docig="16396" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-cluster-set-data-access"></a>Added support for IT administrators to switch between read-only and read-write cluster data-access modes.

    {{< internal-release-note rnid="new-ui-cluster-set-data-access" >}}
**Requirement:** {{< jira ig="14366" >}}<br/>
**Sub-Requirements:** {{< jira ig="14365" >}} (Engine); {{< jira ig="15582" >}} (UI)<br/>
**Owners:** Shaul K. (shaulk) (Engine); Eran N. (erann) (UI)<br/>
**Doc Issue:** {{< jira ig="16395" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-project-api-gatweways-user-auth"></a>On the <gui-title>API Gateway</gui-title> project tab, added an <gui-label>Access Key</gui-label> authentication type for access-key authentication.

    {{< internal-release-note rnid="new-ui-project-api-gatweways-user-auth" ig="15584" type="req" owner="Eran N. (erann)" docig="14512" >}}
<br/>
(sharonl) (5.10.20) API gateways are used for load balancing of Nuclio functions.
    {{< /internal-release-note >}}

- <a id="new-ui-nuclio-kafka-n-v3io-stream-triggers-consumer-group-cfg"></a>On the <gui-title>Projects</gui-title> project <gui-title>Configuration</gui-title> tab, added consumer-group configuration options for the Nuclio Kafka trigger (<api>kafka-cluster</api>) and the platform trigger (<api>v3ioStream</api>).

    {{< internal-release-note rnid="new-ui-nuclio-kafka-n-v3io-stream-triggers-consumer-group-cfg" >}}
**Requirements:** {{< jira ig="15703" >}} (Kafka); {{< jira ig="15692" >}} (V3IO stream)<br/>
**Owner:** Eran N. (erann)
    {{< /internal-release-note >}}

- <a id="new-ui-last-sequence-data-browse-metadata"></a>Added a <gui-label>Last sequence</gui-label> field to the <gui-title>General</gui-title> object browse metadata for displaying the sequence number of the last record in a stream shard; applicable only to stream-shard objects.

    {{< internal-release-note rnid="new-ui-last-sequence-data-browse-metadata" ig="15550" type="subreq" req="15449" owner="Eran N. (erann)" docig="15551" >}}
<br/>
(sharonl) (11.10.20) See also the related general [#new-sys-attr-__last_sequence_num-sys](#new-sys-attr-__last_sequence_num-sys) enhancement note for the parent Requirement {{< jira ig="15449" >}} &mdash; add a <attr>__last_sequence_num</attr> system attribute whose value is displayed in the new dashboard metadata field.
    {{< /internal-release-note >}}

- <a id="new-ui-smtp-alert-emails"></a>On the <gui-title>SMTP</gui-title> settings page, added an optional <gui-label>Users to notify</gui-label> parameter for specifying users with the IT Admin management policy who will receive alert email notifications for the cluster. In the current release, this option is available only to users who have both the IT Admin and Tenant Admin management policies, such as the predefined tenancy_admin user; see the related [known issue](#ki-ui-smtp-email-notif-tenant-admin-req).
    {{< comment >}}<!-- [ci-underscore-distorts-md-syntax-higlight] See info
      for #ki-ui-smtp-email-notif-tenant-admin-req. -->
    {{< /comment >}}

    {{< internal-release-note rnid="new-ui-smtp-alert-emails" >}}
**Requirement:** {{< jira ig="14371" >}} (Platform); UI Sub-Requirement {{< jira ig="15183" >}}; Bug {{< jira ig="16892" >}} (Platform)<br/>
**Owners:** Oded M. (odedm) (Platform); Eran N. (erann) (UI)<br/>
**Doc Issue:** {{< jira ig="15568" >}}
    {{< /internal-release-note >}}

- <a id="new-ui-cluster-op-status-malfunction-mssg"></a>Display a visible message on the dashboard when the cluster is not in a normal online operational status.

    {{< internal-release-note rnid="new-ui-cluster-op-status-malfunction-mssg" ig="15687" type="req" owner="Eran N. (erann)" >}}
    {{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Fixes -->
{{< rn-heading t="fixes" >}}

[Application Services](#fixes-managed-app-services) | [Frames](#fixes-frames) | [Jupyter](#fixes-jupyter) | [Logging and Monitoring Services](#fixes-logging-n-monitoring-services) | [MLRun](#fixes-mlrun) | [Nuclio](#fixes-nuclio) | [Presto](#fixes-presto)

<!-- ---------------------------------------- -->
{{< rn-heading id="fixes-managed-app-services" text="Managed Application Services" >}}

- <a id="fix-service-admin-policy-via-group-only"></a>Fixed assignment of service-administration permissions to users who are members of a user group with the Service Admin management policy but aren't assigned this policy directly.
    Previously, such users could see only their services on the <gui-label>Services</gui-label> dashboard page, and any changes that they made to their services impacted other users' services.

    {{< internal-release-note rnid="fix-service-admin-policy-via-group-only" ig="16267" type="bug" ki_start_ver="2.8.0" owner="Oded M. (odedm)" docig="16761" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-jupyter" text="Jupyter Notebook" >}}

- <a id="fix-jupyter-scale-to-zero-wakeup-wo-service-admin-policy"></a>Allowed a running user of a scaled-to-zero Jupyter Notebook service to wake up the service by selecting its name on the <gui-title>Services</gui-title> dashboard page, even when the user doesn't have the Service Admin management policy.

    {{< internal-release-note rnid="fix-jupyter-scale-to-zero-wakeup-wo-service-admin-policy" ig="15862" type="bug" ki_start_ver="2.8.0" owner="Hedi I. (hedii)" >}}
    {{< /internal-release-note >}}

- <a id="fix-jupyter-multi-nbs-same-user-run-err"></a>Fixed concurrent execution of multiple notebooks by the same running user.

    {{< internal-release-note rnid="fix-jupyter-multi-nbs-same-user-run-err" ig="15364" type="bug" ki_start_ver="2.8.0" owner="Assaf B. (assafb)" >}}
<br/>
(sharonl) (24.5.20) Assaf B. wanted to restrict this to execution from different instances of the Jupyter NB service, as this is the bug that was reported and tested and Assaf suspects the issue won't occur when running multiple NBs from the same Jupyter NB service. But Orit preferred a more generic phrasing, just to be safe.
See additional info in IG-15364.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-frames" text="V3IO Frames" >}}

- <a id="fix-frames-read-backend-param-no-default-value"></a>Removed the default value for the <func>read</func> client method's mandatory <paramname>backend</paramname> parameter.

    {{< internal-release-note rnid="fix-frames-read-backend-param-no-default-value" ig="15884" type="bug" owner="Tal N. (taln)" docig="16848" >}}
<br/>
(sharonl) (26.9.20) This wasn't previously documented as a known issue, but I added a release note because it's a type of API change, even though the parameter was already documented as a mandatory (required) parameter in previous releases.
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="fix-frames-tsdb" text="Frames TSDB Backend (&quot;tsdb&quot;/&quot; Backend)" >}}

- <a id="fix-frames-tsdb-delete-metrics-or-filter-wo-time-range"></a>Setting the <func>delete</func> client method's metrics-name (<paramname>metrics</paramname>) or filter-expression (<paramname>filter</paramname>) parameter without a time-filter parameter (<paramname>start</paramname> and/or <paramname>end</paramname>) now applies the correct filter instead of deleting the entire table.

    {{< internal-release-note rnid="fix-frames-tsdb-delete-metrics-or-filter-wo-time-range" ig="15658" type="bug" ki_start_ver="2.8.0" owner="Dina N. (dinan)" docig="14819" >}}
{{< comment >}}<!-- [c-frames-tsdb-delete-metrics-or-filter-wo-time-range] -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-mlrun" text="MLRun" techpreview="1" >}}

- <a id="fix-mlrun-no-ui-disable"></a>The MLRun service can now be disabled from the dashboard.

    {{< internal-release-note rnid="fix-mlrun-no-ui-disable" >}}
<br/>
(sharonl) (27.9.20) I couldn't locate a Jira ticket for this (I searched in Jira and consulted Hedi and Oded); this was also documented as a {{< xref f="release-notes/version-2.8/v2.8.0.md" a="ki-mlrun-no-ui-disable" text="v2.8.0 release-notes known issue" >}} without a Jira ticket.
I confirmed with Hedi and Oded that in v2.10.0 it's possible to disable MLRun from the dashboard.
(30.9.20) I mentioned the v2.8.0 known issue and v2.10.0 fix in the _Release Note Information_ for Requirement {{< jira ig="14393" >}} for adding MLRun as a predefined service (v2.10.0 DOC {{< jira ig="15882" >}}).
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-nuclio" text="Nuclio Serverless Functions" >}}

- <a id="fix-ui-nuclio-func-volume-cfg-edit-collapse-entry-to-save"></a>Changes to a Nuclio function's volume configuration from the dashboard (see the <gui-label>Volumes</gui-label> section of the function <gui-title>Configuration</gui-title> tab) are now reflected immediately in the UI, without the need to first  collapse the volume entry in the dashboard.

    {{< internal-release-note rnid="fix-ui-nuclio-func-volume-cfg-edit-collapse-entry-to-save" ig="11094" type="bug" ki_start_ver="2.5.0" owner="Eran N. (erann)" >}}
<br/>
(sharonl) (24.5.20) In the v2.5.0 release notes, this known issue was associated with Bug {{< jira ig="13478" >}} (_Customer Name_ field = Samsung).
This bug has now been _closed as a **duplicate**_ of Bug {{< jira ig="11094" >}}, which is currently marked as _Fixed_ in v2.10.0.
=> In the v2.8.0 release notes, I changed the KI association to IG-11094, but I kept the old KI ID description, which reflects what's visible to the user.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-presto" text="Presto" >}}

- <a id="fix-presto-same-resource-name-for-diff-cfg-types"></a>It's now possible to edit a configuration file with the same name for different types of resources (namely, coordinator and worker).

    {{< internal-release-note rnid="fix-presto-same-resource-name-for-diff-cfg-types" ig="16600" type="bug" ki_start_ver="2.8.0" owner="Oded M. (odedm)" >}}
<br/>
(sharonl) (1.10.20) Added retroactively as a known issue in the {{< xref f="release-notes/version-2.8/v2.8.0.md" a="ki-presto-same-resource-name-for-diff-cfg-types" text="v2.8.0 release notes" >}}, and replaced with a fix note in the v2.10.0 release notes.
(11.10.20) Slightly rephrased following R&D input (see the {{< jira ig="16600" >}} comments).
<br/>
In v2.8.0, if you try to add a coordinator or worker Presto configuration file with the same name as an existing file of the other type (worker/coordinator), you get an error (`"ERROR Duplicate presto resource - <resource file>"`).
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="fixes-logging-n-monitoring-services" text="Logging and Monitoring Services" >}}

- <a id="fix-log-forwarder-memory-consumption"></a>Keeping the log-forwarder service enabled over time no longer causes extensive memory consumption or disruption of other application services.

    {{< internal-release-note rnid="fix-log-forwarder-memory-consumption" ig="15675" type="bug" ki_start_ver="2.8.0" owner="Hedi I. (hedii)" >}}
    {{< /internal-release-note >}}

- <a id="fix-log-forwarder-ui-res-cfg"></a>Changes to the log-forwarder resource configuration that are done from the dashboard are no longer ignored.

    {{< internal-release-note rnid="fix-log-forwarder-ui-res-cfg" ig="15675" type="bug" ki_start_ver="2.8.0" owner="Hedi I. (hedii)" >}}
    {{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Known Issues -->
{{< rn-heading t="known-issues" >}}

[Application Services](#ki-managed-app-services)  | [Backup and Recovery](#ki-backup-recovery-n-ha) | [Dashboard (UI](#ki-dashboard) | [File System](#ki-file-system) | [Frames](#ki-frames) | [Hadoop](#ki-hadoop) | [High Availability (HA)](#ki-backup-recovery-n-ha)| [Hive](#ki-presto-n-hive) | [Jupyter](#ki-jupyter) | [Nuclio](#ki-nuclio) | [Presto](#ki-presto-n-hive) | [Prometheus](#ki-tsdb-prometheus) | [Spark](#ki-spark) | [TSDB](#ki-tsdb) | [Web APIs](#ki-web-apis)

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-managed-app-services" text="Managed Application Services" >}}
{{< comment >}}<!-- [IntInfo] (sharonl) (12.5.19) At Adi's request, I removed
  the note that I previously added here to refer to the dashboard known issues
  for UI issues related to services. -->
{{< /comment >}}

- <a id="ki-services-failure-for-reused-usernames"></a>A user whose username was previously in use in the platform cannot run application services.

    {{< internal-release-note rnid="ki-services-failure-for-reused-usernames" ig="14150" type="bug" ki_start_ver="2.0.0" owner="Felix G. (felixg)" >}}
<br/>
(sharonl) (21.3.19) In consultation with Adi, I documented this only as a known issue in the release notes, but not in other doc locations.
<br/>
(25.5.20) In the v2.0.0&ndash;v2.5.0 RNs, this issue was associated with Bug {{< jira ig="11202" >}}.
On 6.5.20, Orit closed this bug as _Not a bug_ and opened Requirement {{< jira ig="14150" >}} "deletion of user workspace when deleting a user" (currently planned for v3.0.0), which according to Orit should also resolve this issue.
=> I've now linked the KI in Jira to requirement IG-14150 instead of the closed IG-11202 bug (including duplicating the existing RN labels and adding a KI RN label for v2.8.0), and I've marked IG-11202 as _External Release Note_ = _DONE_ so that it doesn't appear in related queries.
{{< comment >}}<!-- [IntInfo] (sharonl) (25.5.20) In the v2.8.0 RNs I also
  removed the connection to DOC IG-10843, as this was the general v2.0.0/v2.1.0
  doc task for managed application services. -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-jupyter" text="Jupyter Notebook" >}}

- <a id="ki-jupyter-notebooks-scala-not-supported"></a>Running Scala code from a Jupyter notebook isn't supported in the current release.
    Instead, run Scala code from a Zeppelin notebook or by using <file>spark-submit</file>, or use Python.

    {{< internal-release-note rnid="ki-jupyter-notebooks-scala-not-supported" ig="11174" type="bug" ki_start_ver="2.0.0" owner="Gal T. (galt)" docig="10216" id="ig-11174" >}}
<br/>
(sharonl) This is documented also in the **Software Specifications and Restrictions** and in other locations in the doc.
{{< comment >}}<!-- [c-jupyter-notebooks-scala-not-supported] -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-jupyter-dir-delete"></a>You cannot currently delete directories from the Jupyter UI.
    You can still delete directories by running a file-system command in a command-line interface, such as a Jupyter terminal or notebook or a web shell.

    {{< internal-release-note rnid="ki-jupyter-dir-delete" ig="16587" type="bug" ki_start_ver="2.8.0" owner="Uri H. (urih)" >}}
<br/>
(sharonl) (13.10.20) Added in the v2.10.0 RNs and also retroactively in the v2.8.0 RNs.
The bug existed also to earlier releases.
<br/>
The bug was fixed for v3.0.0.
    {{< /internal-release-note >}}

- <a id="ki-jupyter-wrong-ver-in-services-ui"></a>The <gui-title>Services</gui-title> dashboard page displays an incorrect version for the Jupyter Notebook service (version 1.0.2 instead of the installed JupyterLab version 1.0.10).

    {{< internal-release-note rnid="ki-jupyter-wrong-ver-in-services-ui" ig="16755" type="bug" owner="Uri H. (urih)" ki_start_ver="2.8.0" docig="15178" >}}
<br/>
(sharonl) (22.9.20) This was also a known issue in the v2.8.0 RNs, and at the time it wasn't connected to a specific Jira ticket (see the **"2.8.0 Support matrix"** email thread).
When it was discovered that the bug persists in v2.10.0, bug {{< jira ig="16755" >}} was opened (see the **"2.10.0 Support Matrix"** email thread, copied in DOC {{< jira ig="16762" >}}).
(25.2.21) Bug IG-16755 was fixed in v2.10.x (post the v2.10.0 release) and a separate Bug {{< jira ig="17040" >}} was opened for v3.0.0 and fixed and closed for this version, so in the v3.0.0 release notes the known issue was replaced with this fix note (`fix-jupyter-wrong-ver-in-services-ui`).
{{< comment >}}<!-- [IntInfo] (sharonl) (25.2.21) I edited the RN retroactively
  to fix a typo - "JupyterLab Notebook" > "Jupyter Notebook" - and I updated
  the internal info. -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-frames" text="V3IO Frames" >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-frames-nosql" text="Frames NoSQL Backend (&quot;nosql&quot;/&quot;kv&quot;)" >}}

- <a id="ki-frames-nosql-write-partitioned-table-overwriteTable-not-supported"></a>The <func>write</func> client method's <api>"overwriteTable"</api> save mode isn't supported for partitioned tables {{< techpreview mark="1" >}}.

    {{< internal-release-note rnid="ki-frames-nosql-write-partitioned-table-overwriteTable-not-supported" ig="16167" type="bug" ki_start_ver="2.10.0" owner="Dina N. (dinan)" docig="15553" >}}
<br/>
(sharon) (27.9.20) I added the {{< techpreview mark="1" >}} mark because the support for writing partitioned tables is {{< techpreview fmt="0" >}} in v2.10.0 (see Requirement {{< jira ig="13538" >}}, DOC {{< jira ig="15553" >}}) and the related {{< xref f="release-notes/version-2.10/v2.10.0.md" a="new-frames-nosql-write-partitioned-tables-tp" text="v2.10.0 new-feature release note" >}}.

    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-nuclio" text="Nuclio Serverless Functions" >}}

- <a id="ki-nuclio-function-name-dashboard"></a>Deploying a Nuclio function named <func>dashboard</func> renders the <gui-title>Projects</gui-title> dashboard page (the Nuclio dashboard) inaccessible.
  To avoid this, refrain from naming Nuclio functions <func>dashboard</func>.

    {{< internal-release-note rnid="ki-nuclio-function-name-dashboard" ig="16818" type="bug" ki_start_ver="2.8.0" owner="Oded M. (odedm)" >}}
<br/>
(sharonl) (25.9.20) I added the KI in the new v2.10.0 release notes and also retroactively in the v2.8.0 release notes, and also documented this restriction in the {{< xref f="data-layer/reference/reserved-names.md" >}} reference.
{{< comment >}}<!-- [c-nuclio-function-name-dashboard] [IntInfo] (sharonl)
  (23.2.21) In v3.0.0, I updated the Nuclio reserved-names documentation and
  moved it to cluster-mgmt/deployment/sw-specifications.md
  (#nuclio-reserved-names) [c-nuclio-reserved-names]. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-nuclio-unsupported-dark-site-langs"></a>Function deployment for the Ruby, Node.js, and .NET Core runtimes isn't supported for platform installations without an internet connection (offline installations).

    {{< internal-release-note rnid="ki-nuclio-unsupported-dark-site-langs" ig="13134" type="bug" ki_start_ver="2.0.0" owner="Oded M. (odedm)" id="ig-13134" >}}
<br/>
(sharonl) (16.10.19) In the v2.5.0 release notes, I moved the Java reference that was in the similar v2.3 release-notes known issue for Bug {{< jira ig="11148" >}} to a {{< xref f="release-notes/version-2.5/v2.5.0.md" a="fix-nuclio-unsupported-dark-site-java" text="fix note" >}} for this bug, and I added .NET Core to the remaining known issue and associated it with the new bug {{< jira ig="13134" >}}.
{{< comment >}}<!-- [c-nuclio-unsupported-dark-site-langs] [IntInfo] (sharonl)
  (7.10.20) In v2.10.0, I added related info also in specs/sw-specifications.md
  - #nuclio-offline-deploy > #nuclio-offline-deploy-unsupported-langs. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-nuclio-ui-error-for-successful-deployment-after-prev-timeout-failure"></a>In case of a successful automatic deployment of a Nuclio function for a request that previously timed out &mdash; for example, if a requested resource, such as a GPU, becomes available after the time-out failure &mdash; the function status in the dashboard is still <gui-label>Error</gui-label>.

    {{< internal-release-note rnid="ki-nuclio-ui-error-for-successful-deployment-after-prev-timeout-failure" ig="12487" type="bug" ki_start_ver="2.3.0" owner="Adi H. (adih)" id="ig-12487" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-tsdb" text="Time-Series Databases (TSDBs)" >}}

{{< comment >}}<!-- [TODO-NEXT-RNS] Uncomment if we add a ki-tsdb-nuclio-funcs
  section (and add a link to the section at the start of the KIs section). -->
{{< note id="ki-tsdb-other-ki-secs-ref-note" >}}
See also the [TSDB Nuclio functions](#ki-tsdb-nuclio-funcs) section under the Nuclio known issues.
{{< /note >}}
{{< comment >}}<!-- [c-tsdb-nuclio-funcs-classif] -->
{{< /comment >}}
{{< /comment >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-tsdb-prometheus" text="Prometheus" >}}

- <a id="ki-tsdb-prometheus-service-change-to-invalid-path"></a>Changing the TSDB-table configuration for an existing Prometheus service to an invalid path doesn't return a deployment error.

    {{< internal-release-note rnid="ki-tsdb-prometheus-service-change-to-invalid-path" ig="12187" type="bug" owner="Golan S. (golans)" docig="10843" id="ig-12187" >}}
Known issue since RN v2.2.0 / TSDB v0.9.2<br/>
<br/>
(sharonl) (6.6.19) The previous known issue of succeeding to create a new Prometheus table with an invalid TSDB table path was {{< xref s="release-notes" f="version-2.2/v2.2.0.md" text="v2.2.0 release notes" a="fix-tsdb-prometheus-service-create-for-non-existent-tsdb-table" text="fixed" >}} in v2.2.0.
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-presto-n-hive" text="Presto and Hive" >}}

- <a id="ki-presto-hive-username-cant-be-changed-for-existing-db"></a>You can't change the MySQL DB user for an existing Hive MySQL DB.
    To change the user, you need to either reconfigure Hive for Presto to set a different DB path that doesn't yet exist (in addition to the different user); or disable Hive for Presto, apply your changes, delete the current MySQL DB directory (using a file-system command), and then re-enable Hive for Presto and configure the same DB path with a new user.

    {{< internal-release-note rnid="ki-presto-hive-username-cant-be-changed-for-existing-db" ig="11757" type="bug" ki_start_ver="2.2.0" owner="Golan S. (golans)" docig="9650" id="ig-11757" >}}
    {{< /internal-release-note >}}
    {{< comment >}}<!-- [ci-presto-hive-username-cant-change-for-existing-db]
      (sharonl) (5.7.20) In consultation with Dina and Orit, I documented this
      restriction also in the Hive notes in presto/overview.md (added
      retroactively to the current active doc sites when this was in a
      reference/ section - v2.8.0 & v2.5.4). -->
    {{< /comment >}}

- <a id="ki-presto-invalid-cfg-service-n-dependent-services-failure"></a><a id="ki-presto-invalid-connector-cfg-service-n-dependent-services-failure"></a>Invalid Presto configuration files or connector definitions might cause the Presto service and all dependent services to fail.
    This is the expected behavior.
    To recover, delete the problematic configuration or manually revert relevant changes to the default Presto configuration files, save the changes to the Presto service, and select <gui-label>Apply Changes</gui-label> on the dashboard <gui-title>Services</gui-title> page.
    {{< internal-release-note rnid="ki-presto-invalid-cfg-service-n-dependent-services-failure" ig="15849" type="bug" ki_start_ver="2.10.0" owner="Dina N. (dinan)" >}}
<br/>
(sharonl) (7.3.21) Following input from Customer Success (Nir), I edited the note for the new v3.0.0 release notes and retroactively in the v2.10.0 release notes to expand it also to invalid editing of Presto configuration files and not only of an invalid Presto connector configuration.
(I changed the note anchor ID, accordingly, from `#ki-presto-invalid-connector-cfg-service-n-dependent-services-failure` to `#ki-presto-invalid-cfg-service-n-dependent-services-failure`, but in the v2.10.0 release notes I also kept the old anchor so as not to break the old link.)
    {{< /internal-release-note >}}

- <a id="ki-hive-cli-no-web-shell-multi-user-exec-permiss"></a>Running the Hive CLI from a Jupyter Notebook service when there's no web-shell service in the cluster might fail if another user had previously run the CLI from another instance of Jupyter.
    To resolve this, ensure that there's at least one web-shell service in the cluster.

    {{< internal-release-note rnid="ki-hive-cli-no-web-shell-multi-user-exec-permiss" ig="18009" type="bug" ki_start_ver="3.0.0" owner="Dina N. (dinan)" >}}
<br/>
(sharonl (4.3.21) I added the bug in the new v3.0.0 release notes and in consultation with Orit, I added it also, retroactively, to the v2.10.0 RNs; (the affected version in IG-18009 is 2.2.0, which according to Orit is when we added Hive).
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-spark" text="Spark" >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-spark-ui" text="Spark UI" >}}

- <a id="ki-spark-ui-broken-links-after-app-kill"></a>The Spark UI might display broken links after killing a running application from the UI.
    To resolve this, close the current Spark UI browser tab and reopen the UI in a new tab.

    {{< internal-release-note rnid="ki-spark-ui-broken-links-after-app-kill" ig="12143" type="bug" ki_start_ver="2.3.0" owner="Uri H. (urih)" id="ig-12143" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (24.9.20) The bug was closed as _Won't Fix_.
Because this is expected to be a permanent KI, I set the bug's <gui-label>External Release Note</gui-label> field to _DONE_ and not to _DONE-KNOWN-ISSUES_, as there's no point in updating the ticket for each new RN with this KI.
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-spark-streaming" text="Spark Streaming" >}}

- <a id="ki-spark-streaming-consume-after-shard-count-increase-ig-6471"></a>To consume records from new shards after increasing a stream's shard count, you must first restart the Spark Streaming consumer application.

    {{< internal-release-note rnid="ki-spark-streaming-consume-after-shard-count-increase-ig-6471" ig="6471" type="bug" owner="Golan (golans)" docig="7157" ki_start_ver="1.5.3" id="ig-6471" >}}
<br/>
(sharonl) The shard-count increase can currently be done only using the <api>UpdateStream</api> Streaming Web API operation.
<br/>
This bug exists also in earlier versions but was discovered only before the v1.5.3 release and the v1.5.0 documentation publication, and therefore added to the RNs only in v1.5.3 + added to the **Software Specifications and Restrictions** and <api>UpdateStream</api> Streaming Web API reference documentation in v1.5.0 and retroactively also in the v1.0.3 reference &mdash; see DOC Task {{< jira ig="7157" >}}.
    {{< /internal-release-note >}}
    {{< comment >}}<!-- [c-spark-streaming-consume-after-shard-increase] -->
    {{< /comment >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-web-apis" text="Web APIs" >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-simple-object-web-api" text="Simple-Object Web API" >}}

- <a id="ki-s3-web-api-get-n-head-object-wrong-ret-data"></a><api>GET Object</api> and <api>HEAD Object</api> don't return correct `ETag` and `Content-Type` metadata.

    {{< internal-release-note rnid="ki-s3-web-api-get-n-head-object-wrong-ret-data" ig="3453" type="bug" ki_start_ver="1.0.0" owner="Assaf B. (assafb)" >}}
<br/>
(sharonl)
(25.5.20) In the v1.5.0&ndash;v2.5.0 RNs we had a joint KI for Bugs {{< jira ig="3451" >}} and {{< jira ig="3453" >}}.
In v2.8.0 QA verified that the part of the existing RNs KI related to Bug IG-3451 &mdash; <api>GET Object</api> doesn't return the correct <api>Last-Modified</api> metadata &mdash; no longer exists in v2.8.0.
According to Assaf and Orit, it was probably fixed before v2.8.0.
=> In consultation with Orit, I edited the existing KI to restrict it to the remaining issues for Bug IG-3453, associated the KI only with IG-3453, and marked IG-3451 as _External Release Note_ = _Done_, without a v2.8.0 RN label and without adding a v2.8.0 fix note.
    {{< /internal-release-note >}}

- <a id="ki-s3-web-api-ofb-range-req-wrong-ret-code"></a>A range request with an out-of-bounds range returns HTTP status code 200 instead of 400.

    {{< internal-release-note rnid="ki-s3-web-api-ofb-range-req-wrong-ret-code" ig="13449" type="bug" ki_start_ver="2.5.0" owner="Roy B. (royb))" id="ig-13449" >}}
    {{< /internal-release-note >}}

<!-- ++++++++++++++++++++++++++++++++++++++++ -->
{{< rn-heading h="5" id="ki-streaming-web-api" text="Streaming Web API" >}}

-	<a id="ki-putrecords-for-non-stream-dir-error-ig-4310"></a><api>PutRecords</api> with a stream path that points to an existing non-stream directory returns error <api>ResourceNotFoundException</api> instead of <api>ResourceIsnotStream</api>.

    {{< internal-release-note rnid="ki-putrecords-for-non-stream-dir-error-ig-4310" ig="4310" owner="Assaf B. (assafb)" ki_start_ver="1.0.0" >}}
    {{< /internal-release-note >}}

-	<a id="ki-web-invalid-updatestream-req-body-resp-error"></a><api>UpdateStream</api> with an invalid request-body JSON format may not return the expected <api>InvalidArgumentException</api> error.

    {{< internal-release-note rnid="ki-web-invalid-updatestream-req-body-resp-error" ig="4310" owner="Assaf B. (assafb)" ki_start_ver="1.0.0" >}}
    {{< /internal-release-note >}}

<!-- ======================================== -->
{{< rn-heading h="4" id="ki-hadoop" text="Hadoop" >}}

-	<a id="ki-hadoop-unsupported-hdfs-commands"></a>The following Hadoop (`hdfs`) commands aren't supported:
    <cmd>createSnapshot</cmd>, <cmd>deleteSnapshot</cmd>, <cmd>getfattr</cmd>, <cmd>setfattr</cmd>, <cmd>setfacl</cmd>, and <cmd>setrep</cmd>.

    {{< internal-release-note rnid="ki-hadoop-unsupported-hdfs-commands" ki_start_ver="1.0.0" id="hdfs-unsupported-cmds" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (26.6.18) Initially, this known issue included the <cmd>truncate</cmd> command and was associated with Bug {{< jira ig="3106" >}} to implement this command.
Bug 3106 and duplicate Bug {{< jira ig="5239" >}} were resolved in v1.7.0 as {{< techpreview fmt="0" >}} (not tested by QA), at which point I documented the support for <cmd>truncate</cmd> as an {{< xref s="release-notes" f="version-1.7/v1.7.1.md" a="ig-3106" text="enhancement" >}} in the v1.7.1 (v1.7 GA) release notes, removed the reference to the command from the known-issue note, and disassociated the known-issue note from Bug 3106.
The changes were done after the initial publication of the v1.7.0 Equinix Early Edition release notes.
<br/>
Golan said there's no Jira ticket to support the other commands.
<br/>
(2.5.19) In the v2.1.0 release notes, the reference to the supported <cmd>truncate</cmd> command was marked as  {{< techpreview fmt="0" >}}.
(27.10.19) In the v2.5.0 RNs, the <cmd>truncate</cmd> reference was removed from this note, after QA have verified the support for this command.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-file-system" text="File System" >}}

{{< note id="ki-file-system-hadoop-fs-note" >}}
For Hadoop FS known issues, see the [Hadoop known issues](#ki-hadoop).
{{< /note >}}

-	<a id="ki-fs-ops-no-stream-shard-sup"></a>File-system operations are not supported for stream shards.

    {{< internal-release-note rnid="ki-fs-ops-no-stream-shard-sup" ig="3605" type="task" owner="Ortal L. (ortall))" ki_start_ver="1.0.0" id="ig-3604" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(17.10.19) I decided to stop making RN-updates in Task {{< jira ig="3604" >}} because the task has now been canceled => permanent known issue.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-backup-recovery-n-ha" text="Backup, Recovery, and High Availability (HA)" >}}

- <a id="ki-stream-data-no-backup"></a>Stream data is not backed up and restored as part of the platform's backup and upgrade operations.

    {{< internal-release-note rnid="ki-stream-data-no-backup" ki_start_ver="1.5.1" >}}
**[PERMANENT-KI]**
<br/>
<br/>
(sharonl) (13.2.18) Adi requested that we document this as a known issue in the RNs even though this behavior is by design and currently there's no plan to change it => permanent known issue.
<br/>
This is also documented in the **Software Specifications and Restrictions** document (DOC Task {{< jira ig="3582" >}}).
    {{< /internal-release-note >}}

- <a id="ki-ha-main-k8s-master-app-node-failure-cloud-azure"></a>In Azure cloud environments, in case of failure in the main master application node of a Kubernetes application cluster, the platform's access to the application nodes is lost and it can no longer manage the cluster's application services.
  Previously run application services will continue to run but cannot be stopped, and new services cannot be created.

    {{< internal-release-note rnid="ki-ha-main-k8s-master-app-node-failure-cloud-azure" ig="13840" type="req" owner="Alze P. (alexp)" >}}
Known issue since RN v2.1.0 for all environments; restricted in RN v2.3.0 to the cloud, and restricted in RN v2.8.0 to Azure (see the v2.8.0 {{< xref s="release-notes" f="version-2.8/v2.8.0.md" text="v2.8.0 release notes" a="fix-cloud-vip-aws" text="AWS fix note" >}})<br/>
<br/>
(sharonl) (10.7.19) In the **v2.1.0 &amp; v2.2.0** release notes we had a known issue for this HA bug that wasn't restricted to cloud environments.
The known issue was for Bug {{< jira ig="11774" >}}.
In **v2.3.0**, this bug was fixed only for non-cloud environments and Bug {{< jira ig="12573" >}} and Requirement {{< jira ig="12636" >}} were opened for the remaining cloud issues.
=> In the v2.3.0 release notes, I restricted the known issue to cloud environments and linked it to the new cloud tickets, and I added a {{< xref s="release-notes" f="version-2.3/v2.3.0.md" a="fix-ha-main-k8s-master-app-node-failure-non-cloud" text="fix note" >}} for non-cloud environments.
(24.5.20) In **v2.8.0**, the KI was fixed for AWS as part of Requirement {{< jira ig="12636" >}}, which was then modified not to cover Azure and a separate v3.0.0 Requirement {{< jira ig="13840" >}} was opened for Azure cloud deployment changes that should also fix this KI, and the RN KI was modified accordingly and tagged `#ki-ha-main-k8s-master-app-node-failure-cloud-azure`; see also the v2.8.0 RNs `#fix-cloud-vip-aws` {{< xref s="release-notes" f="version-2.8/v2.8.0.md" text="v2.8.0 release notes" a="fix-cloud-vip-aws" text="AWS fix note" >}}.
    {{< /internal-release-note >}}

- <a id="ki-ha-duplicated-data-on-recovery"></a>The automatic execution of the system-failure recovery process might result in duplicated data for streaming and simple-object data-append operations or duplicated execution of update expressions.

    {{< internal-release-note rnid="ki-ha-duplicated-data-on-recovery" id="ig-4887-7790" >}}
**Bugs:** {{< jira ig="4887" >}} (streaming &amp; S3 data append); {{< jira ig="7790" >}} (update expressions)<br/>
**Owner:** Orit N. (oritn)<br/>
Known issue since RN v1.5.0 for Bug {{< jira ig="4887" >}} and since RN v1.7.0 also for Bug {{< jira ig="7790" >}} (update-expression addition).<br/>
<br/>
(sharonl) This issue is also documented in the **"Software Specifications and Restrictions"** doc (`#duplicated-sys-failure-recovery-data`).
<br/>
(27.2.19) Bug {{< jira ig="7790" >}} has now been closed as _Won't Fix_.
Ori wrote in the comments "This is a know arch limitation of the system, at this point one can use the conditional update with the mtime returned on the command".
(2.6.19) Orit confirmed that the issue persists in v2.2.0 and she and Adi said to keep the known issue in the release notes as-is (see also this IG-7790 [comment](https://jira.iguazeng.com/browse/IG-7790?focusedCommentId=44092&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-44092) in Bug {{< jira ig="7790" >}}).
Orit wrote that this issue isn't related to the app nodes but rather the data path.
<br/>
(17.10.19) I decided to stop making RN-updates in Bug {{< jira ig="7790" >}} because the bug has been closed as _Won't Fix_ for a while and the issue isn't likely to be resolved in the foreseeable future => permanent known issue.
Bug {{< jira ig="4887" >}}, which is also related to this known issue, is still open, so I still update the RN information in that bug.
{{< comment >}}<!-- [PERMANENT-KI] in relation to Bug IG-7790 (not IG-4887). -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-ha-getitems-failure-ig-6475"></a>The <api>GetItems</api> NoSQL Web API operation might fail during failover.

    {{< internal-release-note rnid="ki-ha-getitems-failure-ig-6475" ig="6475" type="bug" owner="Roy B. (royb)" ki_start_ver="1.7.0" id="ig-6475" >}}
<br/>
(sharonl) (26.2.19) In previous release notes we had a combined HA known issue for bugs {{< jira ig="6475" >}} and {{< jira ig="6246" >}}, phrased generically as "I/O requests might fail during failover.", per Ori's guideline (see, for example, the {{< xref s="release-notes" f="version-1.9/v1.9.5.md" a="ki-ha-io-requests-failure-ig-6475-6246" text="v1.9.5 RNs" >}}) and the details in the bugs' _Release Note Information_).
In v2.0.0, Bug IG-6246 was fixed and closed, but Bug IG-6475 is still open and was deferred to v2.4.0.
Roy B. and Ortal confirmed that the remaining known issue for Bug IG-6475 is only for the <api>GetItems</api> NoSQL Web API operation.
Therefore, I rephrased the known issue and created a {{< xref s="release-notes" f="version-2.0/v2.0.0.md" text="v2.0.0 release notes" a="fix-ha-io-stream-requests-failure-ig-6246" text="fix RN" >}} for Bug IG-6246.
    {{< /internal-release-note >}}

<!-- ---------------------------------------- -->
{{< rn-heading id="ki-dashboard" text="Dashboard (UI)" >}}

{{< note id="ki-dashboard-other-ki-secs-ref-note" >}}
See the [Nuclio known issues](#ki-nuclio) for Nuclio-specific dashboard issues.
{{< /note >}}

- <a id="ki-ui-smtp-email-notif-tenant-admin-req"></a>On the <gui-title>SMTP</gui-title> settings page, you might see an "`Error: Failed to fetch user list (you can still update the rest)`" error message, and the <gui-label>Users to notify</gui-label> field might be disabled. However, you can still configure SMTP without email notifications. To avoid the error and use the <gui-label>Users to notify</gui-label> parameter to define IT Admin users who'll receive cluster-alert email notifications, use a user with the IT Admin and Tenant Admin management policies, such as the predefined tenancy_admin user.
    {{< comment >}}<!-- [ci-underscore-distorts-md-syntax-highlight] [InfraInfo]
      (sharonl) (12.10.20) I kept all the sentences on one line in the source,
      because otherwise the underscore in "tenancy_admin" distorts my editor's
      Markdown syntax highlighting, even if the name is quoted. "\_" doesn't
      help, and I didn't want to use an HTML character entity for the
      underscore not format the name as code, which isn't our convention. -->
    {{< /comment >}}

    {{< internal-release-note rnid="ki-ui-smtp-email-notif-tenant-admin-req" ig="16892" type="bug" ki_start_ver="2.10.0" owner="Oded M. (odedm)" docig="15568" >}}
<br/>
(sharonl) (12.10.20) See the related v2.10.0 RNs UI enhancement note {{< xref f="release-notes/version-2.10/v2.10.0.md" a="new-ui-smtp-alert-emails" text="#new-ui-smtp-alert-emails" >}}, and the information in Bug {{< jira ig="16892" >}} and related Requirement {{< jira ig="14371" >}} and DOC {{< jira ig="15568" >}}.
{{< comment >}}<!-- [c-smtp-email-notif-tenant-admin-req] [IntInfo] (sharonl)
  This is also documented on the intro/setup/howto/smtp.md page - see the
  #smtp-email-notif-tenant-admin-req-note note. When But IG-16892 is fixed, we
  need to remove that note, in addition to replacing the RN known issue with a
  fix note in the relevant release notes. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-ui-nosql-table-distortion-w-attr-name-closed"></a>When browsing a NoSQL table with an attribute named <attr>closed</attr>, the <gui-label>Table</gui-label> view is distorted.

    {{< internal-release-note rnid="ki-ui-nosql-table-distortion-w-attr-name-closed" ig="16747" type="bug" ki_start_ver="2.8.0" owner="Eran N. (erann)" >}}
<br/>
(sharonl) (24.9.20) This KI was added to the v2.10.0 RNs and also retroactively to the v2.8.0 RNs, in consultation with Orit and Adi (see Bug {{< jira ig="16747" >}}).
The bug has now been closed as fixed in v3.0.0.
{{< comment >}}<!-- [TODO-NEW-RELEASE] Replace this KI with a fix note. -->
{{< /comment >}}
    {{< /internal-release-note >}}

- <a id="ki-ui-used-capacity-long-refresh-in-loaded-systems"></a>In heavily loaded systems, the container used-capacity information might take a long time to refresh.

    {{< internal-release-note rnid="ki-ui-used-capacity-long-refresh-in-loaded-systems" ig="5633" type="bug" ki_start_ver="1.7.0" owner="Alex T. (alext)" id="ig-5633" >}}
<br/>
(21.10.18) In the v1.7 RNs, internally we initially connected this known issue incorrectly to Bug {{< jira ig="7883" >}}.
We discovered the error when we prepared the v1.9.4 (v1.9 GA) RNs, and Ori said that because there's no dedicated bug for this issue, we should connect the issue, for now, to the "KV for file system" Requirement {{< jira ig="5633" >}} &mdash; as implementing and integrating this solution is a prerequisite to resolving the known issue.
{{< comment >}}<!-- (8.3.21) At Oded's request, I changed the internal owner
  info from pavelr to alext, as Pave no longer works at Iguazio and IG-5633 is
  now assigned to Alex T. -->
{{< /comment >}}
    {{< /internal-release-note >}}

<!-- //////////////////////////////////////// -->
<!-- Notes -->
{{< rn-heading t="notes" >}}

- <a id="note-coordinated-platform-actions"></a>Platform start-up, shut-down, and upgrade actions should be coordinated with the Iguazio support team.

    {{< internal-release-note rnid="note-coordinated-platform-actions" docig="5829" >}}
<br/>
(sharonl) (2.4.18) Adi requested that we add this in the release notes (starting with the v1.5.3 RNs, even though it's also true for earlier versions).
<br/>
(20.3.19) At Maor's request, I rephrased the previous note to replace "restart, and shut-down" with "start-up and upgrade" (+ I added "actions"); Maor said that customers don't need to coordinate platform shutdowns.
<br/>
(25.5.20)At Maor's request, I returned the reference to "shut-down". Maor wrote (in his v2.8.0 RNs review) &mdash; "I know I (probably) said a user can shutdown the platform by himself (cause he can, thru the UI), but let's add "shutdown" there as well.
The reason is that customer can't really verify a successful shutdown, and might end up with a full data loss if they physically shutdown the servers without verifying the data from the cache was dumped into the disks."
    {{< /internal-release-note >}}

